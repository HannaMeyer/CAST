[{"path":[]},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"background","dir":"Articles","previous_headings":"Introduction","what":"Background","title":"1. Introduction to CAST 1.0.0","text":"One key task environmental science obtaining information environmental variables continuously space space time, usually based remote sensing limited field data. respect, machine learning algorithms proven important tool learn patterns nonlinear complex systems. However, field data (reference data general) often extremely clustered space (time) rarely present independent representative sample prediction area. case, standard machine learning strategies suitable, usually ignore spatio-temporal dependencies. becomes problematic (least) two aspects predictive modelling: Overfitted models hardly able make predictions beyond location reference data, well overly optimistic error assessment. approach problems, CAST supports well-known caret package (Kuhn 2018 provide machine learning strategies designed spatio-temporal data. tutorial, guide package show CAST can used train spatial prediction models including objective error estimation, detection spatial overfitting assessment area applicability prediction models. order follow tutorial, assume reader familiar basics predictive modelling nicely explained Kuhn Johnson 2013 well machine learning applications via caret package.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"how-to-start","dir":"Articles","previous_headings":"Introduction","what":"How to start","title":"1. Introduction to CAST 1.0.0","text":"work tutorial, first install CAST package load library: need help, see tutorial, need additional packages:","code":"#install.packages(\"CAST\") library(CAST) help(CAST) library(geodata) library(terra) library(sf) library(caret) library(tmap)"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"example-of-a-typical-spatio-temporal-prediction-task","dir":"Articles","previous_headings":"","what":"Example of a typical spatio-temporal prediction task","title":"1. Introduction to CAST 1.0.0","text":"demonstration CAST functionalities, go typical spatial prediction task, aims producing spatially-continuous map plant species richness South America. reference data, use plant species richness data plot-based vegetation surveys compiled sPlotOpen database described Sabatini et al. 2021 made available CAST package. use WorldClim climatic variables elevation predictors, assuming relevant drivers species richness. use Random Forests machine learning algorithm tutorial.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"description-of-the-example-dataset","dir":"Articles","previous_headings":"Example of a typical spatio-temporal prediction task","what":"Description of the example dataset","title":"1. Introduction to CAST 1.0.0","text":"data include extracted WorldClim elevation information well sampled species richness. Since aim making predictions entire South America, need WorldClim data area. get impression spatial properties dataset, let’s look spatial distribution vegetation plots South America, plotted top elevation model:","code":"data(splotdata) head(splotdata) ## Simple feature collection with 6 features and 16 fields ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: -63.94722 ymin: -30.98694 xmax: -63.32139 ymax: -29.75833 ## Geodetic CRS:  WGS 84 ##   PlotObservationID   GIVD_ID   Country                      Biome ## 1              1955 SA-AR-002 Argentina Dry tropics and subtropics ## 2              1956 SA-AR-002 Argentina Dry tropics and subtropics ## 3              1958 SA-AR-002 Argentina Dry tropics and subtropics ## 4              1960 SA-AR-002 Argentina Dry tropics and subtropics ## 5              1961 SA-AR-002 Argentina Dry tropics and subtropics ## 6              1963 SA-AR-002 Argentina Dry tropics and subtropics ##   Species_richness    bio_1    bio_4 bio_5 bio_6    bio_8    bio_9 bio_12 ## 1               52 17.65000 463.9651  30.5   3.6 23.25000 11.70000    760 ## 2               56 17.35417 459.5525  30.1   3.5 22.91667 11.46667    731 ## 3               65 18.31667 473.3216  31.4   4.2 24.03333 12.25000    810 ## 4               50 18.04167 485.8116  31.2   4.2 23.93333 11.81667    842 ## 5               45 18.79167 478.4959  32.0   4.4 24.48333 12.65000    853 ## 6               31 18.92083 478.9594  32.2   4.5 24.61667 12.76667    842 ##   bio_13 bio_14   bio_15 elev                    geometry ## 1    119      9 68.89403  416 POINT (-63.86056 -30.29722) ## 2    115      9 68.93396  468 POINT (-63.94722 -30.37222) ## 3    129     12 66.87429  232 POINT (-63.66278 -30.37806) ## 4    140     13 65.54655  129 POINT (-63.32139 -30.98694) ## 5    134     12 68.29005  231 POINT (-63.55694 -29.89889) ## 6    133     12 68.72808  243 POINT (-63.53972 -29.75833) wc <- worldclim_global(var=\"bio\",res = 10,path=tempdir()) elev <- elevation_global(res = 10, path=tempdir()) predictors_sp <- crop(c(wc,elev),st_bbox(splotdata)) names(predictors_sp) <- c(paste0(\"bio_\",1:19),\"elev\") plot(predictors_sp$elev) plot(splotdata[,\"Species_richness\"],add=T)"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"model-training-and-prediction","dir":"Articles","previous_headings":"","what":"Model training and prediction","title":"1. Introduction to CAST 1.0.0","text":"start , lets use dataset create “default” Random Forest model predicts species richness based predictor variables. performance assessment use default 3-fold random cross-validation. keep computation time minimum, don’t include hyperparameter tuning (hence mtry set 2) reasonable Random Forests comparably insensitive tuning. Based trained model can make spatial predictions species richness. load multiband raster contains spatial data predictor variables South America. apply trained model data set.  result spatially comprehensive map species richness South America. see simply creating map using machine learning caret easy task, however accurately measuring performance less simple. Though map looks good first sight now follow question accurate map , hence need ask well model able map species richness.","code":"predictors <- c(\"bio_1\", \"bio_4\", \"bio_5\", \"bio_6\",                 \"bio_8\", \"bio_9\", \"bio_12\", \"bio_13\",                 \"bio_14\", \"bio_15\", \"elev\")  # note that to use the data for model training we have to get rid of the  # geometry column of the sf object: st_drop_geometry(splotdata)  set.seed(10) # set seed to reproduce the model model_default <- train(st_drop_geometry(splotdata)[,predictors],                st_drop_geometry(splotdata)$Species_richness,                method=\"rf\",tuneGrid=data.frame(\"mtry\"=2),                importance=TRUE, ntree=50,                trControl=trainControl(method=\"cv\",number=3, savePredictions = \"final\")) prediction <- predict(predictors_sp,model_default,na.rm=TRUE) plot(prediction)"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"cross-validation-strategies-for-spatio-temporal-data","dir":"Articles","previous_headings":"","what":"Cross validation strategies for spatio-temporal data","title":"1. Introduction to CAST 1.0.0","text":"Among validation strategies, k-fold cross validation (CV) popular estimate performance model view data used model training. CV, models repeatedly trained (k models) model run, data one fold put side used model training model validation. way, performance model can estimated using data included model training. Summary statistics can calculated either averaging performance per fold (default caret) calculating statistics held-back data (CAST::global_validation).","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"the-standard-approach-random-k-fold-cv","dir":"Articles","previous_headings":"Cross validation strategies for spatio-temporal data","what":"The Standard approach: Random k-fold CV","title":"1. Introduction to CAST 1.0.0","text":"example used random k-fold CV defined caret’s trainControl argument. specifically, used random 3-fold CV. Hence, data points dataset RANDOMLY split 3 folds. assess performance model let’s look output Random CV: see species richness modelled R² 0.66 indicates good fit data. Sounds good, unfortunately, random k fold CV give us good indication map accuracy reference data random sample prediction area. Random k-fold CV means three folds (highest certainty) contains data points spatial cluster. Therefore, random CV indicate ability model make predictions beyond location training data (.e. map species richness). Since aim map species richness, rather need perform target-oriented validation validates model view spatial mapping.","code":"model_default ## Random Forest  ##  ## 703 samples ##  11 predictor ##  ## No pre-processing ## Resampling: Cross-Validated (3 fold)  ## Summary of sample sizes: 470, 469, 467  ## Resampling results: ##  ##   RMSE      Rsquared   MAE      ##   25.75544  0.6651004  15.08417 ##  ## Tuning parameter 'mtry' was held constant at a value of 2 global_validation(model_default) ##       RMSE   Rsquared        MAE  ## 25.9283336  0.6672803 15.0845659"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"target-oriented-validation","dir":"Articles","previous_headings":"Cross validation strategies for spatio-temporal data","what":"Target-oriented validation","title":"1. Introduction to CAST 1.0.0","text":"interested model performance view random subsets vegetation plots, need know well model able make predictions areas without reference samples. find , need repeatedly leave larger spatial regions one vegetation plots use test data CV. Several suggestions spatial CV exist. CAST implements straightforward way (CAST::createSpaceTimeFolds) sophisticated method nearest neighbor distance matching (Mila et al 2022, Linnenbrink et al 2023) either leave-one-CV (CAST::nndm) k-fold CV (CAST::knndm). CAST’s function “CreateSpaceTimeFolds” designed provide index arguments used caret’s trainControl. index defines data points used model training model run reversely defines data points held back. Hence, using index argument can account dependencies data leaving complete data one regions (LLO CV), one time steps (LTO CV) locations time steps (LLTO CV). example ’re focusing LLO CV. use column “Country” define location samples split data folds using information, .e. avoid data country located , training test data. Analog random CV split data three folds, hence three model runs performed leaving one third data validation. Knndm tries find k-fold configuration integral absolute differences (Wasserstein W statistic) empirical nearest neighbour distance distribution function test training data CV, empirical nearest neighbour distance distribution function prediction training points, minimised. words, try split data folds difficulty prediction cross-validation comparable difficulty deploying model entire South America, difficulty defined spatial distances. Note, feature space distances also implemented option. See tutorial cross-validation package information. Example CreateSpacetimeFolds: Example knndm: Let’s compare well strategies fit prediction task, especially comparison random cross-validation. use CAST’s geodist function . Geodist calculates nearest neighbor distances geographic space feature space training data well training data prediction locations. Optional, want focus , nearest neighbor distances training data CV folds computed. See tutorial geodistance visualizations package information.    see using random folds, ’re testing well model can make predictions new areas hundreds meters away training data. Using createSpaceTimeFolds Country spatial unit (.e. leave-country-CV), produce prediction situations even harder prediction task. Knndm provides best option, since CV distances create cross-validation comparable required predict entire area. inspecting output model, see view new locations, R² much lower RMSE much higher compared expected random CV. Apparently, considerable overfitting model, causing good random performance poor performance view new locations. might partly attributed choice variables must suspect certain variables misinterpreted model (see Meyer et al 2018 talk OpenGeoHub summer school 2019). Let’s look variable importance ranking Random Forest. Assuming certain variables misinterpreted algorithm able produce higher LLO performance variables removed. Let’s see true…","code":"set.seed(10) indices_LLO <- CreateSpacetimeFolds(splotdata,spacevar = \"Country\",                                 k=3) set.seed(10) indices_knndm <- knndm(splotdata,predictors_sp,k=3) plot(geodist(splotdata,predictors_sp,cvfolds =model_default$control$indexOut))+    scale_x_log10(labels=round) plot(geodist(splotdata,predictors_sp,cvfolds =indices_LLO$indexOut))+    scale_x_log10(labels=round) plot(geodist(splotdata,predictors_sp,cvfolds =indices_knndm$indx_test))+    scale_x_log10(labels=round) model <- train(st_drop_geometry(splotdata)[,predictors],                    st_drop_geometry(splotdata)$Species_richness,                    method=\"rf\",                    tuneGrid=data.frame(\"mtry\"=2),                     importance=TRUE,                    trControl=trainControl(method=\"cv\",                                           index = indices_knndm$indx_train,                                           savePredictions = \"final\")) model ## Random Forest  ##  ## 703 samples ##  11 predictor ##  ## No pre-processing ## Resampling: Cross-Validated (10 fold)  ## Summary of sample sizes: 520, 442, 444  ## Resampling results: ##  ##   RMSE      Rsquared   MAE      ##   34.13482  0.4754385  20.79699 ##  ## Tuning parameter 'mtry' was held constant at a value of 2 global_validation(model) ##       RMSE   Rsquared        MAE  ## 34.5314356  0.4269648 20.9480996 plot(varImp(model))"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"removing-variables-that-cause-overfitting","dir":"Articles","previous_headings":"","what":"Removing variables that cause overfitting","title":"1. Introduction to CAST 1.0.0","text":"CAST’s forward feature selection (ffs) selects variables make sense view selected CV method excludes counterproductive (meaningless) view selected CV method. use LLO CV method, ffs selects variables lead combination highest LLO performance (.e. best spatial model). variables spatial meaning even counterproductive won’t improve even reduce LLO performance therefore excluded model ffs. ffs job first training models using possible pairs two predictor variables. best model initial models kept. basis best model predictor variables iterativly increased remaining variables tested improvement currently best model. process stops none remaining variables increases model performance added current best model. let’s run ffs case study. process take 1-2 minutes… Using ffs LLO CV, RMSE decreased. variables selected final model. others removed (least small example) spatial meaning even counterproductive. plotting results ffs, can visualize performance model changed depending variables used:  See best model using combinations two variables. Based best performing two variables, using third variable slightly increase R², applies fourth variable. variables improve LLO performance. Note R² features high standard deviation regardless variables used. due small dataset used lead robust results . effect new model spatial representation species richness?","code":"set.seed(10) ffsmodel <- ffs(st_drop_geometry(splotdata)[,predictors],                     st_drop_geometry(splotdata)$Species_richness,                     method=\"rf\",                      tuneGrid=data.frame(\"mtry\"=2),                     verbose=FALSE,                     ntree=25, #make it faster for this tutorial                     trControl=trainControl(method=\"cv\",                                            index = indices_knndm$indx_train,                                            savePredictions = \"final\")) ffsmodel ## Selected Variables:  ## bio_1 bio_14 bio_12 elev ## --- ## Random Forest  ##  ## 703 samples ##   4 predictor ##  ## No pre-processing ## Resampling: Cross-Validated (10 fold)  ## Summary of sample sizes: 520, 442, 444  ## Resampling results: ##  ##   RMSE      Rsquared   MAE     ##   32.74653  0.5299616  20.5449 ##  ## Tuning parameter 'mtry' was held constant at a value of 2 global_validation(ffsmodel) ##       RMSE   Rsquared        MAE  ## 33.2113246  0.4628401 20.8088080 ffsmodel$selectedvars ## [1] \"bio_1\"  \"bio_14\" \"bio_12\" \"elev\" plot(ffsmodel) prediction_ffs <- predict(predictors_sp, ffsmodel, na.rm=TRUE) plot(prediction_ffs)"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"area-of-applicability","dir":"Articles","previous_headings":"","what":"Area of Applicability","title":"1. Introduction to CAST 1.0.0","text":"Still required analyse model can applied entire study area locations different predictor properties model learned . See details vignette Area applicability Meyer Pebesma 2021.  figure shows grey areas outside area applicability, hence predictions considered locations. See tutorial AOA package information. also see area low random CV error applies, comparably small.","code":"### AOA for which the spatial CV error applies: AOA <- aoa(predictors_sp,ffsmodel,LPD = TRUE,verbose=FALSE) tm_shape(prediction) +   tm_raster(col.scale = tm_scale_continuous(values = \"-viridis\"),             col.legend = tm_legend(title = \"Species \\nrichness\")) +   tm_shape(AOA$AOA) +   tm_raster(col.scale = tm_scale_categorical(values = c(\"grey\", \"#00000000\")),             col.legend = tm_legend(show = FALSE)) +   tm_layout(frame = FALSE) +   tm_add_legend(type=\"polygons\", fill = \"grey\", labels = \"Outside \\nAOA\")"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"error-profiles","dir":"Articles","previous_headings":"Area of Applicability","what":"Error profiles","title":"1. Introduction to CAST 1.0.0","text":"aoa function returned, addition AOA also dissimilarity index (DI) local point density (LPD). DI indicates distance feature space nearest training data point LPD indicates data point density feature space. expect, relationship DI/LPD prediction performance, .e. expect areas low DI high LPD (well covered reference data) lower error. going analyze using CAST::errorProfiles.    Since relationship LPD RMSE seems comparably high, ’re going use model error entire prediction area.","code":"plot(c(AOA$DI,AOA$LPD)) errormodel_DI <- errorProfiles(model_default,AOA,variable=\"DI\") errormodel_LPD <- errorProfiles(model_default,AOA,variable=\"LPD\")  plot(errormodel_DI) plot(errormodel_LPD) expected_error_LPD = terra::predict(AOA$LPD, errormodel_LPD) plot(expected_error_LPD)"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"1. Introduction to CAST 1.0.0","text":"conclude, tutorial shown CAST can used facilitate target-oriented (: spatial) CV spatial spatio-temporal data crucial obtain meaningful validation results. Using ffs conjunction target-oriented validation, variables can excluded counterproductive view target-oriented performance due misinterpretations algorithm. ffs therefore helps select ideal set predictor variables spatio-temporal prediction tasks gives objective error estimates.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"final-notes","dir":"Articles","previous_headings":"","what":"Final notes","title":"1. Introduction to CAST 1.0.0","text":"intention tutorial describe motivation led development CAST well functionality. Priority modelling species richnessin best possible way provide example motivation functionality CAST can run within minutes. Hence, small subset entire sPlotOpen dataset used. Keep mind due small subset example robust quite different results might obtained depending small changes settings.","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"tutorials","dir":"Articles","previous_headings":"Further Reading","what":"Tutorials","title":"1. Introduction to CAST 1.0.0","text":"Tutorials package talk OpenGeoHub summer school 2019 spatial validation variable selection: https://www.youtube.com/watch?v=mkHlmYEzsVQ. Tutorial (https://youtu./EyP04zLe9qo) Lecture (https://youtu./OoNH6Nl-X2s) recording OpenGeoHub summer school 2020 area applicability. well talk OpenGeoHub summer school 2021: https://av.tib.eu/media/54879 Talk tutorial OpenGeoHub 2022 summer school Machine learning-based maps environment - challenges extrapolation overfitting, including discussions area applicability nearest neighbor distance matching cross-validation (https://doi.org/10.5446/59412).","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"spatial-cross-validation","dir":"Articles","previous_headings":"Further Reading > Scientific documentation of the methods","what":"Spatial cross-validation","title":"1. Introduction to CAST 1.0.0","text":"Milà, C., Mateu, J., Pebesma, E., Meyer, H. (2022): Nearest Neighbour Distance Matching Leave-One-Cross-Validation map validation. Methods Ecology Evolution 00, 1– 13. https://doi.org/10.1111/2041-210X.13851 Linnenbrink, J., Milà, C., Ludwig, M., Meyer, H.: kNNDM (2023): k-fold Nearest Neighbour Distance Matching Cross-Validation map accuracy estimation. EGUsphere [preprint]. https://doi.org/10.5194/egusphere-2023-1308 Meyer, H., Reudenbach, C., Hengl, T., Katurji, M., Nauss, T. (2018): Improving performance spatio-temporal machine learning models using forward feature selection target-oriented validation. Environmental Modelling & Software, 101, 1-9. https://doi.org/10.1016/j.envsoft.2017.12.001","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"spatial-variable-selection","dir":"Articles","previous_headings":"Further Reading > Scientific documentation of the methods","what":"Spatial variable selection","title":"1. Introduction to CAST 1.0.0","text":"Meyer, H., Reudenbach, C., Hengl, T., Katurji, M., Nauss, T. (2018): Improving performance spatio-temporal machine learning models using forward feature selection target-oriented validation. Environmental Modelling & Software, 101, 1-9. https://doi.org/10.1016/j.envsoft.2017.12.001 Meyer, H., Reudenbach, C., Wöllauer, S., Nauss, T. (2019): Importance spatial predictor variable selection machine learning applications - Moving data reproduction spatial prediction. Ecological Modelling. 411. https://doi.org/10.1016/j.ecolmodel.2019.108815","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"area-of-applicability-1","dir":"Articles","previous_headings":"Further Reading > Scientific documentation of the methods","what":"Area of applicability","title":"1. Introduction to CAST 1.0.0","text":"Meyer, H., Pebesma, E. (2021). Predicting unknown space? Estimating area applicability spatial prediction models. Methods Ecology Evolution, 12, 1620– 1633. https://doi.org/10.1111/2041-210X.13650","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"applications-and-use-cases","dir":"Articles","previous_headings":"Further Reading > Scientific documentation of the methods","what":"Applications and use cases","title":"1. Introduction to CAST 1.0.0","text":"Meyer, H., Pebesma, E. (2022): Machine learning-based global maps ecological variables challenge assessing . Nature Communications, 13. https://www.nature.com/articles/s41467-022-29838-9 Ludwig, M., Moreno-Martinez, ., Hoelzel, N., Pebesma, E., Meyer, H. (2023): Assessing improving transferability current global spatial prediction models. Global Ecology Biogeography. https://doi.org/10.1111/geb.13635.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast02-plotgeodist.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"2. Visualization of nearest neighbor distance distributions","text":"tutorial shows euclidean nearest neighbor distances geographic space feature space can calculated visualized using CAST. type visualization allows assess whether training data feature representative coverage prediction area cross-validation (CV) folds (independent test data) adequately chosen representative prediction locations. See e.g. Meyer Pebesma (2022) Milà et al. (2022) discussion topic.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast02-plotgeodist.html","id":"sample-data","dir":"Articles","previous_headings":"","what":"Sample data","title":"2. Visualization of nearest neighbor distance distributions","text":"example data, use two different sets global virtual reference data: One spatial random sample second example, reference data clustered geographic space (see Meyer Pebesma (2022) discussions ). can define parameters run example different settings","code":"library(CAST) library(caret) library(terra) library(sf) library(rnaturalearth) library(ggplot2) library(geodata) seed <- 10 # random realization samplesize <- 250 # how many samples will be used? nparents <- 20 #For clustered samples: How many clusters?  radius <- 500000 # For clustered samples: What is the radius of a cluster?"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-plotgeodist.html","id":"prediction-area","dir":"Articles","previous_headings":"Sample data","what":"Prediction area","title":"2. Visualization of nearest neighbor distance distributions","text":"prediction area entire global land area, .e. imagine prediction task aim making global predictions based set reference data.","code":"ee <- st_crs(\"+proj=eqearth\") co <- ne_countries(returnclass = \"sf\") co.ee <- st_transform(co, ee)"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-plotgeodist.html","id":"spatial-random-sample","dir":"Articles","previous_headings":"Sample data","what":"Spatial random sample","title":"2. Visualization of nearest neighbor distance distributions","text":", simulate random sample visualize data entire global prediction area.","code":"sf_use_s2(FALSE) set.seed(seed) pts_random <- st_sample(co.ee, samplesize) ### See points on the map: ggplot() + geom_sf(data = co.ee, fill=\"#00BFC4\",col=\"#00BFC4\") +   geom_sf(data = pts_random, color = \"#F8766D\",size=0.5, shape=3) +   guides(fill = \"none\", col = \"none\") +   labs(x = NULL, y = NULL)"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-plotgeodist.html","id":"clustered-sample","dir":"Articles","previous_headings":"Sample data","what":"Clustered sample","title":"2. Visualization of nearest neighbor distance distributions","text":"second data set use clustered design size.","code":"set.seed(seed) sf_use_s2(FALSE) pts_clustered <- clustered_sample(co.ee, samplesize, nparents, radius)  ggplot() + geom_sf(data = co.ee, fill=\"#00BFC4\",col=\"#00BFC4\") +   geom_sf(data = pts_clustered, color = \"#F8766D\",size=0.5, shape=3) +   guides(fill = \"none\", col = \"none\") +   labs(x = NULL, y = NULL)"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-plotgeodist.html","id":"distances-in-geographic-space","dir":"Articles","previous_headings":"","what":"Distances in geographic space","title":"2. Visualization of nearest neighbor distance distributions","text":"can plot distributions spatial distances reference data nearest neighbor (“sample--sample”) distribution distances points global land surface nearest reference data point (“sample--prediction”). Note samples prediction locations used calculate sample--prediction nearest neighbor distances. Since ’re using global case study , throughout tutorial use sampling=Fibonacci draw prediction locations constant point density sphere.   Note random data set nearest neighbor distance distribution training data quasi identical nearest neighbor distance distribution prediction area. comparison, second data set number training data heavily clustered geographic space. therefore see nearest neighbor distances within reference data rather small. Prediction locations, however, average much away.","code":"dist_random <- geodist(pts_random,co.ee,                             sampling=\"Fibonacci\") dist_clstr <- geodist(pts_clustered,co.ee,                            sampling=\"Fibonacci\")  plot(dist_random, unit = \"km\")+scale_x_log10(labels=round)+ggtitle(\"Randomly distributed reference data\") plot(dist_clstr, unit = \"km\")+scale_x_log10(labels=round)+ggtitle(\"Clustered reference data\")"},{"path":[]},{"path":"https://hannameyer.github.io/CAST/articles/cast02-plotgeodist.html","id":"random-cross-validation","dir":"Articles","previous_headings":"Distances in geographic space > Accounting for cross-validation folds","what":"Random Cross-validation","title":"2. Visualization of nearest neighbor distance distributions","text":"Let’s use clustered data set show distribution spatial nearest neighbor distances cross-validation can visualized well. Therefore, first use “default” way random 10-fold cross validation randomly split reference data training test (see Meyer et al., 2018 2019 see might good idea).   Obviously CV folds representative prediction locations (least terms distance nearest training data point). .e. folds used performance assessment model, can expect overly optimistic estimates validate predictions close proximity reference data.","code":"randomfolds <- caret::createFolds(1:nrow(pts_clustered)) dist_clstr <- geodist(pts_clustered,co.ee,                            sampling=\"Fibonacci\",                             cvfolds= randomfolds) plot(dist_clstr, unit = \"km\")+scale_x_log10(labels=round)"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-plotgeodist.html","id":"spatial-cross-validation","dir":"Articles","previous_headings":"Distances in geographic space > Accounting for cross-validation folds","what":"Spatial Cross-validation","title":"2. Visualization of nearest neighbor distance distributions","text":", however, case CV performance regarded representative prediction task. Therefore, use spatial CV instead. , use leave-cluster-CV, means iteration, one spatial clusters held back.   See fits nearest neighbor distribution prediction area much better. Note geodist also allows inspecting independent test data instead cross validation folds. See ?geodist ?plot.geodist.","code":"spatialfolds <- CreateSpacetimeFolds(pts_clustered,spacevar=\"parent\",k=length(unique(pts_clustered$parent))) dist_clstr <- geodist(pts_clustered,co.ee,                            sampling=\"Fibonacci\",                            cvfolds= spatialfolds$indexOut) plot(dist_clstr, unit = \"km\")+scale_x_log10(labels=round)"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-plotgeodist.html","id":"why-has-spatial-cv-sometimes-blamed-for-being-too-pessimistic","dir":"Articles","previous_headings":"Distances in geographic space > Accounting for cross-validation folds","what":"Why has spatial CV sometimes blamed for being too pessimistic ?","title":"2. Visualization of nearest neighbor distance distributions","text":"Recently, Wadoux et al. (2021) published paper title “Spatial cross-validation right way evaluate map accuracy” state “spatial cross-validation strategies resulted grossly pessimistic map accuracy assessment”. come conclusion? reference data used study either regularly, random comparably mildly clustered geographic space, applied spatial CV strategies held large spatial units back CV. can see happens apply spatial CV randomly distributed reference data.   see nearest neighbor distances cross-validation don’t match nearest neighbor distances prediction. compared section , time cross-validation folds far away reference data. Naturally end overly pessimistic performance estimates make prediction situations cross-validation harder, compared required model application entire area interest (global). spatial CV chosen therefore suitable prediction task, prediction situations created CV resemble encountered prediction.","code":"# create a spatial CV for the randomly distributed data. Here: # \"leave region-out-CV\" sf_use_s2(FALSE) pts_random_co <- st_join(st_as_sf(pts_random),co.ee)   ggplot() + geom_sf(data = co.ee, fill=\"#00BFC4\",col=\"#00BFC4\") +   geom_sf(data = pts_random_co, aes(color=subregion),size=0.5, shape=3) +   scale_color_manual(values=rainbow(length(unique(pts_random_co$subregion))))+   guides(fill = FALSE, col = FALSE) +   labs(x = NULL, y = NULL)+ ggtitle(\"spatial fold membership by color\") spfolds_rand <- CreateSpacetimeFolds(pts_random_co,spacevar = \"subregion\",                                      k=length(unique(pts_random_co$subregion))) dist_rand_sp <- geodist(pts_random_co,co.ee,                              sampling=\"Fibonacci\",                               cvfolds= spfolds_rand$indexOut) plot(dist_rand_sp, unit = \"km\")+scale_x_log10(labels=round)"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-plotgeodist.html","id":"nearest-neighbour-distance-matching-cv","dir":"Articles","previous_headings":"Distances in geographic space > Accounting for cross-validation folds","what":"Nearest Neighbour Distance Matching CV","title":"2. Visualization of nearest neighbor distance distributions","text":"good way approximate geographical prediction distances CV use Nearest Neighbour Distance Matching (NNDM) CV (see Milà et al., 2022 details). NNDM CV variation LOO CV empirical distribution function nearest neighbour distances found prediction matched CV process. Since NNDM CV highly time consuming, k-fold version may provide good trade-. See (see Linnenbrink et al., 2023 details knndm)  NNDM CV-distance distribution matches sample--prediction distribution well. happens use NNDM CV randomly-distributed sampling points instead?  NNDM CV-distance still matches sample--prediction distance function.","code":"nndmfolds_clstr <- knndm(pts_clustered, modeldomain=co.ee, samplesize = 2000) dist_clstr <- geodist(pts_clustered,co.ee,                            sampling = \"Fibonacci\",                            cvfolds = nndmfolds_clstr$indx_test,                             cvtrain = nndmfolds_clstr$indx_train) plot(dist_clstr, unit = \"km\")+scale_x_log10(labels=round) nndmfolds_rand <- knndm(pts_random_co,  modeldomain=co.ee, samplesize = 2000) dist_rand <- geodist(pts_random_co,co.ee,                           sampling = \"Fibonacci\",                           cvfolds = nndmfolds_rand$indx_test,                            cvtrain = nndmfolds_rand$indx_train) plot(dist_rand, unit = \"km\")+scale_x_log10(labels=round)"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-plotgeodist.html","id":"distances-in-feature-space","dir":"Articles","previous_headings":"","what":"Distances in feature space","title":"2. Visualization of nearest neighbor distance distributions","text":"far compared nearest neighbor distances geographic space. can also feature space. Therefore, set bioclimatic variables used (https://www.worldclim.org) features (.e. predictors) virtual prediction task.  visualize nearest neighbor feature space distances consideration cross-validation.   regard chosen predictor variables see nearest neighbor distance clustered training data rather small, compared required prediction. random CV representative prediction locations spatial CV better job.","code":"predictors_global <- worldclim_global(var=\"bio\",res = 10,path=tempdir()) names(predictors_global) <- c(paste0(\"bio_\",1:19))  predictors_global <- predictors_global[[c(\"bio_2\", \"bio_10\", \"bio_13\", \"bio_19\")]] plot(predictors_global) # use random CV: dist_clstr_rCV <- geodist(pts_clustered,predictors_global,                                type = \"feature\",                                 sampling=\"Fibonacci\",                                cvfolds = randomfolds)  # use spatial CV: dist_clstr_sCV <- geodist(pts_clustered,predictors_global,                                type = \"feature\", sampling=\"Fibonacci\",                                cvfolds = spatialfolds$indexOut)   # Plot results: plot(dist_clstr_rCV)+scale_x_log10()+ggtitle(\"Clustered reference data and random CV\") plot(dist_clstr_sCV)+scale_x_log10()+ggtitle(\"Clustered reference data and spatial CV\")"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-plotgeodist.html","id":"references","dir":"Articles","previous_headings":"Distances in feature space","what":"References","title":"2. Visualization of nearest neighbor distance distributions","text":"Meyer, H., Pebesma, E. (2022): Machine learning-based global maps ecological variables challenge assessing . Nature Communications 13, 2208. https://doi.org/10.1038/s41467-022-29838-9 Milà, C., Mateu, J., Pebesma, E., Meyer, H. (2022): Nearest Neighbour Distance Matching Leave-One-Cross-Validation map validation. Methods Ecology Evolution 00, 1– 13. https://doi.org/10.1111/2041-210X.13851. Linnenbrink, J., Milà, C., Ludwig, M., Meyer, H. (2023): kNNDM: k-fold Nearest Neighbour Distance Matching Cross-Validation map accuracy estimation, EGUsphere [preprint], https://doi.org/10.5194/egusphere-2023-1308.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast03-CV.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"3. Nearest neighbor distance matching Cross-validation in CAST","text":"Cross-Validation (CV) important many tasks predictive mapping workflow, including feature selection (see CAST::ffs CAST::bss), hyperparameter tuning, area applicability estimation (see CAST::aoa), error profiling (see CAST::errorProfiles). Moreover, unfortunate case independent samples available estimate performance final predicted surfaces, can used last resort obtain estimate map error. objective vignette showcase CV methods implemented CAST. , work two datasets annual average air temperature fine Particulate Matter (PM2.5_{2.5}) air pollution continental Spain 2019, several predictors collected. details, check preprint dataset described detail.  Looking two maps can see air temperature stations nicely distributed around area interest, air pollution stations concentrated specific regions.","code":"# Read data temperature <- read_sf(\"https://github.com/carlesmila/RF-spatial-proxies/raw/main/data/temp/temp_train.gpkg\") pm25 <- read_sf(\"https://github.com/carlesmila/RF-spatial-proxies/raw/main/data/AP/PM25_train.gpkg\") spain <- read_sf(\"https://github.com/carlesmila/RF-spatial-proxies/raw/main/data/boundaries/spain.gpkg\")  # df versions temperature_df <- as.data.frame(st_drop_geometry(temperature)) pm25_df <- as.data.frame(st_drop_geometry(pm25))  # Plot them p1 <- ggplot() +   geom_sf(data = spain, fill = \"grey\", alpha = 0.1) +   geom_sf(data = temperature, aes(col = temp)) +   scale_colour_distiller(palette = \"RdYlBu\") +   theme_bw() +   labs(col = \"\") +   ggtitle(\"Average 2019 temperature (ºC)\") p2 <- ggplot() +   geom_sf(data = spain, fill = \"grey\", alpha = 0.1) +   geom_sf(data = pm25, aes(col = PM25)) +   scale_colour_viridis_c(option = \"cividis\") +   theme_bw() +   labs(col = \"\") +   ggtitle(expression(Average~2019~PM[2.5]~(mu*g/m^3))) grid.arrange(p1, p2, nrow=2)"},{"path":"https://hannameyer.github.io/CAST/articles/cast03-CV.html","id":"cross-validation-in-geographical-space","dir":"Articles","previous_headings":"","what":"Cross-validation in geographical space","title":"3. Nearest neighbor distance matching Cross-validation in CAST","text":"CAST, deal data indexed space, .e. data likely present dependencies comply assumptions independence standard CV methods Leave-One-(LOO) CV k-fold CV. Several CV approaches proposed try deal dependency train test data CV, spatial blocking buffering two used strategies. However, CV methods included CAST propose strategies focused ensuring independence, rather prediction-oriented: assess predictive conditions found using model specific prediction task, implement CV methods aim match . can define conditions? different options: consider geographical space focusing spatial locations training prediction points. Another possibility consider feature space, .e. covariate values train prediction set. yet possibilities, space-time mixture geographical feature space. now, though, let’s focus geographical space first.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast03-CV.html","id":"evaluation-of-spatial-predictive-conditions","dir":"Articles","previous_headings":"Cross-validation in geographical space","what":"Evaluation of spatial predictive conditions","title":"3. Nearest neighbor distance matching Cross-validation in CAST","text":"First , let’s see mean “predictive conditions” practice. geographical space, consider predictive conditions terms geographical nearest neighbour distances prediction training locations. CAST, distribution distances can easily explored using CAST::geodist function. CAST::geodist computes distribution nearest neighbour distances 1) prediction training points (prediction--sample), also 2) test points training points LOO CV (sample--sample), 3) test points training points given CV configuration (CV-distances). Ideally, like distribution distances CV resemble much possible distribution found prediction, since predictive conditions well-reflected CV. Let’s see practice environmental datasets random 5-fold CV, set modeldomain polygon study area prediction points regularly sampled:  see , temperature, although distribution geographical distances CV vs. prediction exactly match, overlap two substantial. PM2.5_{2.5} case, however, clustering stations make prediction--sample distances much longer found CV. words, parts prediction area PM2.5_{2.5} station nearby, happen nearly often random 5-fold CV. jump CV methods included CAST, worth considering alternative visualization distance distributions using Empirical Cumulative Density Functions (ECDF) rather density functions. ECDFs express, given distance, proportion distances distribution value equal lower value. Working ECDFs advantage choose additional parameters estimate density function, one building blocks proposed methods.","code":"# Random 5-fold CV fold5_temp <- createFolds(1:nrow(temperature), k=5, returnTrain=FALSE) fold5_pm25 <- createFolds(1:nrow(pm25), k=5, returnTrain=FALSE)  # Explore geographic predictive conditions predcond_temp <- geodist(temperature, modeldomain = spain, cvfolds = fold5_temp) predcond_pm25 <- geodist(pm25, modeldomain = spain, cvfolds = fold5_pm25)  # Plot density functions p1 <- plot(predcond_temp) + ggtitle(\"Temperature\") p2 <- plot(predcond_pm25) + ggtitle(expression(PM[2.5])) grid.arrange(p1, p2, nrow=2) # Plot ECDF functions p1 <- plot(predcond_temp, stat = \"ecdf\") + ggtitle(\"Temperature\") p2 <- plot(predcond_pm25, stat = \"ecdf\") + ggtitle(expression(PM[2.5])) grid.arrange(p1, p2, nrow=2)"},{"path":"https://hannameyer.github.io/CAST/articles/cast03-CV.html","id":"nndm-loo-cv-for-small-datasets","dir":"Articles","previous_headings":"Cross-validation in geographical space","what":"NNDM LOO CV for small datasets","title":"3. Nearest neighbor distance matching Cross-validation in CAST","text":"CV methods included CAST named Nearest Neighbour Distance Matching (NNDM), goal match ECDF nearest neighbour distances found CV ECDF found prediction just saw last plots. geographical space, match geographical Euclidean distances distributions depending CRS input data (examples projected CRS Euclidean distances used). first two NNDM methods NNDM LOO CV. greedy, iterative algorithm starts standard LOO CV repetitively compares prediction CV ECDFs. finds , given distance, value ECDF CV greater ECDF prediction, excludes points neighbourhood sample validated match achieved. practice, NNDM exclude points whenever training data clustered, generalize standard LOO CV data random regularly-distributed. details regarding algorithm simulation study evaluating performance can found article (also listed end vignette). Now, let’s run NNDM LOO CV algorithm two datasets check output. , use polygon Spain modeldomain 1,000 regularly sampled prediction points. However, also possible pass function set prediction points previously defined, example set raster cell centroids (check argument predpoints). run first temperature:  temperature, see ECDF NNDM LOO CV (CV-distances) similar LOO CV (sample--sample), virtually point excluded training data CV. data fairly regularly-distributed, hence ECDF LOO already lower prediction--sample ECDF. Now, air pollution:  , things quite different many points excluded order match prediction--sample ECDF successfully. pointed , caused clustered pattern training samples. can see iteration NNDM LOO CV looks like plotting :  CV iteration, many samples around point validated excluded. Now ’s time fit models estimate performance using 1) standard LOO CV, 2) NNDM LOO CV. Note LOO methods, need use CAST::global_validation, stacks observed --sample predicted values get relevant metrics single iteration. First, fit temperature models using Digital Elevation Model (DEM), NDVI Land Surface Temperature (LST) predictors: Next, run PM2.5_{2.5} models using population road density, nighttime lights (NTL), impervious surface (IMD) predictors: summarise CV results table: can observe statistics temperature models almost CV methods since NNDM algorithm detect spatial clustering thus returned configuration almost identical standard LOO CV. However, PM2.5_{2.5}, important differences performance estimated two CV methods, NNDM LOO CV suggesting much lower performance actual predictive conditions taken account.","code":"temp_nndm <- nndm(temperature, modeldomain = spain, samplesize = 1000) print(temp_nndm) ## nndm object ## Total number of points: 195 ## Mean number of training points: 193.91 ## Minimum number of training points: 191 plot(temp_nndm, type = \"simple\") pm25_nndm <- nndm(pm25, modeldomain = spain, samplesize = 1000) print(pm25_nndm) ## nndm object ## Total number of points: 124 ## Mean number of training points: 118.74 ## Minimum number of training points: 105 plot(pm25_nndm, type = \"simple\") # LOO CV temp_loo_ctrl <- trainControl(method=\"LOOCV\", savePredictions=TRUE) temp_loo_mod <- train(temperature_df[c(\"dem\", \"ndvi\", \"lst_day\",  \"lst_night\")],                       temperature_df[,\"temp\"],                       method=\"rf\", importance=FALSE,                       trControl=temp_loo_ctrl, ntree=100, tuneLength=1) temp_loo_res <- global_validation(temp_loo_mod)  # NNDM LOO CV temp_nndm_ctrl <- trainControl(method=\"cv\",                                 index=temp_nndm$indx_train, # Obs to fit the model to                                indexOut=temp_nndm$indx_test, # Obs to validate                                savePredictions=TRUE) temp_nndm_mod <- train(temperature_df[c(\"dem\", \"ndvi\", \"lst_day\",  \"lst_night\")],                        temperature_df[,\"temp\"],                        method=\"rf\", importance=FALSE,                        trControl=temp_nndm_ctrl, ntree=100, tuneLength=1) temp_nndm_res <- global_validation(temp_nndm_mod) # LOO CV pm25_loo_ctrl <- trainControl(method=\"LOOCV\", savePredictions=TRUE) pm25_loo_mod <- train(pm25_df[c(\"popdens\", \"primaryroads\", \"ntl\", \"imd\")],                       pm25_df[,\"PM25\"],                       method=\"rf\", importance=FALSE,                       trControl=pm25_loo_ctrl, ntree=100, tuneLength=1) pm25_loo_res <- global_validation(pm25_loo_mod)  # NNDM LOO CV pm25_nndm_ctrl <- trainControl(method=\"cv\",                                 index=pm25_nndm$indx_train, # Obs to fit the model to                                indexOut=pm25_nndm$indx_test, # Obs to validate                                savePredictions=TRUE) pm25_nndm_mod <- train(pm25_df[c(\"popdens\", \"primaryroads\", \"ntl\", \"imd\")],                        pm25_df[,\"PM25\"],                        method=\"rf\", importance=FALSE,                        trControl=pm25_nndm_ctrl, ntree=100, tuneLength=1) pm25_nndm_res <- global_validation(pm25_nndm_mod)"},{"path":"https://hannameyer.github.io/CAST/articles/cast03-CV.html","id":"knndm-cv-for-medium-and-large-datasets","dir":"Articles","previous_headings":"Cross-validation in geographical space","what":"kNNDM CV for medium and large datasets","title":"3. Nearest neighbor distance matching Cross-validation in CAST","text":"One straightforward limitation NNDM sample size. computationally expensive run NNDM algorithm large sample sizes, also model fitting runtime becomes long since, LOO CV, model observation needs fit. address limitation, developed k-fold version NNDM named kNNDM. goal kNNDM exactly NNDM LOO CV: match ECDF nearest neighbour distances CV ECDF found prediction. Nonetheless, means achieve different. kNNDM, use clustering algorithm (k-means agglomerative clustering currently implemented) cluster samples data qq groups based coordinates, merged final kk folds. smaller qq, stronger spatial structure CV . Among candidate qq, choose fold configuration offers best match two ECDFs. choose based Wassterstein’s W statistic, integral absolute value differences two ECDFs. lower W, better match. kNNDM yield configurations spatial structure clustered training samples generalize standard random k-fold CV whenever training data random regularly-distributed. want learn finer details kNNDM check simulation study evaluating performance, link document (also listed end vignette). Now let’s apply kNNDM data! First, temperature default clustering algorithm:  Similarly NNDM LOO CV, kNNDM find clustering generalizes random k-fold CV. Now let’s see happens PM2.5_{2.5} dataset:  case, find clustered samples kNNDM spatially clusters observations folds indicated number intermediate clusters qq. kNNDM configuration, match CV prediction distance distribution now seems quite good! Another thing can check whether clustering algorithms, different number folds, might yield better match. example, let’s try using k-means clustering PM2.5_{2.5} dataset: see W statistic larger , hence indicating quality match alternative kNNDM configuration actually poorer, stick first one. can easily visualize resulting 5-fold kNNDM configurations temperature PM2.5_{2.5}:  already knew, 5-fold kNNDM configuration temperature random whereas one PM2.5_{2.5} clear spatial pattern observations close space tend fold. Now, can finally run models (predictors ) compare results 5-fold random vs. kNNDM CV. , still use function CAST::global_validation compute statistics since 1) folds can unbalanced thus averaging might weight observations equally, 2) match ECDF based whole distribution fold. First, run temperature models: PM2.5_{2.5} models: summarize results table: , see pattern LOO CV results. Since temperature data aren’t clustered, kNNDM CV generalized random k-fold CV thus results methods similar. true PM2.5_{2.5}, kNNDM CV shows predictive conditions taken account, estimated performance lower.","code":"temp_knndm <- knndm(temperature, k = 5, modeldomain = spain, samplesize = 1000,                     clustering = \"hierarchical\", linkf = \"ward.D2\") print(temp_knndm) ## knndm object ## Space: geographical ## Clustering algorithm: hierarchical ## Intermediate clusters (q): random CV ## W statistic: 9384.0966 ## Number of folds: 5 ## Observations in each fold:  39 39 39 39 39 plot(temp_nndm, type = \"simple\") pm25_knndm <- knndm(pm25, k = 5, modeldomain = spain, samplesize = 1000,                      clustering = \"hierarchical\", linkf = \"ward.D2\") print(pm25_knndm) ## knndm object ## Space: geographical ## Clustering algorithm: hierarchical ## Intermediate clusters (q): 46 ## W statistic: 4919.5574 ## Number of folds: 5 ## Observations in each fold:  29 22 18 32 23 plot(pm25_knndm, type = \"simple\") pm25_knndm_v2 <- knndm(pm25, k = 5, modeldomain = spain, samplesize = 1000,                         clustering = \"kmeans\") print(pm25_knndm_v2) ## knndm object ## Space: geographical ## Clustering algorithm: kmeans ## Intermediate clusters (q): 35 ## W statistic: 5022.7072 ## Number of folds: 5 ## Observations in each fold:  21 23 17 40 23 # Random 5-fold CV temp_rndmk_ctrl <- trainControl(method=\"cv\", number=5, savePredictions=TRUE) temp_rndmk_mod <- train(temperature_df[c(\"dem\", \"ndvi\", \"lst_day\",  \"lst_night\")],                       temperature_df[,\"temp\"],                       method=\"rf\", importance=FALSE,                       trControl=temp_rndmk_ctrl, ntree=100, tuneLength=1) temp_rndmk_res <- global_validation(temp_rndmk_mod)  # kNNDM 5-fold CV temp_knndm_ctrl <- trainControl(method=\"cv\",                                  index=temp_knndm$indx_train,                                 savePredictions=TRUE) temp_knndm_mod <- train(temperature_df[c(\"dem\", \"ndvi\", \"lst_day\",  \"lst_night\")],                         temperature_df[,\"temp\"],                         method=\"rf\", importance=FALSE,                         trControl=temp_knndm_ctrl, ntree=100, tuneLength=1) temp_knndm_res <- global_validation(temp_knndm_mod) # Random 5-fold CV pm25_rndmk_ctrl <- trainControl(method=\"cv\", number=5, savePredictions=TRUE) pm25_rndmk_mod <- train(pm25_df[c(\"popdens\", \"primaryroads\", \"ntl\", \"imd\")],                         pm25_df[,\"PM25\"],                         method=\"rf\", importance=FALSE,                         trControl=pm25_rndmk_ctrl, ntree=100, tuneLength=1) pm25_rndmk_res <- global_validation(pm25_rndmk_mod)  # kNNDM 5-fold CV pm25_knndm_ctrl <- trainControl(method=\"cv\",                                  index=pm25_knndm$indx_train,                                 savePredictions=TRUE) pm25_knndm_mod <- train(pm25_df[c(\"popdens\", \"primaryroads\", \"ntl\", \"imd\")],                         pm25_df[,\"PM25\"],                         method=\"rf\", importance=FALSE,                         trControl=pm25_knndm_ctrl, ntree=100, tuneLength=1) pm25_knndm_res <- global_validation(pm25_knndm_mod)"},{"path":"https://hannameyer.github.io/CAST/articles/cast03-CV.html","id":"cross-validation-in-feature-space","dir":"Articles","previous_headings":"","what":"Cross-validation in feature space","title":"3. Nearest neighbor distance matching Cross-validation in CAST","text":"ideas underlying NNDM methods geographical space can transferred feature space just tweaks. current version CAST, implemented feature space experimental versions CAST::nndm CAST::knndm (see argument space= 'feature') running validation analyses verify performance variety settings. Stay tuned future version vignette overview feature space NNDM CV methods apply datasets!","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast03-CV.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further reading","title":"3. Nearest neighbor distance matching Cross-validation in CAST","text":"Milà, C., Mateu, J., Pebesma, E., Meyer, H. (2022): Nearest Neighbour Distance Matching Leave-One-Cross-Validation map validation. Methods Ecology Evolution 00, 1– 13. https://doi.org/10.1111/2041-210X.13851 Linnenbrink, J., Milà, C., Ludwig, M., Meyer, H.: kNNDM: k-fold Nearest Neighbour Distance Matching Cross-Validation map accuracy estimation, EGUsphere [preprint], https://doi.org/10.5194/egusphere-2023-1308","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"4. Area of applicability of spatial prediction models","text":"spatial predictive mapping, models often applied make predictions far beyond sampling locations (.e. field observations used map variable even global scale), new locations might considerably differ environmental properties. However, areas predictor space without support training data problematic. model enabled learn relationships environments predictions areas considered highly uncertain. CAST, implement methodology described Meyer&Pebesma (2021) estimate “area applicability” (AOA) (spatial) prediction models. AOA defined area enabled model learn relationships based training data, estimated cross-validation performance holds. delineate AOA, first dissimilarity index (DI) calculated based distances training data multidimensional predictor variable space. account relevance predictor variables responsible prediction patterns weight variables model-derived importance scores prior distance calculation. AOA derived applying threshold based DI observed training data using cross-validation. tutorial shows example estimate area applicability spatial prediction models. information see: Meyer, H., & Pebesma, E. (2021). Predicting unknown space? Estimating area applicability spatial prediction models. Methods Ecology Evolution, 12, 1620– 1633. [https://doi.org/10.1111/2041-210X.13650]","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"getting-started","dir":"Articles","previous_headings":"Introduction","what":"Getting started","title":"4. Area of applicability of spatial prediction models","text":"","code":"library(CAST) library(caret) library(terra) library(sf) library(viridis) library(gridExtra)"},{"path":[]},{"path":[]},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"generate-predictors","dir":"Articles","previous_headings":"Example 1: Using simulated data > Get data","what":"Generate Predictors","title":"4. Area of applicability of spatial prediction models","text":"predictor variables, set bioclimatic variables used (https://www.worldclim.org). tutorial, originally downloaded using getData function raster package cropped area central Europe. cropped data provided CAST package.","code":"predictors <- rast(system.file(\"extdata\",\"bioclim.tif\",package=\"CAST\")) plot(predictors,col=viridis(100))"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"generate-response","dir":"Articles","previous_headings":"Example 1: Using simulated data > Get data","what":"Generate Response","title":"4. Area of applicability of spatial prediction models","text":"able test reliability method, ’re using simulated prediction task. therefore simulate virtual response variable bioclimatic variables.","code":"generate_random_response <- function(raster, predictornames = names(raster), seed = sample(seq(1000), 1)){   operands_1 = c(\"+\", \"-\", \"*\", \"/\")   operands_2 = c(\"^1\",\"^2\")      expression <- paste(as.character(predictornames, sep=\"\"))   # assign random power to predictors   set.seed(seed)   expression <- paste(expression,                       sample(operands_2, length(predictornames), replace = TRUE),                       sep = \"\")      # assign random math function between predictors (expect after the last one)   set.seed(seed)   expression[-length(expression)] <- paste(expression[- length(expression)],                                            sample(operands_1, length(predictornames)-1, replace = TRUE),                                            sep = \" \")   print(paste0(expression, collapse = \" \"))   # collapse   e = paste0(\"raster$\", expression, collapse = \" \")      response = eval(parse(text = e))   names(response) <- \"response\"   return(response)    } response <- generate_random_response (predictors, seed = 10) ## [1] \"bio2^1 * bio5^1 + bio10^2 - bio13^2 / bio14^2 / bio19^1\" plot(response,col=viridis(100),main=\"virtual response\")"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"simulate-sampling-locations","dir":"Articles","previous_headings":"Example 1: Using simulated data > Get data","what":"Simulate sampling locations","title":"4. Area of applicability of spatial prediction models","text":"simulate typical prediction task, field sampling locations randomly selected. , randomly select 20 points. Note small data set, used avoid long computation times.","code":"mask <- predictors[[1]] values(mask)[!is.na(values(mask))] <- 1 mask <- st_as_sf(as.polygons(mask)) mask <- st_make_valid(mask) set.seed(15) samplepoints <- st_as_sf(st_sample(mask,20,\"random\"))  plot(response,col=viridis(100)) plot(samplepoints,col=\"red\",add=T,pch=3)"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"model-training","dir":"Articles","previous_headings":"Example 1: Using simulated data","what":"Model training","title":"4. Area of applicability of spatial prediction models","text":"Next, machine learning algorithm applied learn relationships predictors response.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"prepare-data","dir":"Articles","previous_headings":"Example 1: Using simulated data > Model training","what":"Prepare data","title":"4. Area of applicability of spatial prediction models","text":"Therefore, predictors response extracted sampling locations.","code":"trainDat <- extract(predictors,samplepoints,na.rm=FALSE) trainDat$response <- extract(response,samplepoints,na.rm=FALSE, ID=FALSE)$response trainDat <- na.omit(trainDat)"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"train-the-model","dir":"Articles","previous_headings":"Example 1: Using simulated data > Model training","what":"Train the model","title":"4. Area of applicability of spatial prediction models","text":"Random Forest applied machine learning algorithm (others can used well, long variable importance returned). model validated default cross-validation estimate prediction error.","code":"set.seed(10) model <- train(trainDat[,names(predictors)],                trainDat$response,                method=\"rf\",                importance=TRUE,                trControl = trainControl(method=\"cv\")) print(model) ## Random Forest  ##  ## 20 samples ##  6 predictor ##  ## No pre-processing ## Resampling: Cross-Validated (10 fold)  ## Summary of sample sizes: 18, 18, 18, 18, 18, 18, ...  ## Resampling results across tuning parameters: ##  ##   mtry  RMSE      Rsquared  MAE      ##   2     3854.481  1         3310.203 ##   4     3084.764  1         2675.126 ##   6     2960.314  1         2571.475 ##  ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 6."},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"variable-importance","dir":"Articles","previous_headings":"Example 1: Using simulated data > Model training","what":"Variable importance","title":"4. Area of applicability of spatial prediction models","text":"estimation AOA require importance individual predictor variables.","code":"plot(varImp(model,scale = F),col=\"black\")"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"predict-and-calculate-error","dir":"Articles","previous_headings":"Example 1: Using simulated data > Model training","what":"Predict and calculate error","title":"4. Area of applicability of spatial prediction models","text":"trained model used make predictions entire area interest. Since simulated area-wide response used, ’s possible tutorial compare predictions true reference.","code":"prediction <- predict(predictors,model,na.rm=T) truediff <- abs(prediction-response) plot(rast(list(prediction,response)),main=c(\"prediction\",\"reference\"))"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"aoa-and-lpd-calculation","dir":"Articles","previous_headings":"Example 1: Using simulated data","what":"AOA and LPD calculation","title":"4. Area of applicability of spatial prediction models","text":"visualization shows predictions made model. next step, DI, LPD AOA calculated. Note possible calculate AOA without calculating LPD, can time consuming (arg: LPD = FALSE). AOA calculation takes model input extract importance predictors, used weights multidimensional distance calculation. Note AOA can also calculated without trained model (.e. using training data new data ). case predictor variables treated equally important (unless weights given form table). Plotting aoa object shows distribution DI values within training data DI new data. important output aoa function three raster data sets: first DI normalized weighted minimum distance nearest training data point divided average distance within training data. second AOA derived DI using threshold. threshold (outlier-removed) maximum DI observed training data DI training data calculated considering cross-validation folds. last LPD absolute count training data points, within AOA threshold new prediction location. LPD 0 therefore signifies prediction location outside AOA >1 inside AOA. specific LPD values give good indication coverage similar training data new prediction location. used threshold relevant information DI LPD training data returned parameters list entry. can plot DI LPD well predictions within AOA:  patterns DI LPD general agreement true prediction error. high DI values present Alps, covered training data feature distinct environmental conditions. Since DI values areas threshold, regard area outside AOA.","code":"AOA <- aoa(predictors, model, LPD = TRUE, verbose = FALSE) class(AOA) ## [1] \"aoa\" names(AOA) ## [1] \"parameters\" \"DI\"         \"AOA\"        \"LPD\" print(AOA) ## DI: ## class       : SpatRaster  ## dimensions  : 102, 123, 1  (nrow, ncol, nlyr) ## resolution  : 14075.98, 14075.98  (x, y) ## extent      : 3496791, 5228136, 2143336, 3579086  (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs  ## source(s)   : memory ## varname     : bioclim  ## name        :       DI  ## min value   : 0.000000  ## max value   : 3.408739  ## LPD: ## class       : SpatRaster  ## dimensions  : 102, 123, 1  (nrow, ncol, nlyr) ## resolution  : 14075.98, 14075.98  (x, y) ## extent      : 3496791, 5228136, 2143336, 3579086  (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs  ## source(s)   : memory ## varname     : bioclim  ## name        : LPD  ## min value   :   0  ## max value   :   9  ## AOA: ## class       : SpatRaster  ## dimensions  : 102, 123, 1  (nrow, ncol, nlyr) ## resolution  : 14075.98, 14075.98  (x, y) ## extent      : 3496791, 5228136, 2143336, 3579086  (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs  ## source(s)   : memory ## varname     : bioclim  ## name        : AOA  ## min value   :   0  ## max value   :   1  ##  ##  ## Predictor Weights: ##       bio2     bio5    bio10   bio13 bio14 bio19 ## 1 3.746582 17.92456 17.04888 2.15925     0     0 ##  ##  ## AOA Threshold: 0.3221291 plot(AOA) plot(truediff,col=viridis(100),main=\"true prediction error\") plot(AOA$DI,col=viridis(100),main=\"DI\") plot(AOA$LPD,col=viridis(100),main=\"LPD\") plot(prediction, col=viridis(100),main=\"prediction for AOA\") plot(AOA$AOA,col=c(\"grey\",\"transparent\"),add=T,plg=list(x=\"topleft\",box.col=\"black\",bty=\"o\",title=\"AOA\"))"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"aoa-for-spatially-clustered-data","dir":"Articles","previous_headings":"Example 1: Using simulated data","what":"AOA for spatially clustered data?","title":"4. Area of applicability of spatial prediction models","text":"example randomly distributed training samples. However, sampling locations might also highly clustered space. case, random cross-validation meaningful (see e.g. Meyer et al. 2018, Meyer et al. 2019, Valavi et al. 2019, Roberts et al. 2018, Pohjankukka et al. 2017, Brenning 2012) random cross-validation case lead apparently high prediction performance, apply small AOA, threshold DI based distance nearest data point within training data (usually small data clustered). assess model performance larger areas, cross-validation based spatial CV, e.g. leave-cluster-approach (see vignette spatial CV package detailed discussion cross validation strategies), also AOA estimation based distances nearest data point located spatial cluster. show looks like, use 15 spatial locations simulate 5 data points around location.  first train model (case) inappropriate random cross-validation. …model based leave-cluster-cross-validation. AOA calculated (comparison) using model validated random cross-validation, second taking spatial clusters account calculating threshold based minimum distances nearest training point located cluster. done aoa function, folds used cross-validation automatically extracted model.  Note AOA much larger spatial CV approach. However, spatial cross-validation error considerably larger, hence also area error applies larger. random cross-validation performance high, however, area performance applies small. fact also apparent plot aoa objects display distributions DI training data well DI new data. random CV predictionDI larger AOA threshold determined trainDI. Using spatial CV, predictionDI well within DI training sample.","code":"set.seed(25) samplepoints <- clustered_sample(mask,75,15,radius=25000)  plot(response,col=viridis(100)) plot(samplepoints,col=\"red\",add=T,pch=3) trainDat <- extract(predictors,samplepoints,na.rm=FALSE) trainDat$response <- extract(response,samplepoints,na.rm=FALSE)$response trainDat <- data.frame(trainDat,samplepoints) trainDat <- na.omit(trainDat) set.seed(10) model_random <- train(trainDat[,names(predictors)],                trainDat$response,                method=\"rf\",                importance=TRUE,                trControl = trainControl(method=\"cv\")) prediction_random <- predict(predictors,model_random,na.rm=TRUE) print(model_random) ## Random Forest  ##  ## 75 samples ##  6 predictor ##  ## No pre-processing ## Resampling: Cross-Validated (10 fold)  ## Summary of sample sizes: 68, 67, 68, 68, 68, 67, ...  ## Resampling results across tuning parameters: ##  ##   mtry  RMSE       Rsquared   MAE      ##   2     1088.1729  0.9956237  790.2191 ##   4      921.1760  0.9968527  717.5578 ##   6      922.1137  0.9967308  715.7016 ##  ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 4. folds <- CreateSpacetimeFolds(trainDat, spacevar=\"parent\",k=10) set.seed(15) model <- train(trainDat[,names(predictors)],                  trainDat$response,                      method=\"rf\",                  importance=TRUE,                  tuneGrid = expand.grid(mtry = c(2:length(names(predictors)))),                  trControl = trainControl(method=\"cv\",index=folds$index))   print(model) ## Random Forest  ##  ## 75 samples ##  6 predictor ##  ## No pre-processing ## Resampling: Cross-Validated (10 fold)  ## Summary of sample sizes: 70, 70, 65, 70, 70, 65, ...  ## Resampling results across tuning parameters: ##  ##   mtry  RMSE      Rsquared   MAE      ##   2     3227.421  0.9382904  2740.529 ##   3     2761.092  0.9433621  2396.941 ##   4     2677.002  0.9570317  2349.310 ##   5     2587.598  0.9486190  2282.064 ##   6     2494.756  0.9425158  2190.718 ##  ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 6. prediction <- predict(predictors,model,na.rm=TRUE) AOA_spatial <- aoa(predictors, model, LPD = TRUE, verbose = FALSE)  AOA_random <- aoa(predictors, model_random, LPD = FALSE, verbose = FALSE) plot(AOA_spatial$DI,col=viridis(100),main=\"DI\") plot(AOA_spatial$LPD,col=viridis(100),main=\"LPD\") plot(prediction, col=viridis(100),main=\"prediction for AOA \\n(spatial CV error applies)\") plot(AOA_spatial$AOA,col=c(\"grey\",\"transparent\"),add=TRUE,plg=list(x=\"topleft\",box.col=\"black\",bty=\"o\",bg = 'white',title=\"AOA\")) plot(prediction_random, col=viridis(100),main=\"prediction for AOA \\n(random CV error applies)\") plot(AOA_random$AOA,col=c(\"grey\",\"transparent\"),add=TRUE,plg=list(x=\"topleft\",box.col=\"black\",bty=\"o\",bg = 'white',title=\"AOA\")) grid.arrange(plot(AOA_spatial, variable = \"DI\") + ggplot2::ggtitle(\"Spatial CV\"),              plot(AOA_random, variable = \"DI\") + ggplot2::ggtitle(\"Random CV\"), ncol = 2)"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"comparison-prediction-error-with-model-error","dir":"Articles","previous_headings":"Example 1: Using simulated data","what":"Comparison prediction error with model error","title":"4. Area of applicability of spatial prediction models","text":"Since used simulated response variable, can now compare prediction error within AOA model error, assuming model error applies inside AOA outside. results indicate high agreement model CV error (RMSE) true prediction RMSE. case , random well spatial model.","code":"###for the spatial CV: RMSE(values(prediction)[values(AOA_spatial$AOA)==1],      values(response)[values(AOA_spatial$AOA)==1]) ## [1] 3308.808 RMSE(values(prediction)[values(AOA_spatial$AOA)==0],      values(response)[values(AOA_spatial$AOA)==0]) ## [1] 10855.31 model$results ##   mtry     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD ## 1    2 3227.421 0.9382904 2740.529 2335.609 0.06774290 2168.398 ## 2    3 2761.092 0.9433621 2396.941 1823.280 0.07190124 1674.310 ## 3    4 2677.002 0.9570317 2349.310 1690.078 0.04208035 1549.323 ## 4    5 2587.598 0.9486190 2282.064 1595.276 0.05220790 1410.225 ## 5    6 2494.756 0.9425158 2190.718 1507.700 0.07431001 1289.825 ###and for the random CV: RMSE(values(prediction_random)[values(AOA_random$AOA)==1],      values(response)[values(AOA_random$AOA)==1]) ## [1] 1365.329 RMSE(values(prediction_random)[values(AOA_random$AOA)==0],      values(response)[values(AOA_random$AOA)==0]) ## [1] 3959.685 model_random$results ##   mtry      RMSE  Rsquared      MAE   RMSESD  RsquaredSD    MAESD ## 1    2 1088.1729 0.9956237 790.2191 595.2632 0.004567068 407.8754 ## 2    4  921.1760 0.9968527 717.5578 437.1580 0.002792369 311.1915 ## 3    6  922.1137 0.9967308 715.7016 412.0427 0.002498990 306.1030"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"relationship-between-the-dilpd-and-the-performance-measure","dir":"Articles","previous_headings":"Example 1: Using simulated data","what":"Relationship between the DI/LPD and the performance measure","title":"4. Area of applicability of spatial prediction models","text":"relationship error DI LPD can used limit predictions area (within AOA) required performance (e.g. RMSE, R2, Kappa, Accuracy) applies. can done using result errorProfiles used relationship analyzed window DI/LPD values. corresponding model (: shape constrained additive models default: Monotone increasing P-splines dimension basis used represent smooth term 6 2nd order penalty.) can used estimate performance pixel level, allows limiting predictions using threshold. Note used multi-purpose CV estimate relationship DI RMSE (see details paper).","code":"DI_RMSE_relation <- errorProfiles(model, AOA_spatial$parameters, multiCV=TRUE,                                     window.size = 5, length.out = 5, variable = \"DI\") plot(DI_RMSE_relation) LPD_RMSE_relation <- errorProfiles(model, AOA_spatial$parameters, multiCV=TRUE,                                     window.size = 5, length.out = 5, variable = \"LPD\") plot(LPD_RMSE_relation) DI_expected_RMSE = terra::predict(AOA_spatial$DI, DI_RMSE_relation) LPD_expected_RMSE = terra::predict(AOA_spatial$LPD, LPD_RMSE_relation)  # account for multiCV changing the DI threshold DI_updated_AOA = AOA_spatial$DI > attr(DI_RMSE_relation, \"AOA_threshold\")  # account for multiCV changing the DI threshold LPD_updated_AOA = AOA_spatial$DI > attr(LPD_RMSE_relation, \"AOA_threshold\")    plot(DI_expected_RMSE,col=viridis(100),main=\"DI expected RMSE\") plot(DI_updated_AOA, col=c(\"grey\",\"transparent\"),add=TRUE,plg=list(x=\"topleft\",box.col=\"black\",bty=\"o\",bg = 'white',title=\"AOA\")) plot(LPD_expected_RMSE,col=viridis(100),main=\"LPD expected RMSE\") plot(LPD_updated_AOA, col=c(\"grey\",\"transparent\"),add=TRUE,plg=list(x=\"topleft\",box.col=\"black\",bty=\"o\",bg = 'white',title=\"AOA\"))"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"example-2-a-real-world-example","dir":"Articles","previous_headings":"","what":"Example 2: A real-world example","title":"4. Area of applicability of spatial prediction models","text":"example used simulated data allows analyze reliability AOA. However, simulated area-wide response available usual prediction tasks. Therefore, second example AOA estimated dataset point observations reference .","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"data-and-preprocessing","dir":"Articles","previous_headings":"Example 2: A real-world example","what":"Data and preprocessing","title":"4. Area of applicability of spatial prediction models","text":", work cookfarm dataset, described e.g. Gasch et al 2015. dataset included CAST re-structured dataset. Find details also vignette “Introduction CAST”. use soil moisture (VW) response variable . Hence, ’re aiming making spatial continuous prediction based limited measurements data loggers.","code":"data(cookfarm) # calculate average of VW for each sampling site: dat <- aggregate(cookfarm[,c(\"VW\",\"Easting\",\"Northing\")],by=list(as.character(cookfarm$SOURCEID)),mean) # create sf object from the data: pts <- st_as_sf(dat,coords=c(\"Easting\",\"Northing\"))  ##### Extract Predictors for the locations of the sampling points studyArea <- rast(system.file(\"extdata\",\"predictors_2012-03-25.tif\",package=\"CAST\")) st_crs(pts) <- crs(studyArea) trainDat <- extract(studyArea,pts,na.rm=FALSE) pts$ID <- 1:nrow(pts) trainDat <- merge(trainDat,pts,by.x=\"ID\",by.y=\"ID\") # The final training dataset with potential predictors and VW: head(trainDat) ##   ID      DEM      TWI  BLD       NDRE.M   NDRE.Sd     Bt Easting Northing ## 1  1 788.1906 4.304258 1.42 -0.051189531 0.2506899 0.0000  493384  5180587 ## 2  2 788.3813 3.863605 1.29 -0.046459336 0.1754623 0.0000  493514  5180567 ## 3  3 790.5244 3.947488 1.36 -0.040845532 0.2225785 0.0000  493574  5180577 ## 4  4 775.7229 5.395786 1.55 -0.004329725 0.2099845 0.0501  493244  5180587 ## 5  5 796.7618 3.534822 1.31  0.027252737 0.2002646 0.0000  493624  5180607 ## 6  6 795.8370 3.815516 1.40 -0.123434804 0.2180606 0.0000  493694  5180607 ##   MinT_wrcc MaxT_wrcc Precip_cum  cday Precip_wrcc Group.1        VW ## 1       1.1      36.2       10.6 15425           0  CAF003 0.2894505 ## 2       1.1      36.2       10.6 15425           0  CAF007 0.2705531 ## 3       1.1      36.2       10.6 15425           0  CAF009 0.2629683 ## 4       1.1      36.2       10.6 15425           0  CAF019 0.2993580 ## 5       1.1      36.2       10.6 15425           0  CAF031 0.2664754 ## 6       1.1      36.2       10.6 15425           0  CAF033 0.2650177 ##                   geometry ## 1 POINT (493383.1 5180586) ## 2 POINT (493510.7 5180568) ## 3 POINT (493574.6 5180573) ## 4 POINT (493246.6 5180590) ## 5 POINT (493628.3 5180612) ## 6 POINT (493692.2 5180610)"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"model-training-and-prediction","dir":"Articles","previous_headings":"Example 2: A real-world example","what":"Model training and prediction","title":"4. Area of applicability of spatial prediction models","text":"set variables used predictors VW random Forest model. model validated leave one cross-validation. Note model performance low, due small dataset used (small dataset low ability predictors model VW).","code":"predictors <- c(\"DEM\",\"NDRE.Sd\",\"TWI\",\"Bt\") response <- \"VW\"  model <- train(trainDat[,predictors],trainDat[,response],                method=\"rf\",tuneLength=3,importance=TRUE,                trControl=trainControl(method=\"LOOCV\")) model ## Random Forest  ##  ## 42 samples ##  4 predictor ##  ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation  ## Summary of sample sizes: 41, 41, 41, 41, 41, 41, ...  ## Resampling results across tuning parameters: ##  ##   mtry  RMSE        Rsquared    MAE        ##   2     0.04043599  0.02194320  0.03272203 ##   3     0.04130865  0.01657314  0.03310715 ##   4     0.04090019  0.02358726  0.03295413 ##  ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 2."},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"prediction","dir":"Articles","previous_headings":"Example 2: A real-world example > Model training and prediction","what":"Prediction","title":"4. Area of applicability of spatial prediction models","text":"Next, model used make predictions entire study area.","code":"#Predictors: plot(stretch(studyArea[[predictors]])) #prediction: prediction <- predict(studyArea,model,na.rm=TRUE)"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"aoa-estimation","dir":"Articles","previous_headings":"Example 2: A real-world example","what":"AOA estimation","title":"4. Area of applicability of spatial prediction models","text":"Next ’re limiting predictions AOA. Predictions outside AOA excluded.","code":"AOA <- aoa(studyArea, model, LPD = TRUE, verbose = FALSE)  #### Plot results: plot(AOA$DI,col=viridis(100),main=\"DI with sampling locations (red)\") plot(pts,zcol=\"ID\",col=\"red\",add=TRUE)  plot(AOA$LPD,col=viridis(100),main=\"LPD with sampling locations (red)\") plot(pts,zcol=\"ID\",col=\"red\",add=TRUE)  plot(prediction, col=viridis(100),main=\"prediction for AOA \\n(LOOCV error applies)\") plot(AOA$AOA,col=c(\"grey\",\"transparent\"),add=TRUE,plg=list(x=\"topleft\",box.col=\"black\",bty=\"o\", bg = 'white', title=\"AOA\"))"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"final-notes","dir":"Articles","previous_headings":"","what":"Final notes","title":"4. Area of applicability of spatial prediction models","text":"AOA estimated based training data new data (.e. raster group entire area interest). trained model used getting variable importance needed weight predictor variables. can given table either, approach can used packages caret well. Knowledge AOA important predictions used baseline decision making subsequent environmental modelling. suggest AOA provided alongside prediction map complementary communication validation performances.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast04-AOA-tutorial.html","id":"further-reading","dir":"Articles","previous_headings":"Final notes","what":"Further reading","title":"4. Area of applicability of spatial prediction models","text":"Meyer, H., & Pebesma, E. (2022): Machine learning-based global maps ecological variables challenge assessing . Nature Communications. [https://doi.org/10.1038/s41467-022-29838-9] Meyer, H., & Pebesma, E. (2021). Predicting unknown space? Estimating area applicability spatial prediction models. Methods Ecology Evolution, 12, 1620– 1633. [https://doi.org/10.1111/2041-210X.13650] Tutorial (https://youtu./EyP04zLe9qo) Lecture (https://youtu./OoNH6Nl-X2s) recording OpenGeoHub summer school 2020 area applicability. well talk OpenGeoHub summer school 2022: https://doi.org/10.5446/59412","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast05-parallel.html","id":"forward-feature-selection","dir":"Articles","previous_headings":"","what":"Forward feature selection","title":"5. Improve computation time of CAST methods","text":"forward feature selection basically brute force grid search best predictor variable combination, ffs large number machine learning models trained compare performance. model trainings can run parallel. Unix machines, implemented using mclapply parallel-package. Simply set cores argument value > 1 use multiple cores model training. windows machines, get warning Parallel computations ffs implemented unix systems. cores set 1. Regardless system, can speed model training ffs, caret::train parallelization option trains models multiple cores. code adapated https://topepo.github.io/caret/parallel-processing Another option use different model algorithm. example, ranger package fast multicore implementation random forest model.","code":"data(\"splotdata\") spatial_cv = CreateSpacetimeFolds(splotdata, spacevar = \"Biome\", k = 5) ctrl <- trainControl(method=\"cv\",index = spatial_cv$index)  ffsmodel <- ffs(predictors = splotdata[,6:16],                 response = splotdata$Species_richness,                 tuneLength = 1,                 method = \"rf\",                 trControl = ctrl,                 ntree = 20,                 seed = 1,                 cores = 4) library(doParallel)  data(\"splotdata\") spatial_cv = CreateSpacetimeFolds(splotdata, spacevar = \"Biome\", k = 4) ctrl <- trainControl(method=\"cv\",index = spatial_cv$index)  cl <- makeCluster(4) registerDoParallel(cl) ffsmodel <- ffs(predictors = splotdata[,6:16],                 response = splotdata$Species_richness,                 tuneLength = 4,                 method = \"rf\",                 trControl = ctrl,                 ntree = 20,                 seed = 1,                 cores = 1) stopCluster(cl) ffsmodel <- ffs(predictors = splotdata[,6:16],                 response = splotdata$Species_richness,                 method = \"ranger\")"},{"path":"https://hannameyer.github.io/CAST/articles/cast05-parallel.html","id":"area-of-applicability","dir":"Articles","previous_headings":"","what":"Area of applicability","title":"5. Improve computation time of CAST methods","text":"Estimating Area Applicability (AOA) can computationally expansive method based finding minimum distances nearest neighbors. can divide computation three major chunks: feature space distance training data. nearest feature space distance training data different CV folds. distance new locations (pixel) nearest training data feature space. (1.) (2.) needed derive AOA threshold solely based training data cross validation configuration. computed together trainDI function. (3.) needed calculate dissimilarity index predictor stack aoa function. hood, aoa function calls trainDI ’s first step calculates DI predictor stack. can speed computation times aoa splitting two processes calculate DI multiple raster tiles .","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast05-parallel.html","id":"traindi","dir":"Articles","previous_headings":"Area of applicability","what":"trainDI","title":"5. Improve computation time of CAST methods","text":"Calling trainDI just needs model object, also contains training data, cross-validation information variable importance. result trainDI contains necessary information training data model aoa function needs compute DI new locations.","code":"library(CAST) library(caret) library(terra) library(sf)  data(\"splotdata\") predictors <- rast(system.file(\"extdata\",\"predictors_chile.tif\",package=\"CAST\")) splotdata <- st_drop_geometry(splotdata) set.seed(10) model <- train(splotdata[,names(predictors)],                splotdata$Species_richness,                method=\"rf\",                tuneLength = 1,                importance=TRUE,                ntrees = 20,                trControl = trainControl(method=\"cv\"), number = 5) prediction <- predict(predictors, model, na.rm=TRUE) tdi = trainDI(model, verbose = FALSE) print(tdi) ## DI of 703 observation  ## Predictors: bio_1 bio_4 bio_5 bio_6 bio_8 bio_9 bio_12 bio_13 bio_14 bio_15 elev  ##  ## AOA Threshold: 0.1903705 class(tdi) ## [1] \"trainDI\" str(tdi) ## List of 10 ##  $ train             :'data.frame':  703 obs. of  11 variables: ##   ..$ bio_1 : num [1:703] 17.6 17.4 18.3 18 18.8 ... ##   ..$ bio_4 : num [1:703] 464 460 473 486 478 ... ##   ..$ bio_5 : num [1:703] 30.5 30.1 31.4 31.2 32 ... ##   ..$ bio_6 : num [1:703] 3.6 3.5 4.2 4.2 4.4 ... ##   ..$ bio_8 : num [1:703] 23.2 22.9 24 23.9 24.5 ... ##   ..$ bio_9 : num [1:703] 11.7 11.5 12.2 11.8 12.6 ... ##   ..$ bio_12: num [1:703] 760 731 810 842 853 842 823 525 524 554 ... ##   ..$ bio_13: num [1:703] 119 115 129 140 134 133 129 92 90 90 ... ##   ..$ bio_14: num [1:703] 9 9 12 13 12 12 12 7 6 5 ... ##   ..$ bio_15: num [1:703] 68.9 68.9 66.9 65.5 68.3 ... ##   ..$ elev  : num [1:703] 416 468 232 129 231 243 193 188 261 468 ... ##  $ weight            :'data.frame':  1 obs. of  11 variables: ##   ..$ bio_1 : num 20.4 ##   ..$ bio_4 : num 20.3 ##   ..$ bio_5 : num 14.6 ##   ..$ bio_6 : num 22.6 ##   ..$ bio_8 : num 18.5 ##   ..$ bio_9 : num 15.1 ##   ..$ bio_12: num 25.9 ##   ..$ bio_13: num 17.3 ##   ..$ bio_14: num 18.6 ##   ..$ bio_15: num 18.2 ##   ..$ elev  : num 14.3 ##  $ variables         : chr [1:11] \"bio_1\" \"bio_4\" \"bio_5\" \"bio_6\" ... ##  $ catvars           : chr(0)  ##  $ scaleparam        :List of 4 ##   ..$ dim          : int [1:2] 703 11 ##   ..$ dimnames     :List of 2 ##   .. ..$ : chr [1:703] \"1\" \"2\" \"3\" \"4\" ... ##   .. ..$ : chr [1:11] \"bio_1\" \"bio_4\" \"bio_5\" \"bio_6\" ... ##   ..$ scaled:center: Named num [1:11] 14.26 171.83 21.77 6.71 14.4 ... ##   .. ..- attr(*, \"names\")= chr [1:11] \"bio_1\" \"bio_4\" \"bio_5\" \"bio_6\" ... ##   ..$ scaled:scale : Named num [1:11] 7.27 156.2 8.2 6.67 8.28 ... ##   .. ..- attr(*, \"names\")= chr [1:11] \"bio_1\" \"bio_4\" \"bio_5\" \"bio_6\" ... ##  $ trainDist_avrg    : num [1:703] 87.2 87.2 87.1 86.9 87.6 ... ##  $ trainDist_avrgmean: num 81.5 ##  $ trainDI           : num [1:703] 0.0222 0.0173 0.0258 0.0409 0.0115 ... ##  $ threshold         : Named num 0.19 ##   ..- attr(*, \"names\")= chr \"75%\" ##  $ method            : chr \"L2\" ##  - attr(*, \"class\")= chr \"trainDI\" # you can save the trainDI object for later application saveRDS(tdi, \"path/to/file\")"},{"path":"https://hannameyer.github.io/CAST/articles/cast05-parallel.html","id":"aoa-for-multiple-rasters","dir":"Articles","previous_headings":"Area of applicability","what":"AOA for multiple rasters","title":"5. Improve computation time of CAST methods","text":"large area interest want derive Area Applicability, computing AOA entire area might possible takes long time. can use tile based approach apply trainDI object previous step multiple rasters .  Use trainDI argument aoa function specify, want use previously computed trainDI object.  can now run aoa function parallel different tiles! course can use favorite parallel backend task, use mclapply parallel package.  even larger tasks might useful save tiles hard-drive load one one avoid filling RAM.","code":"r1 = crop(predictors, c(-75.66667, -67, -30, -17.58333)) r2 = crop(predictors, c(-75.66667, -67, -45, -30)) r3 = crop(predictors, c(-75.66667, -67, -55.58333, -45))   plot(r1[[1]],main = \"Tile 1\") plot(r2[[1]],main = \"Tile 2\") plot(r3[[1]],main = \"Tile 3\") aoa_r1 = aoa(newdata = r1, trainDI = tdi)  plot(r1[[1]], main = \"Tile 1: Predictors\") plot(aoa_r1$DI, main = \"Tile 1: DI\") plot(aoa_r1$AOA, main = \"Tile 1: AOA\") library(parallel)  tiles_aoa = mclapply(list(r1, r2, r3), function(tile){   aoa(newdata = tile, trainDI = tdi)    }, mc.cores = 3) plot(tiles_aoa[[1]]$AOA, main = \"Tile 1\") plot(tiles_aoa[[2]]$AOA, main = \"Tile 2\") plot(tiles_aoa[[3]]$AOA, main = \"Tile 3\") # Simple Example Code for raster tiles on the hard drive  tiles = list.files(\"path/to/tiles\", full.names = TRUE)  tiles_aoa = mclapply(tiles, function(tile){   current = terra::rast(tile)   aoa(newdata = current, trainDI = model_random_trainDI)    }, mc.cores = 3)"},{"path":"https://hannameyer.github.io/CAST/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Hanna Meyer. Maintainer, author. Carles Milà. Author. Marvin Ludwig. Author. Jan Linnenbrink. Author. Fabian Schumacher. Author. Philipp Otto. Contributor. Chris Reudenbach. Contributor. Thomas Nauss. Contributor. Edzer Pebesma. Contributor.","code":""},{"path":"https://hannameyer.github.io/CAST/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Meyer H, Milà C, Ludwig M, Linnenbrink J, Schumacher F (2024). CAST: 'caret' Applications Spatial-Temporal Models. R package version 1.0.2, https://hannameyer.github.io/CAST/, https://github.com/HannaMeyer/CAST.","code":"@Manual{,   title = {CAST: 'caret' Applications for Spatial-Temporal Models},   author = {Hanna Meyer and Carles Milà and Marvin Ludwig and Jan Linnenbrink and Fabian Schumacher},   year = {2024},   note = {R package version 1.0.2,     https://hannameyer.github.io/CAST/},   url = {https://github.com/HannaMeyer/CAST}, }"},{"path":"https://hannameyer.github.io/CAST/index.html","id":"cast-caret-applications-for-spatio-temporal-models-","dir":"","previous_headings":"","what":"caret Applications for Spatial-Temporal Models","title":"caret Applications for Spatial-Temporal Models","text":"Supporting functionality run ‘caret’ spatial spatial-temporal data. ‘caret’ frequently used package model training prediction using machine learning. CAST includes functions improve spatial spatial-temporal modelling tasks using ‘caret’. decrease spatial overfitting improve model performances, package implements forward feature selection selects suitable predictor variables view contribution spatial spatio-temporal model performance. CAST includes functionality estimate (spatial) area applicability prediction models. Note: developer version CAST can found https://github.com/HannaMeyer/CAST. CRAN Version can found https://CRAN.R-project.org/package=CAST figure shows simple workflow spatial prediction mapping workflow, indicating function CAST can used different steps support spatial prediction.","code":""},{"path":"https://hannameyer.github.io/CAST/index.html","id":"package-website","dir":"","previous_headings":"","what":"Package Website","title":"caret Applications for Spatial-Temporal Models","text":"https://hannameyer.github.io/CAST/","code":""},{"path":"https://hannameyer.github.io/CAST/index.html","id":"tutorials","dir":"","previous_headings":"","what":"Tutorials","title":"caret Applications for Spatial-Temporal Models","text":"CAST package training assessment spatial prediction models R Introduction CAST Visualization nearest neighbor distance distributions Nearest neighbor distance matching Cross-validation CAST Area applicability spatial prediction models Improve computation time CAST methods talk OpenGeoHub summer school 2019 spatial validation variable selection: https://www.youtube.com/watch?v=mkHlmYEzsVQ. Tutorial (https://youtu./EyP04zLe9qo) Lecture (https://youtu./OoNH6Nl-X2s) recording OpenGeoHub summer school 2020 area applicability. well talk OpenGeoHub summer school 2021: https://av.tib.eu/media/54879 Talk tutorial OpenGeoHub 2022 summer school Machine learning-based maps environment - challenges extrapolation overfitting, including discussions area applicability nearest neighbor distance matching cross-validation (https://doi.org/10.5446/59412).","code":""},{"path":"https://hannameyer.github.io/CAST/index.html","id":"scientific-documentation-of-the-methods","dir":"","previous_headings":"","what":"Scientific documentation of the methods","title":"caret Applications for Spatial-Temporal Models","text":"Meyer, H., Ludwig, L., Milà, C., Linnenbrink, J., Schumacher, F. (2024): CAST package training assessment spatial prediction models R. arXiv, https://doi.org/10.48550/arXiv.2404.06978.","code":""},{"path":"https://hannameyer.github.io/CAST/index.html","id":"spatial-cross-validation","dir":"","previous_headings":"Scientific documentation of the methods","what":"Spatial cross-validation","title":"caret Applications for Spatial-Temporal Models","text":"Milà, C., Mateu, J., Pebesma, E., Meyer, H. (2022): Nearest Neighbour Distance Matching Leave-One-Cross-Validation map validation. Methods Ecology Evolution 00, 1– 13. https://doi.org/10.1111/2041-210X.13851 Linnenbrink, J., Milà, C., Ludwig, M., Meyer, H.: kNNDM (2023): k-fold Nearest Neighbour Distance Matching Cross-Validation map accuracy estimation. EGUsphere [preprint]. https://doi.org/10.5194/egusphere-2023-1308 Meyer, H., Reudenbach, C., Hengl, T., Katurji, M., Nauss, T. (2018): Improving performance spatio-temporal machine learning models using forward feature selection target-oriented validation. Environmental Modelling & Software, 101, 1-9. https://doi.org/10.1016/j.envsoft.2017.12.001","code":""},{"path":"https://hannameyer.github.io/CAST/index.html","id":"spatial-variable-selection","dir":"","previous_headings":"Scientific documentation of the methods","what":"Spatial variable selection","title":"caret Applications for Spatial-Temporal Models","text":"Meyer, H., Reudenbach, C., Hengl, T., Katurji, M., Nauss, T. (2018): Improving performance spatio-temporal machine learning models using forward feature selection target-oriented validation. Environmental Modelling & Software, 101, 1-9. https://doi.org/10.1016/j.envsoft.2017.12.001 Meyer, H., Reudenbach, C., Wöllauer, S., Nauss, T. (2019): Importance spatial predictor variable selection machine learning applications - Moving data reproduction spatial prediction. Ecological Modelling. 411. https://doi.org/10.1016/j.ecolmodel.2019.108815","code":""},{"path":"https://hannameyer.github.io/CAST/index.html","id":"area-of-applicability","dir":"","previous_headings":"Scientific documentation of the methods","what":"Area of applicability","title":"caret Applications for Spatial-Temporal Models","text":"Meyer, H., Pebesma, E. (2021). Predicting unknown space? Estimating area applicability spatial prediction models. Methods Ecology Evolution, 12, 1620– 1633. https://doi.org/10.1111/2041-210X.13650","code":""},{"path":"https://hannameyer.github.io/CAST/index.html","id":"applications-and-use-cases","dir":"","previous_headings":"Scientific documentation of the methods","what":"Applications and use cases","title":"caret Applications for Spatial-Temporal Models","text":"Meyer, H., Pebesma, E. (2022): Machine learning-based global maps ecological variables challenge assessing . Nature Communications, 13. https://www.nature.com/articles/s41467-022-29838-9 Ludwig, M., Moreno-Martinez, ., Hoelzel, N., Pebesma, E., Meyer, H. (2023): Assessing improving transferability current global spatial prediction models. Global Ecology Biogeography. https://doi.org/10.1111/geb.13635. Milà, C., Ludwig, M., Pebesma, E., Tonne, C., Meyer, H.: Random forests spatial proxies environmental modelling: opportunities pitfalls, EGUsphere [preprint]. https://doi.org/10.5194/egusphere-2024-138, 2024.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CAST.html","id":null,"dir":"Reference","previous_headings":"","what":"'caret' Applications for Spatial-Temporal Models — CAST","title":"'caret' Applications for Spatial-Temporal Models — CAST","text":"Supporting functionality run 'caret' spatial spatial-temporal data. 'caret' frequently used package model training prediction using machine learning. CAST includes functions improve spatial-temporal modelling tasks using 'caret'. includes newly suggested 'Nearest neighbor distance matching' cross-validation estimate performance spatial prediction models allows spatial variable selection selects suitable predictor variables view contribution spatial model performance. CAST includes functionality estimate (spatial) area applicability prediction models analysing similarity new data training data. Methods described Meyer et al. (2018); Meyer et al. (2019); Meyer Pebesma (2021); Milà et al. (2022); Meyer Pebesma (2022); Linnenbrink et al. (2023). package described detail Meyer et al. (2024).","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CAST.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"'caret' Applications for Spatial-Temporal Models — CAST","text":"'caret' Applications Spatio-Temporal models","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CAST.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"'caret' Applications for Spatial-Temporal Models — CAST","text":"Meyer, H., Ludwig, L., Milà, C., Linnenbrink, J., Schumacher, F. (2024): CAST package training assessment spatial prediction models R. arXiv, https://doi.org/10.48550/arXiv.2404.06978. Linnenbrink, J., Milà, C., Ludwig, M., Meyer, H.: kNNDM: k-fold Nearest Neighbour Distance Matching Cross-Validation map accuracy estimation, EGUsphere [preprint], https://doi.org/10.5194/egusphere-2023-1308, 2023. Milà, C., Mateu, J., Pebesma, E., Meyer, H. (2022): Nearest Neighbour Distance Matching Leave-One-Cross-Validation map validation. Methods Ecology Evolution 00, 1– 13. Meyer, H., Pebesma, E. (2022): Machine learning-based global maps ecological variables challenge assessing . Nature Communications. 13. Meyer, H., Pebesma, E. (2021): Predicting unknown space? Estimating area applicability spatial prediction models. Methods Ecology Evolution. 12, 1620– 1633. Meyer, H., Reudenbach, C., Wöllauer, S., Nauss, T. (2019): Importance spatial predictor variable selection machine learning applications - Moving data reproduction spatial prediction. Ecological Modelling. 411, 108815. Meyer, H., Reudenbach, C., Hengl, T., Katurji, M., Nauß, T. (2018): Improving performance spatio-temporal machine learning models using forward feature selection target-oriented validation. Environmental Modelling & Software 101: 1-9.","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/CAST.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"'caret' Applications for Spatial-Temporal Models — CAST","text":"Hanna Meyer, Carles Milà, Marvin Ludwig, Jan Linnenbrink, Fabian Schumacher","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Space-time Folds — CreateSpacetimeFolds","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"Create spatial, temporal spatio-temporal Folds cross validation based pre-defined groups","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"","code":"CreateSpacetimeFolds(   x,   spacevar = NA,   timevar = NA,   k = 10,   class = NA,   seed = sample(1:1000, 1) )"},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"x data.frame containing spatio-temporal data spacevar Character indicating column x identifies spatial units (e.g. ID weather stations) timevar Character indicating column x identifies temporal units (e.g. day year) k numeric. Number folds. spacevar timevar NA leave one location leave one time step cv performed, set k number unique spatial temporal units. class Character indicating column x identifies class unit (e.g. land cover) seed numeric. See ?seed","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"list contains list model training list model validation can directly used \"index\" \"indexOut\" caret's trainControl function","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"function creates train test sets taking (spatial /temporal) groups account. contrast nndm, requires groups already defined (e.g. spatial clusters blocks temporal units). Using \"class\" helpful case data clustered space categorical. E.g case land cover classifications training data come training polygons. case data split way entire polygons held back (spacevar=\"polygonID\") time distribution classes similar fold (class=\"LUC\").","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"Standard k-fold cross-validation can lead considerable misinterpretation spatial-temporal modelling tasks. function can used prepare Leave-Location-, Leave-Time-Leave-Location--Time-cross-validation target-oriented validation strategies spatial-temporal prediction tasks. See Meyer et al. (2018) information.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"Meyer, H., Reudenbach, C., Hengl, T., Katurji, M., Nauß, T. (2018): Improving performance spatio-temporal machine learning models using forward feature selection target-oriented validation. Environmental Modelling & Software 101: 1-9.","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"Hanna Meyer","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"","code":"if (FALSE) { # \\dontrun{ data(cookfarm) ### Prepare for 10-fold Leave-Location-and-Time-Out cross validation indices <- CreateSpacetimeFolds(cookfarm,\"SOURCEID\",\"Date\") str(indices) ### Prepare for 10-fold Leave-Location-Out cross validation indices <- CreateSpacetimeFolds(dat,spacevar=\"SOURCEID\") str(indices) ### Prepare for leave-One-Location-Out cross validation indices <- CreateSpacetimeFolds(dat,spacevar=\"SOURCEID\",     k=length(unique(dat$SOURCEID))) str(indices) } # }"},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":null,"dir":"Reference","previous_headings":"","what":"Area of Applicability — aoa","title":"Area of Applicability — aoa","text":"function estimates Dissimilarity Index (DI) derived Area Applicability (AOA) spatial prediction models considering distance new data (.e. SpatRaster spatial predictors used models) predictor variable space data used model training. Predictors can weighted based internal variable importance machine learning algorithm used model training. AOA derived applying threshold DI (outlier-removed) maximum DI cross-validated training data. Optionally, local point density calculated indicates number similar training data points DI threshold.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Area of Applicability — aoa","text":"","code":"aoa(   newdata,   model = NA,   trainDI = NA,   train = NULL,   weight = NA,   variables = \"all\",   CVtest = NULL,   CVtrain = NULL,   method = \"L2\",   useWeight = TRUE,   useCV = TRUE,   LPD = FALSE,   maxLPD = 1,   indices = FALSE,   verbose = TRUE )"},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Area of Applicability — aoa","text":"newdata SpatRaster, stars object data.frame containing data model meant make predictions . model train object created caret used extract weights (based variable importance) well cross-validation folds. See examples case model available models trained via e.g. mlr3. trainDI trainDI object. Optional trainDI calculated beforehand. train data.frame containing data used model training. Optional. required model given weight data.frame containing weights variable. Optional. required model given. variables character vector predictor variables. \"\" variables model used model given train dataset. CVtest list vector. Either list element contains data points used testing cross validation iteration (.e. held back data). vector contains ID fold training point. required model given. CVtrain list. element contains data points used training cross validation iteration (.e. held back data). required model given required CVtrain opposite CVtest (.e. data point used testing, used training). Relevant data points excluded, e.g. using nndm. method Character. Method used distance calculation. Currently euclidean distance (L2) Mahalanobis distance (MD) implemented L2 tested. Note MD takes considerably longer. useWeight Logical. model given. Weight variables according importance model? useCV Logical. model given. Use CV folds calculate DI threshold? LPD Logical. Indicates whether local point density calculated . maxLPD numeric integer. LPD = TRUE. Number nearest neighbors considered calculation LPD. Either define number 0 1 use percentage number training samples LPD calculation whole number larger 1 smaller number training samples. CAUTION! training samples considered, fitted relationship LPD error metric make sense (@seealso DItoErrormetric) indices logical. Calculate indices training data points responsible LPD new prediction location? Output matrix dimensions num(raster_cells) x maxLPD. row holds indices training data points relevant specific LPD value location. Can used combination exploreAOA(aoa) function CASTvis package better visual interpretation results. Note matrix can quite big examples high resolution larger number training samples, can cause memory issues. verbose Logical. Print progress ?","code":""},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Area of Applicability — aoa","text":"object class aoa containing: parameters object class trainDI. see trainDI DI SpatRaster, stars object data frame. Dissimilarity index newdata LPD SpatRaster, stars object data frame. Local Point Density newdata. AOA SpatRaster, stars object data frame. Area Applicability newdata. AOA values 0 (outside AOA) 1 (inside AOA)","code":""},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Area of Applicability — aoa","text":"Dissimilarity Index (DI), Local Data Point Density (LPD) corresponding Area Applicability (AOA) calculated. variables factors, dummy variables created prior weighting distance calculation. Interpretation results: location similar properties training data low distance predictor variable space (DI towards 0) locations different properties high DI. easier interpretation see normalize_DI See Meyer Pebesma (2021) full documentation methodology.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Area of Applicability — aoa","text":"classification models used, currently variable importance can automatically retrieved models trained via train(predictors,response) via formula-interface. fixed.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Area of Applicability — aoa","text":"Meyer, H., Pebesma, E. (2021): Predicting unknown space? Estimating area applicability spatial prediction models. Methods Ecology Evolution 12: 1620-1633. doi:10.1111/2041-210X.13650","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Area of Applicability — aoa","text":"Hanna Meyer, Fabian Schumacher","code":""},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Area of Applicability — aoa","text":"","code":"if (FALSE) { # \\dontrun{ library(sf) library(terra) library(caret) library(viridis)  # prepare sample data: data(cookfarm) dat <- aggregate(cookfarm[,c(\"VW\",\"Easting\",\"Northing\")],    by=list(as.character(cookfarm$SOURCEID)),mean) pts <- st_as_sf(dat,coords=c(\"Easting\",\"Northing\"),crs=26911) pts$ID <- 1:nrow(pts) set.seed(100) pts <- pts[1:30,] studyArea <- rast(system.file(\"extdata\",\"predictors_2012-03-25.tif\",package=\"CAST\"))[[1:8]] trainDat <- extract(studyArea,pts,na.rm=FALSE) trainDat <- merge(trainDat,pts,by.x=\"ID\",by.y=\"ID\")  # visualize data spatially: plot(studyArea) plot(studyArea$DEM) plot(pts[,1],add=TRUE,col=\"black\")  # train a model: set.seed(100) variables <- c(\"DEM\",\"NDRE.Sd\",\"TWI\") model <- train(trainDat[,which(names(trainDat)%in%variables)], trainDat$VW, method=\"rf\", importance=TRUE, tuneLength=1, trControl=trainControl(method=\"cv\",number=5,savePredictions=T)) print(model) #note that this is a quite poor prediction model prediction <- predict(studyArea,model,na.rm=TRUE) plot(varImp(model,scale=FALSE))  #...then calculate the AOA of the trained model for the study area: AOA <- aoa(studyArea, model) plot(AOA) plot(AOA$AOA) #... or if preferred calculate the aoa and the LPD of the study area: AOA <- aoa(studyArea, model, LPD = TRUE, maxLPD = 1) plot(AOA$LPD)  #### #The AOA can also be calculated without a trained model. #All variables are weighted equally in this case: #### AOA <- aoa(studyArea,train=trainDat,variables=variables)   #### # The AOA can also be used for models trained via mlr3 (parameters have to be assigned manually): ####  library(mlr3) library(mlr3learners) library(mlr3spatial) library(mlr3spatiotempcv) library(mlr3extralearners)  # initiate and train model: train_df <- trainDat[, c(\"DEM\",\"NDRE.Sd\",\"TWI\", \"VW\")] backend <- as_data_backend(train_df) task <- as_task_regr(backend, target = \"VW\") lrn <- lrn(\"regr.randomForest\", importance = \"mse\") lrn$train(task)  # cross-validation folds rsmp_cv <- rsmp(\"cv\", folds = 5L)$instantiate(task)  ## predict: prediction <- predict(studyArea,lrn$model,na.rm=TRUE)  ### Estimate AOA AOA <- aoa(studyArea,            train = as.data.frame(task$data()),            variables = task$feature_names,            weight = data.frame(t(lrn$importance())),            CVtest = rsmp_cv$instance[order(row_id)]$fold)  } # }"},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":null,"dir":"Reference","previous_headings":"","what":"Best subset feature selection — bss","title":"Best subset feature selection — bss","text":"Evaluate combinations predictors model training","code":""},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Best subset feature selection — bss","text":"","code":"bss(   predictors,   response,   method = \"rf\",   metric = ifelse(is.factor(response), \"Accuracy\", \"RMSE\"),   maximize = ifelse(metric == \"RMSE\", FALSE, TRUE),   globalval = FALSE,   trControl = caret::trainControl(),   tuneLength = 3,   tuneGrid = NULL,   seed = 100,   verbose = TRUE,   ... )"},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Best subset feature selection — bss","text":"predictors see train response see train method see train metric see train maximize see train globalval Logical. models evaluated based 'global' performance? See global_validation trControl see train tuneLength see train tuneGrid see train seed random number verbose Logical. information progress printed? ... arguments passed classification regression routine (randomForest).","code":""},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Best subset feature selection — bss","text":"list class train. Beside usual train content object contains vector \"selectedvars\" \"selectedvars_perf\" give best variables selected well corresponding performance. also contains \"perf_all\" gives performance model runs.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Best subset feature selection — bss","text":"bss alternative ffs ideal training set small. Models iteratively fitted using different combinations predictor variables. Hence, 2^X models calculated. try running bss large datasets computation time much higher compared ffs. internal cross validation can run parallel. See information parallel processing carets train functions details.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Best subset feature selection — bss","text":"variable selection particularly suitable spatial cross validations variable selection MUST based performance model predicting new spatial units. Note bss slow since combinations variables tested. time efficient alternative forward feature selection (ffs).","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Best subset feature selection — bss","text":"Hanna Meyer","code":""},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Best subset feature selection — bss","text":"","code":"if (FALSE) { # \\dontrun{ data(iris) bssmodel <- bss(iris[,1:4],iris$Species) bssmodel$perf_all plot(bssmodel) } # }"},{"path":"https://hannameyer.github.io/CAST/reference/clustered_sample.html","id":null,"dir":"Reference","previous_headings":"","what":"Clustered samples simulation — clustered_sample","title":"Clustered samples simulation — clustered_sample","text":"simple procedure simulate clustered points based two-step sampling.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/clustered_sample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clustered samples simulation — clustered_sample","text":"","code":"clustered_sample(sarea, nsamples, nparents, radius)"},{"path":"https://hannameyer.github.io/CAST/reference/clustered_sample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clustered samples simulation — clustered_sample","text":"sarea polygon. Area samples simulated. nsamples integer. Number samples simulated. nparents integer. Number parents. radius integer. Radius buffer around parent offspring simulation.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/clustered_sample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clustered samples simulation — clustered_sample","text":"sf object simulated points parent point belongs .","code":""},{"path":"https://hannameyer.github.io/CAST/reference/clustered_sample.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Clustered samples simulation — clustered_sample","text":"simple procedure simulate clustered points based two-step sampling. First, pre-specified number parents simulated using random sampling. parent, `(nsamples-nparents)/nparents` simulated within radius parent point using random sampling.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/clustered_sample.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Clustered samples simulation — clustered_sample","text":"Carles Milà","code":""},{"path":"https://hannameyer.github.io/CAST/reference/clustered_sample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clustered samples simulation — clustered_sample","text":"","code":"# Simulate 100 points in a 100x100 square with 5 parents and a radius of 10. library(sf) #> Linking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE library(ggplot2)  set.seed(1234) simarea <- list(matrix(c(0,0,0,100,100,100,100,0,0,0), ncol=2, byrow=TRUE)) simarea <- sf::st_polygon(simarea) simpoints <- clustered_sample(simarea, 100, 5, 10) simpoints$parent <- as.factor(simpoints$parent) ggplot() +     geom_sf(data = simarea, alpha = 0) +     geom_sf(data = simpoints, aes(col = parent))"},{"path":"https://hannameyer.github.io/CAST/reference/cookfarm.html","id":null,"dir":"Reference","previous_headings":"","what":"Cookfarm soil logger data — cookfarm","title":"Cookfarm soil logger data — cookfarm","text":"spatio-temporal data soil properties associated predictors Cookfarm Washington, USA. data subset cookfarm dataset provided GSIF package.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/cookfarm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cookfarm soil logger data — cookfarm","text":"","code":"data(cookfarm)"},{"path":"https://hannameyer.github.io/CAST/reference/cookfarm.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Cookfarm soil logger data — cookfarm","text":"sf data.frame 128545 rows 17 columns: SOURCEID ID logger VW Response Variable - Soil Moisture altitude Measurement depth VW Date, cdata Measurement Date, Cumulative Date Easting, Northing Location Coordinates (EPSG:26911) DEM, TWI, NDRE.M, NDRE.Sd, Precip_wrcc, MaxT_wrcc, MinT_wrcc, Precip_cum Predictor Variables","code":""},{"path":"https://hannameyer.github.io/CAST/reference/cookfarm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Cookfarm soil logger data — cookfarm","text":"Gash et al. 2015 - Spatio-temporal interpolation soil water, temperature, electrical conductivity 3D + T: Cook Agronomy Farm data set doi:10.1016/j.spasta.2015.04.001 Meyer et al. 2018 - Improving performance spatio-temporal machine learning models using forward feature selection target-oriented validation doi:10.1016/j.envsoft.2017.12.001","code":""},{"path":"https://hannameyer.github.io/CAST/reference/errorProfiles.html","id":null,"dir":"Reference","previous_headings":"","what":"Model and inspect the relationship between the prediction error and measures of dissimilarities and distances — errorProfiles","title":"Model and inspect the relationship between the prediction error and measures of dissimilarities and distances — errorProfiles","text":"Performance metrics calculated moving windows dissimilarity values based cross-validated training data","code":""},{"path":"https://hannameyer.github.io/CAST/reference/errorProfiles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model and inspect the relationship between the prediction error and measures of dissimilarities and distances — errorProfiles","text":"","code":"errorProfiles(   model,   trainDI = NULL,   locations = NULL,   variable = \"DI\",   multiCV = FALSE,   length.out = 10,   window.size = 5,   calib = \"scam\",   method = \"L2\",   useWeight = TRUE,   k = 6,   m = 2 )"},{"path":"https://hannameyer.github.io/CAST/reference/errorProfiles.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model and inspect the relationship between the prediction error and measures of dissimilarities and distances — errorProfiles","text":"model model used get AOA trainDI result trainDI aoa object aoa locations Optional. sf object training data used model. used variable==\"geodist\". Note must order model$trainingData. variable Character. dissimilarity distance measure use error metric. Current options \"DI\" \"LPD\" multiCV Logical. Re-run model fitting validation different CV strategies. See details. length.Numeric. used multiCV=TRUE. Number cross-validation folds. See details. window.size Numeric. Size moving window. See rollapply. calib Character. Function model DI/LPD~performance relationship. Currently lm scam supported method Character. Method used distance calculation. Currently euclidean distance (L2) Mahalanobis distance (MD) implemented L2 tested. Note MD takes considerably longer. See ?aoa explanation useWeight Logical. model given. Weight variables according importance model? k Numeric. See mgcv::s m Numeric. See mgcv::s","code":""},{"path":"https://hannameyer.github.io/CAST/reference/errorProfiles.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model and inspect the relationship between the prediction error and measures of dissimilarities and distances — errorProfiles","text":"scam, linear model exponential model","code":""},{"path":"https://hannameyer.github.io/CAST/reference/errorProfiles.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model and inspect the relationship between the prediction error and measures of dissimilarities and distances — errorProfiles","text":"multiCV=TRUE model re-fitted validated length.new cross-validations cross-validation folds defined clusters predictor space, ranging three clusters LOOCV. Hence, large range dissimilarity values created cross-validation. AOA threshold based calibration data multiple CV larger original AOA threshold (likely extrapolation situations created CV), AOA threshold changes accordingly. See Meyer Pebesma (2021) full documentation methodology.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/errorProfiles.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Model and inspect the relationship between the prediction error and measures of dissimilarities and distances — errorProfiles","text":"Meyer, H., Pebesma, E. (2021): Predicting unknown space? Estimating area applicability spatial prediction models. doi:10.1111/2041-210X.13650","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/errorProfiles.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Model and inspect the relationship between the prediction error and measures of dissimilarities and distances — errorProfiles","text":"Hanna Meyer, Marvin Ludwig, Fabian Schumacher","code":""},{"path":"https://hannameyer.github.io/CAST/reference/errorProfiles.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model and inspect the relationship between the prediction error and measures of dissimilarities and distances — errorProfiles","text":"","code":"if (FALSE) { # \\dontrun{ library(CAST) library(sf) library(terra) library(caret)  data(splotdata) predictors <- terra::rast(system.file(\"extdata\",\"predictors_chile.tif\", package=\"CAST\"))  model <- caret::train(st_drop_geometry(splotdata)[,6:16], splotdata$Species_richness,    ntree = 10, trControl = trainControl(method = \"cv\", savePredictions = TRUE))  AOA <- aoa(predictors, model, LPD = TRUE, maxLPD = 1)  ### DI ~ error errormodel_DI <- errorProfiles(model, AOA, variable = \"DI\") plot(errormodel_DI) summary(errormodel_DI)  expected_error_DI = terra::predict(AOA$DI, errormodel_DI) plot(expected_error_DI)  ### LPD ~ error errormodel_LPD <- errorProfiles(model, AOA, variable = \"LPD\") plot(errormodel_LPD) summary(errormodel_DI)  expected_error_LPD = terra::predict(AOA$LPD, errormodel_LPD) plot(expected_error_LPD)  ### geodist ~ error errormodel_geodist = errorProfiles(model, locations=splotdata, variable = \"geodist\") plot(errormodel_geodist) summary(errormodel_DI)  dist <- terra::distance(predictors[[1]],vect(splotdata)) names(dist) <- \"geodist\" expected_error_DI <- terra::predict(dist, errormodel_geodist) plot(expected_error_DI)   ### with multiCV = TRUE (for DI ~ error) errormodel_DI = errorProfiles(model, AOA, multiCV = TRUE, length.out = 3, variable = \"DI\") plot(errormodel_DI)  expected_error_DI = terra::predict(AOA$DI, errormodel_DI) plot(expected_error_DI)  # mask AOA based on new threshold from multiCV mask_aoa = terra::mask(expected_error_DI, AOA$DI > attr(errormodel_DI, 'AOA_threshold'),   maskvalues = 1) plot(mask_aoa) } # }"},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":null,"dir":"Reference","previous_headings":"","what":"Forward feature selection — ffs","title":"Forward feature selection — ffs","text":"simple forward feature selection algorithm","code":""},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forward feature selection — ffs","text":"","code":"ffs(   predictors,   response,   method = \"rf\",   metric = ifelse(is.factor(response), \"Accuracy\", \"RMSE\"),   maximize = ifelse(metric == \"RMSE\", FALSE, TRUE),   globalval = FALSE,   withinSE = FALSE,   minVar = 2,   trControl = caret::trainControl(),   tuneLength = 3,   tuneGrid = NULL,   seed = sample(1:1000, 1),   verbose = TRUE,   cores = 1,   ... )"},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forward feature selection — ffs","text":"predictors see train response see train method see train metric see train maximize see train globalval Logical. models evaluated based 'global' performance? See global_validation withinSE Logical Models selected better currently best models Standard error minVar Numeric. Number variables combine first selection. See Details. trControl see train tuneLength see train tuneGrid see train seed random number used model training verbose Logical. information progress printed? cores Numeric. > 2, mclapply used. see mclapply ... arguments passed classification regression routine (randomForest).","code":""},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forward feature selection — ffs","text":"list class train. Beside usual train content object contains vector \"selectedvars\" \"selectedvars_perf\" give order best variables selected well corresponding performance (starting first two variables). also contains \"perf_all\" gives performance model runs.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forward feature selection — ffs","text":"Models two predictors first trained using possible pairs predictor variables. best model initial models kept. basis best model predictor variables iteratively increased remaining variables tested improvement currently best model. process stops none remaining variables increases model performance added current best model. forward feature selection can run parallel forking Linux systems (mclapply). fork computes model, drastically speeds runtime - especially initial predictor search. internal cross validation can run parallel systems. See information parallel processing carets train functions details. Using withinSE favour models less variables probably shorten calculation time Per Default, ffs starts possible 2-pair combinations. minVar allows start selection 2 variables, e.g. minVar=3 starts ffs testing combinations 3 (instead 2) variables first increasing number. important e.g. neural networks often make sense two variables. also relevant assumed optimal variables can found 2 considered time.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Forward feature selection — ffs","text":"variable selection particularly suitable spatial cross validations variable selection MUST based performance model predicting new spatial units. See Meyer et al. (2018) Meyer et al. (2019) details.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forward feature selection — ffs","text":"Gasch, C.K., Hengl, T., Gräler, B., Meyer, H., Magney, T., Brown, D.J. (2015): Spatio-temporal interpolation soil water, temperature, electrical conductivity 3D+T: Cook Agronomy Farm data set. Spatial Statistics 14: 70-90. Meyer, H., Reudenbach, C., Hengl, T., Katurji, M., Nauß, T. (2018): Improving performance spatio-temporal machine learning models using forward feature selection target-oriented validation. Environmental Modelling & Software 101: 1-9.  doi:10.1016/j.envsoft.2017.12.001 Meyer, H., Reudenbach, C., Wöllauer, S., Nauss, T. (2019): Importance spatial predictor variable selection machine learning applications - Moving data reproduction spatial prediction. Ecological Modelling. 411, 108815. doi:10.1016/j.ecolmodel.2019.108815 . Ludwig, M., Moreno-Martinez, ., Hölzel, N., Pebesma, E., Meyer, H. (2023): Assessing improving transferability current global spatial prediction models. Global Ecology Biogeography. doi:10.1111/geb.13635 .","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Forward feature selection — ffs","text":"Hanna Meyer","code":""},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forward feature selection — ffs","text":"","code":"if (FALSE) { # \\dontrun{ data(splotdata) ffsmodel <- ffs(splotdata[,6:12], splotdata$Species_richness, ntree = 20)  ffsmodel$selectedvars ffsmodel$selectedvars_perf plot(ffsmodel) #or only selected variables: plot(ffsmodel,plotType=\"selected\") } # }  # or perform model with target-oriented validation (LLO CV) #the example is described in Gasch et al. (2015). The ffs approach for this dataset is described in #Meyer et al. (2018). Due to high computation time needed, only a small and thus not robust example #is shown here.  if (FALSE) { # \\dontrun{ # run the model on three cores (see vignette for details): library(doParallel) library(lubridate) cl <- makeCluster(3) registerDoParallel(cl)  #load and prepare dataset: data(cookfarm) trainDat <- cookfarm[cookfarm$altitude==-0.3&   year(cookfarm$Date)==2012&week(cookfarm$Date)%in%c(13:14),]  #visualize dataset: ggplot(data = trainDat, aes(x=Date, y=VW)) + geom_line(aes(colour=SOURCEID))  #create folds for Leave Location Out Cross Validation: set.seed(10) indices <- CreateSpacetimeFolds(trainDat,spacevar = \"SOURCEID\",k=3) ctrl <- trainControl(method=\"cv\",index = indices$index)  #define potential predictors: predictors <- c(\"DEM\",\"TWI\",\"BLD\",\"Precip_cum\",\"cday\",\"MaxT_wrcc\", \"Precip_wrcc\",\"NDRE.M\",\"Bt\",\"MinT_wrcc\",\"Northing\",\"Easting\")  #run ffs model with Leave Location out CV set.seed(10) ffsmodel <- ffs(trainDat[,predictors],trainDat$VW,method=\"rf\", tuneLength=1,trControl=ctrl) ffsmodel plot(ffsmodel) #or only selected variables: plot(ffsmodel,plotType=\"selected\")  #compare to model without ffs: model <- train(trainDat[,predictors],trainDat$VW,method=\"rf\", tuneLength=1, trControl=ctrl) model stopCluster(cl) } # }  if (FALSE) { # \\dontrun{ ## on linux machines, you can also run the ffs in parallel with forks: data(\"splotdata\") spatial_cv = CreateSpacetimeFolds(splotdata, spacevar = \"Biome\", k = 5) ctrl <- trainControl(method=\"cv\",index = spatial_cv$index)  ffsmodel <- ffs(predictors = splotdata[,6:16],                response = splotdata$Species_richness,                tuneLength = 1,                method = \"rf\",                trControl = ctrl,                ntree = 20,                seed = 1,                cores = 4) } # }"},{"path":"https://hannameyer.github.io/CAST/reference/geodist.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate euclidean nearest neighbor distances in geographic space or feature space — geodist","title":"Calculate euclidean nearest neighbor distances in geographic space or feature space — geodist","text":"Calculates nearest neighbor distances geographic space feature space training data well training data prediction locations. Optional, nearest neighbor distances training data test data training data CV iterations computed.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/geodist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate euclidean nearest neighbor distances in geographic space or feature space — geodist","text":"","code":"geodist(   x,   modeldomain = NULL,   type = \"geo\",   cvfolds = NULL,   cvtrain = NULL,   testdata = NULL,   preddata = NULL,   samplesize = 2000,   sampling = \"regular\",   variables = NULL,   timevar = NULL,   time_unit = \"auto\" )"},{"path":"https://hannameyer.github.io/CAST/reference/geodist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate euclidean nearest neighbor distances in geographic space or feature space — geodist","text":"x object class sf, training data locations modeldomain SpatRaster, stars sf object defining prediction area (see Details) type \"geo\" \"feature\". distance computed geographic space normalized multivariate predictor space (see Details) cvfolds optional. list vector. Either list element contains data points used testing cross validation iteration (.e. held back data). vector contains ID fold training point. See e.g. ?createFolds ?CreateSpacetimeFolds ?nndm cvtrain optional. List row indices x fit model CV iteration. cvtrain null cvfolds , samples included cvfolds used training data testdata optional. object class sf: Point data used independent validation preddata optional. object class sf: Point data indicating locations within modeldomain used target prediction points. Useful prediction objective subset locations within modeldomain rather whole area. samplesize numeric. many prediction samples used? sampling character. draw prediction samples? See spsample. Use sampling = \"Fibonacci\" global applications. variables character vector defining predictor variables used type=\"feature. provided variables included modeldomain used. timevar optional. character. Column indicates date. used type=\"time\". time_unit optional. Character. Unit temporal distances See ?difftime.used type=\"time\".","code":""},{"path":"https://hannameyer.github.io/CAST/reference/geodist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate euclidean nearest neighbor distances in geographic space or feature space — geodist","text":"data.frame containing distances. Unit returned geographic distances meters. attributes contain W statistic prediction area either sample data, CV folds test data. See details.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/geodist.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate euclidean nearest neighbor distances in geographic space or feature space — geodist","text":"modeldomain sf polygon raster defines prediction area. function takes regular point sample (amount defined samplesize) spatial extent.     type = \"feature\", argument modeldomain (provided also testdata /preddata) include predictors. Predictor values x, testdata preddata optional modeldomain raster.     provided extracted modeldomain rasterStack. predictors categorical (.e., class factor character), gower distances used.     W statistic describes match distributions. See Linnenbrink et al (2023) details.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/geodist.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Calculate euclidean nearest neighbor distances in geographic space or feature space — geodist","text":"See Meyer Pebesma (2022) application plotting function","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/geodist.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate euclidean nearest neighbor distances in geographic space or feature space — geodist","text":"Hanna Meyer, Edzer Pebesma, Marvin Ludwig, Jan Linnenbrink","code":""},{"path":"https://hannameyer.github.io/CAST/reference/geodist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate euclidean nearest neighbor distances in geographic space or feature space — geodist","text":"","code":"if (FALSE) { # \\dontrun{ library(CAST) library(sf) library(terra) library(caret) library(rnaturalearth) library(ggplot2)  data(splotdata) studyArea <- rnaturalearth::ne_countries(continent = \"South America\", returnclass = \"sf\")  ########### Distance between training data and new data: dist <- geodist(splotdata, studyArea) # With density functions plot(dist) # Or ECDFs (relevant for nndm and knnmd methods) plot(dist, stat=\"ecdf\")  ########### Distance between training data, new data and test data (here Chile): plot(splotdata[,\"Country\"]) dist <- geodist(splotdata[splotdata$Country != \"Chile\",], studyArea,                 testdata = splotdata[splotdata$Country == \"Chile\",]) plot(dist)  ########### Distance between training data, new data and CV folds: folds <- createFolds(1:nrow(splotdata), k=3, returnTrain=FALSE) dist <- geodist(x=splotdata, modeldomain=studyArea, cvfolds=folds) # Using density functions plot(dist) # Using ECDFs (relevant for nndm and knnmd methods) plot(dist, stat=\"ecdf\")  ########### Distances in the feature space: predictors <- terra::rast(system.file(\"extdata\",\"predictors_chile.tif\", package=\"CAST\")) dist <- geodist(x = splotdata,                 modeldomain = predictors,                 type = \"feature\",                 variables = c(\"bio_1\",\"bio_12\", \"elev\")) plot(dist)  dist <- geodist(x = splotdata[splotdata$Country != \"Chile\",],                 modeldomain = predictors, cvfolds = folds,                 testdata = splotdata[splotdata$Country == \"Chile\",],                 type = \"feature\",                 variables=c(\"bio_1\",\"bio_12\", \"elev\")) plot(dist)  ############Distances in temporal space library(lubridate) library(ggplot2) data(cookfarm) dat <- st_as_sf(cookfarm,coords=c(\"Easting\",\"Northing\")) st_crs(dat) <- 26911 trainDat <- dat[dat$altitude==-0.3&lubridate::year(dat$Date)==2010,] predictionDat <- dat[dat$altitude==-0.3&lubridate::year(dat$Date)==2011,] trainDat$week <- lubridate::week(trainDat$Date) cvfolds <- CreateSpacetimeFolds(trainDat,timevar = \"week\")  dist <- geodist(trainDat,preddata = predictionDat,cvfolds = cvfolds$indexOut,    type=\"time\",time_unit=\"days\") plot(dist)+ xlim(0,10)   ############ Example for a random global dataset ############ (refer to figure in Meyer and Pebesma 2022)  ### Define prediction area (here: global): ee <- st_crs(\"+proj=eqearth\") co <- ne_countries(returnclass = \"sf\") co.ee <- st_transform(co, ee)  ### Simulate a spatial random sample ### (alternatively replace pts_random by a real sampling dataset (see Meyer and Pebesma 2022): sf_use_s2(FALSE) pts_random <- st_sample(co.ee, 2000, exact=FALSE)  ### See points on the map: ggplot() + geom_sf(data = co.ee, fill=\"#00BFC4\",col=\"#00BFC4\") +   geom_sf(data = pts_random, color = \"#F8766D\",size=0.5, shape=3) +   guides(fill = \"none\", col = \"none\") +   labs(x = NULL, y = NULL)  ### plot distances: dist <- geodist(pts_random,co.ee) plot(dist) + scale_x_log10(labels=round)     } # }"},{"path":"https://hannameyer.github.io/CAST/reference/global_validation.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate 'global' cross-validation — global_validation","title":"Evaluate 'global' cross-validation — global_validation","text":"Calculate validation metric using held back predictions ","code":""},{"path":"https://hannameyer.github.io/CAST/reference/global_validation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate 'global' cross-validation — global_validation","text":"","code":"global_validation(model)"},{"path":"https://hannameyer.github.io/CAST/reference/global_validation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate 'global' cross-validation — global_validation","text":"model object class train","code":""},{"path":"https://hannameyer.github.io/CAST/reference/global_validation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate 'global' cross-validation — global_validation","text":"regression (postResample) classification  (confusionMatrix) statistics","code":""},{"path":"https://hannameyer.github.io/CAST/reference/global_validation.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Evaluate 'global' cross-validation — global_validation","text":"Relevant folds representative entire area interest. case, metrics like R2 meaningful since reflect general ability model explain entire gradient response. Comparable LOOCV, predictions held back folds used together calculate validation statistics.","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/global_validation.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Evaluate 'global' cross-validation — global_validation","text":"Hanna Meyer","code":""},{"path":"https://hannameyer.github.io/CAST/reference/global_validation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate 'global' cross-validation — global_validation","text":"","code":"if (FALSE) { # \\dontrun{ library(caret) data(cookfarm) dat <- cookfarm[sample(1:nrow(cookfarm),500),] indices <- CreateSpacetimeFolds(dat,\"SOURCEID\",\"Date\") ctrl <- caret::trainControl(method=\"cv\",index = indices$index,savePredictions=\"final\") model <- caret::train(dat[,c(\"DEM\",\"TWI\",\"BLD\")],dat$VW, method=\"rf\", trControl=ctrl, ntree=10) global_validation(model) } # }"},{"path":"https://hannameyer.github.io/CAST/reference/knndm.html","id":null,"dir":"Reference","previous_headings":"","what":"K-fold Nearest Neighbour Distance Matching — knndm","title":"K-fold Nearest Neighbour Distance Matching — knndm","text":"function implements kNNDM algorithm returns necessary indices perform k-fold NNDM CV map validation.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/knndm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"K-fold Nearest Neighbour Distance Matching — knndm","text":"","code":"knndm(   tpoints,   modeldomain = NULL,   predpoints = NULL,   space = \"geographical\",   k = 10,   maxp = 0.5,   clustering = \"hierarchical\",   linkf = \"ward.D2\",   samplesize = 1000,   sampling = \"regular\",   useMD = FALSE )"},{"path":"https://hannameyer.github.io/CAST/reference/knndm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"K-fold Nearest Neighbour Distance Matching — knndm","text":"tpoints sf sfc point object, data.frame space = \"feature\". Contains training points samples. modeldomain sf polygon object SpatRaster defining prediction area. Optional; alternative predpoints (see Details). predpoints sf sfc point object, data.frame space = \"feature\". Contains target prediction points. Optional; alternative modeldomain (see Details). space character. Either \"geographical\" \"feature\". k integer. Number folds desired CV. Defaults 10. maxp numeric. Maximum fold size allowed, defaults 0.5, .e. single fold can hold maximum half training points. clustering character. Possible values include \"hierarchical\" \"kmeans\". See details. linkf character. relevant clustering = \"hierarchical\". Link function agglomerative hierarchical clustering. Defaults \"ward.D2\". Check `stats::hclust` options. samplesize numeric. many points modeldomain sampled prediction points? required modeldomain used instead predpoints. sampling character. draw prediction points modeldomain? See `sf::st_sample`. required modeldomain used instead predpoints. useMD boolean. `space`=feature: shall Mahalanobis distance calculated instead Euclidean? works numerical variables.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/knndm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"K-fold Nearest Neighbour Distance Matching — knndm","text":"object class knndm consisting list eight elements: indx_train, indx_test (indices observations use training/test data kNNDM CV iteration), Gij (distances G function construction prediction target points), Gj (distances G function construction LOO CV), Gjstar (distances modified G function kNNDM CV), clusters (list cluster IDs), W (Wasserstein statistic), space (stated user function call).","code":""},{"path":"https://hannameyer.github.io/CAST/reference/knndm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"K-fold Nearest Neighbour Distance Matching — knndm","text":"knndm k-fold version NNDM LOO CV medium large datasets. Brielfy, algorithm tries find k-fold configuration integral absolute differences (Wasserstein W statistic) empirical nearest neighbour distance distribution function test training data CV (Gj*), empirical nearest neighbour distance distribution function prediction training points (Gij), minimised. performing clustering training points' coordinates different numbers clusters range k N (number observations), merging k final folds, selecting configuration lowest W. Using projected CRS `knndm` large computational advantages since fast nearest neighbour search can done via `FNN` package, working geographic coordinates requires computing full spherical distance matrices. clustering algorithm, `kmeans` can used projected CRS `hierarchical` can work projected geographical coordinates, though requires calculating full distance matrix training points even projected CRS. order select clustering algorithms number folds `k`, different `knndm` configurations can run compared, one lower W statistic one offers better match. W statistics `knndm` runs comparable long `tpoints` `predpoints` `modeldomain` stay . Map validation using `knndm` used using `CAST::global_validation`, .e. stacking --sample predictions evaluating . reasons behind 1) resulting folds can unbalanced 2) nearest neighbour functions constructed matched using CV folds simultaneously. training data points clustered respect prediction area presented `knndm` configuration still show signs Gj* > Gij, several things can tried. First, increase `maxp` parameter; may help control strong clustering (cost unbalanced folds). Secondly, decrease number final folds `k`, may help larger clusters. `modeldomain` either sf polygon defines prediction area, alternatively SpatRaster polygon, transformed CRS training points, defined outline non-NA cells. , function takes regular point sample (amount defined `samplesize`) spatial extent. alternative use `predpoints` instead `modeldomain`, already defined prediction locations (e.g. raster pixel centroids). using either `modeldomain` `predpoints`, advise plot study area polygon training/prediction points previous step ensure aligned. `knndm` can also performed feature space setting `space` \"feature\". Euclidean distances Mahalanobis distances can used distance calculation, Euclidean tested. case, nearest neighbour distances calculated n-dimensional feature space rather geographical space. `tpoints` `predpoints` can data frames sf objects containing values features. Note names `tpoints` `predpoints` must . `predpoints` can also missing, `modeldomain` class SpatRaster. case, values SpatRaster extracted `predpoints`. case categorical features, Gower distances used calculate Nearest Neighbour distances [Experimental]. categorical features present, `clustering` = \"kmeans\", K-Prototype clustering performed instead.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/knndm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"K-fold Nearest Neighbour Distance Matching — knndm","text":"Linnenbrink, J., Milà, C., Ludwig, M., Meyer, H.: kNNDM: k-fold Nearest Neighbour Distance Matching Cross-Validation map accuracy estimation, EGUsphere [preprint], https://doi.org/10.5194/egusphere-2023-1308, 2023. Milà, C., Mateu, J., Pebesma, E., Meyer, H. (2022): Nearest Neighbour Distance Matching Leave-One-Cross-Validation map validation. Methods Ecology Evolution 00, 1– 13.","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/knndm.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"K-fold Nearest Neighbour Distance Matching — knndm","text":"Carles Milà Jan Linnenbrink","code":""},{"path":"https://hannameyer.github.io/CAST/reference/knndm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"K-fold Nearest Neighbour Distance Matching — knndm","text":"","code":"######################################################################## # Example 1: Simulated data - Randomly-distributed training points ########################################################################  library(sf) library(ggplot2)  # Simulate 1000 random training points in a 100x100 square set.seed(1234) simarea <- list(matrix(c(0,0,0,100,100,100,100,0,0,0), ncol=2, byrow=TRUE)) simarea <- sf::st_polygon(simarea) train_points <- sf::st_sample(simarea, 1000, type = \"random\") pred_points <- sf::st_sample(simarea, 1000, type = \"regular\") plot(simarea) plot(pred_points, add = TRUE, col = \"blue\") plot(train_points, add = TRUE, col = \"red\")   # Run kNNDM for the whole domain, here the prediction points are known. knndm_folds <- knndm(train_points, predpoints = pred_points, k = 5) #> Warning: Missing CRS in training or prediction points. Assuming projected CRS. #> Gij <= Gj; a random CV assignment is returned knndm_folds #> knndm object #> Space: geographical #> Clustering algorithm: hierarchical #> Intermediate clusters (q): random CV #> W statistic: 0.1338 #> Number of folds: 5 #> Observations in each fold:  200 200 200 200 200  plot(knndm_folds)  plot(knndm_folds, type = \"simple\") # For more accessible legend labels  plot(knndm_folds, type = \"simple\", stat = \"density\") # To visualize densities rather than ECDFs  folds <- as.character(knndm_folds$clusters) ggplot() +   geom_sf(data = simarea, alpha = 0) +   geom_sf(data = train_points, aes(col = folds))   ######################################################################## # Example 2: Simulated data - Clustered training points ######################################################################## if (FALSE) { # \\dontrun{ library(sf) library(ggplot2)  # Simulate 1000 clustered training points in a 100x100 square set.seed(1234) simarea <- list(matrix(c(0,0,0,100,100,100,100,0,0,0), ncol=2, byrow=TRUE)) simarea <- sf::st_polygon(simarea) train_points <- clustered_sample(simarea, 1000, 50, 5) pred_points <- sf::st_sample(simarea, 1000, type = \"regular\") plot(simarea) plot(pred_points, add = TRUE, col = \"blue\") plot(train_points, add = TRUE, col = \"red\")  # Run kNNDM for the whole domain, here the prediction points are known. knndm_folds <- knndm(train_points, predpoints = pred_points, k = 5) knndm_folds plot(knndm_folds) plot(knndm_folds, type = \"simple\") # For more accessible legend labels plot(knndm_folds, type = \"simple\", stat = \"density\") # To visualize densities rather than ECDFs folds <- as.character(knndm_folds$clusters) ggplot() +   geom_sf(data = simarea, alpha = 0) +   geom_sf(data = train_points, aes(col = folds)) } # } ######################################################################## # Example 3: Real- world example; using a modeldomain instead of previously # sampled prediction locations ######################################################################## if (FALSE) { # \\dontrun{ library(sf) library(terra) library(ggplot2)  ### prepare sample data: data(cookfarm) dat <- aggregate(cookfarm[,c(\"DEM\",\"TWI\", \"NDRE.M\", \"Easting\", \"Northing\",\"VW\")],    by=list(as.character(cookfarm$SOURCEID)),mean) pts <- dat[,-1] pts <- st_as_sf(pts,coords=c(\"Easting\",\"Northing\")) st_crs(pts) <- 26911 studyArea <- rast(system.file(\"extdata\",\"predictors_2012-03-25.tif\",package=\"CAST\")) pts <- st_transform(pts, crs = st_crs(studyArea)) terra::plot(studyArea[[\"DEM\"]]) terra::plot(vect(pts), add = T)  knndm_folds <- knndm(pts, modeldomain=studyArea, k = 5) knndm_folds plot(knndm_folds) folds <- as.character(knndm_folds$clusters) ggplot() +   geom_sf(data = pts, aes(col = folds))  #use for cross-validation: library(caret) ctrl <- trainControl(method=\"cv\",    index=knndm_folds$indx_train,    savePredictions='final') model_knndm <- train(dat[,c(\"DEM\",\"TWI\", \"NDRE.M\")],    dat$VW,    method=\"rf\",    trControl = ctrl) global_validation(model_knndm) } # } ######################################################################## # Example 4: Real- world example; kNNDM in feature space ######################################################################## if (FALSE) { # \\dontrun{ library(sf) library(terra) library(ggplot2)  data(splotdata) splotdata <- splotdata[splotdata$Country == \"Chile\",]  predictors <- c(\"bio_1\", \"bio_4\", \"bio_5\", \"bio_6\",                \"bio_8\", \"bio_9\", \"bio_12\", \"bio_13\",                \"bio_14\", \"bio_15\", \"elev\")  trainDat <- sf::st_drop_geometry(splotdata) predictors_sp <- terra::rast(system.file(\"extdata\", \"predictors_chile.tif\",package=\"CAST\"))   terra::plot(predictors_sp[[\"bio_1\"]]) terra::plot(vect(splotdata), add = T)  knndm_folds <- knndm(trainDat[,predictors], modeldomain = predictors_sp, space = \"feature\",                     clustering=\"kmeans\", k=4, maxp=0.8) plot(knndm_folds)  } # }"},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":null,"dir":"Reference","previous_headings":"","what":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"function implements NNDM algorithm returns necessary indices perform NNDM LOO CV map validation.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"","code":"nndm(   tpoints,   modeldomain = NULL,   predpoints = NULL,   space = \"geographical\",   samplesize = 1000,   sampling = \"regular\",   phi = \"max\",   min_train = 0.5 )"},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"tpoints sf sfc point object, data.frame space = \"feature\". Contains training points samples. modeldomain sf polygon object SpatRaster defining prediction area. Optional; alternative predpoints (see Details). predpoints sf sfc point object, data.frame space = \"feature\". Contains target prediction points. Optional; alternative modeldomain (see Details). space character. Either \"geographical\" \"feature\". Feature space still experimental, use caution. samplesize numeric. many points modeldomain sampled prediction points? required modeldomain used instead predpoints. sampling character. draw prediction points modeldomain? See `sf::st_sample`. required modeldomain used instead predpoints. phi Numeric. Estimate landscape autocorrelation range units tpoints predpoints projected CRS, meters geographic CRS. Per default (phi=\"max\"), maximum distance found training prediction points used. See Details. min_train Numeric 0 1. Minimum proportion training data must used CV fold. Defaults 0.5 (.e. half training points).","code":""},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"object class nndm consisting list six elements: indx_train, indx_test, indx_exclude (indices observations use training/test/excluded data NNDM LOO CV iteration), Gij (distances G function construction prediction target points), Gj (distances G function construction LOO CV), Gjstar (distances modified G function NNDM LOO CV), phi (landscape autocorrelation range). indx_train indx_test can directly used \"index\" \"indexOut\" caret's trainControl function used initiate custom validation strategy mlr3.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"NNDM proposes LOO CV scheme nearest neighbour distance distribution function test training data CV process matched nearest neighbour distance distribution function prediction training points. Details method can found Milà et al. (2022). Specifying phi allows limiting distance matching area assumed relevant due spatial autocorrelation. Distances matched phi. Beyond range, data points used training, without exclusions. phi set \"max\", nearest neighbor distance matching performed entire prediction area. Euclidean distances used projected non-defined CRS, great circle distances used geographic CRS (units meters). modeldomain either sf polygon defines prediction area, alternatively SpatRaster polygon, transformed CRS training points, defined outline non-NA cells. , function takes regular point sample (amount defined samplesize) spatial extent. alternative use predpoints instead modeldomain, already defined prediction locations (e.g. raster pixel centroids). using either modeldomain predpoints, advise plot study area polygon training/prediction points previous step ensure aligned.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"NNDM variation LOOCV therefore may take long time large training data sets. See knndm efficient k-fold variant method.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"Milà, C., Mateu, J., Pebesma, E., Meyer, H. (2022): Nearest Neighbour Distance Matching Leave-One-Cross-Validation map validation. Methods Ecology Evolution 00, 1– 13. Meyer, H., Pebesma, E. (2022): Machine learning-based global maps ecological variables challenge assessing . Nature Communications. 13.","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"Carles Milà","code":""},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"","code":"######################################################################## # Example 1: Simulated data - Randomly-distributed training points ########################################################################  library(sf)  # Simulate 100 random training points in a 100x100 square set.seed(123) poly <- list(matrix(c(0,0,0,100,100,100,100,0,0,0), ncol=2, byrow=TRUE)) sample_poly <- sf::st_polygon(poly) train_points <- sf::st_sample(sample_poly, 100, type = \"random\") pred_points <- sf::st_sample(sample_poly, 100, type = \"regular\") plot(sample_poly) plot(pred_points, add = TRUE, col = \"blue\") plot(train_points, add = TRUE, col = \"red\")   # Run NNDM for the whole domain, here the prediction points are known nndm_pred <- nndm(train_points, predpoints=pred_points) nndm_pred #> nndm object #> Total number of points: 100 #> Mean number of training points: 98.54 #> Minimum number of training points: 83 plot(nndm_pred)  plot(nndm_pred, type = \"simple\") # For more accessible legend labels   # ...or run NNDM with a known autocorrelation range of 10 # to restrict the matching to distances lower than that. nndm_pred <- nndm(train_points, predpoints=pred_points, phi = 10) nndm_pred #> nndm object #> Total number of points: 100 #> Mean number of training points: 98.72 #> Minimum number of training points: 96 plot(nndm_pred)   ######################################################################## # Example 2: Simulated data - Clustered training points ########################################################################  library(sf)  # Simulate 100 clustered training points in a 100x100 square set.seed(123) poly <- list(matrix(c(0,0,0,100,100,100,100,0,0,0), ncol=2, byrow=TRUE)) sample_poly <- sf::st_polygon(poly) train_points <- clustered_sample(sample_poly, 100, 10, 5) pred_points <- sf::st_sample(sample_poly, 100, type = \"regular\") plot(sample_poly) plot(pred_points, add = TRUE, col = \"blue\") plot(train_points, add = TRUE, col = \"red\")   # Run NNDM for the whole domain nndm_pred <- nndm(train_points, predpoints=pred_points) nndm_pred #> nndm object #> Total number of points: 100 #> Mean number of training points: 86.84 #> Minimum number of training points: 50 plot(nndm_pred)  plot(nndm_pred, type = \"simple\") # For more accessible legend labels   ######################################################################## # Example 3: Real- world example; using a SpatRast modeldomain instead # of previously sampled prediction locations ######################################################################## if (FALSE) { # \\dontrun{ library(sf) library(terra)  ### prepare sample data: data(cookfarm) dat <- aggregate(cookfarm[,c(\"DEM\",\"TWI\", \"NDRE.M\", \"Easting\", \"Northing\",\"VW\")],    by=list(as.character(cookfarm$SOURCEID)),mean) pts <- dat[,-1] pts <- st_as_sf(pts,coords=c(\"Easting\",\"Northing\")) st_crs(pts) <- 26911 studyArea <- rast(system.file(\"extdata\",\"predictors_2012-03-25.tif\",package=\"CAST\")) pts <- st_transform(pts, crs = st_crs(studyArea)) terra::plot(studyArea[[\"DEM\"]]) terra::plot(vect(pts), add = T)  nndm_folds <- nndm(pts, modeldomain = studyArea) plot(nndm_folds)  #use for cross-validation: library(caret) ctrl <- trainControl(method=\"cv\",    index=nndm_folds$indx_train,    indexOut=nndm_folds$indx_test,    savePredictions='final') model_nndm <- train(dat[,c(\"DEM\",\"TWI\", \"NDRE.M\")],    dat$VW,    method=\"rf\",    trControl = ctrl) global_validation(model_nndm) } # }  ######################################################################## # Example 4: Real- world example; nndm in feature space ######################################################################## if (FALSE) { # \\dontrun{ library(sf) library(terra) library(ggplot2)  # Prepare the splot dataset for Chile data(splotdata) splotdata <- splotdata[splotdata$Country == \"Chile\",]  # Select a series of bioclimatic predictors predictors <- c(\"bio_1\", \"bio_4\", \"bio_5\", \"bio_6\",                \"bio_8\", \"bio_9\", \"bio_12\", \"bio_13\",                \"bio_14\", \"bio_15\", \"elev\")  predictors_sp <- terra::rast(system.file(\"extdata\", \"predictors_chile.tif\", package=\"CAST\"))  # Data visualization terra::plot(predictors_sp[[\"bio_1\"]]) terra::plot(vect(splotdata), add = T)  # Run and visualise the nndm results nndm_folds <- nndm(splotdata[,predictors], modeldomain = predictors_sp, space = \"feature\") plot(nndm_folds)   #use for cross-validation: library(caret) ctrl <- trainControl(method=\"cv\",    index=nndm_folds$indx_train,    indexOut=nndm_folds$indx_test,    savePredictions='final') model_nndm <- train(st_drop_geometry(splotdata[,predictors]),    splotdata$Species_richness,    method=\"rf\",    trControl = ctrl) global_validation(model_nndm)  } # }"},{"path":"https://hannameyer.github.io/CAST/reference/normalize_DI.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalize DI values — normalize_DI","title":"Normalize DI values — normalize_DI","text":"DI normalized DI threshold allow straightforward interpretation. value resulting DI larger 1 means data dissimilar observed cross-validation. returned threshold adjusted accordingly , consequence, 1.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/normalize_DI.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalize DI values — normalize_DI","text":"","code":"normalize_DI(AOA)"},{"path":"https://hannameyer.github.io/CAST/reference/normalize_DI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalize DI values — normalize_DI","text":"AOA AOA object","code":""},{"path":"https://hannameyer.github.io/CAST/reference/normalize_DI.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Normalize DI values — normalize_DI","text":"object class aoa","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/normalize_DI.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Normalize DI values — normalize_DI","text":"","code":"if (FALSE) { # \\dontrun{ library(sf) library(terra) library(caret)  # prepare sample data: data(cookfarm) dat <- aggregate(cookfarm[,c(\"VW\",\"Easting\",\"Northing\")],    by=list(as.character(cookfarm$SOURCEID)),mean) pts <- st_as_sf(dat,coords=c(\"Easting\",\"Northing\")) pts$ID <- 1:nrow(pts) set.seed(100) pts <- pts[1:30,] studyArea <- rast(system.file(\"extdata\",\"predictors_2012-03-25.tif\",package=\"CAST\"))[[1:8]] trainDat <- extract(studyArea,pts,na.rm=FALSE) trainDat <- merge(trainDat,pts,by.x=\"ID\",by.y=\"ID\")  # train a model: set.seed(100) variables <- c(\"DEM\",\"NDRE.Sd\",\"TWI\") model <- train(trainDat[,which(names(trainDat)%in%variables)], trainDat$VW, method=\"rf\", importance=TRUE, tuneLength=1, trControl=trainControl(method=\"cv\",number=5,savePredictions=T))  #...then calculate the AOA of the trained model for the study area: AOA <- aoa(studyArea, model) plot(AOA) plot(AOA$DI)  #... then normalize the DI DI_norm <- normalize_DI(AOA) plot(DI_norm) plot(DI_norm$DI)  } # }"},{"path":"https://hannameyer.github.io/CAST/reference/plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot CAST classes — plot","title":"Plot CAST classes — plot","text":"Generic plot function CAST Classes plotting function forward feature selection result. point mean performance model run. Error bars represent standard errors cross validation. Marked points show best model number variables variable improve results. type==\"selected\", contribution selected variables model performance shown. Density plot nearest neighbor distances geographic space feature space training data well training data prediction locations. Optional, nearest neighbor distances training data test data training data CV iterations shown. plot can used check suitability chosen CV method representative estimate map accuracy. Plot DI/LPD errormetric Cross-Validation modeled relationship","code":""},{"path":"https://hannameyer.github.io/CAST/reference/plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot CAST classes — plot","text":"","code":"# S3 method for class 'trainDI' plot(x, ...)  # S3 method for class 'aoa' plot(x, samplesize = 1000, variable = \"DI\", ...)  # S3 method for class 'nndm' plot(x, type = \"strict\", stat = \"ecdf\", ...)  # S3 method for class 'knndm' plot(x, type = \"strict\", stat = \"ecdf\", ...)  # S3 method for class 'ffs' plot(   x,   plotType = \"all\",   palette = rainbow,   reverse = FALSE,   marker = \"black\",   size = 1.5,   lwd = 0.5,   pch = 21,   ... )  # S3 method for class 'geodist' plot(x, unit = \"m\", stat = \"density\", ...)  # S3 method for class 'errorModel' plot(x, ...)"},{"path":"https://hannameyer.github.io/CAST/reference/plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot CAST classes — plot","text":"x errorModel, see DItoErrormetric ... params samplesize numeric. many prediction samples plotted? variable character. Variable generate density plot. 'DI' 'LPD' type String, defaults \"strict\" show original nearest neighbour distance definitions legend. Alternatively, set \"simple\" intuitive labels. stat \"density\" density plot \"ecdf\" empirical cumulative distribution function plot. plotType character. Either \"\" \"selected\" palette color palette reverse Character. palette reversed? marker Character. Color mark best models size Numeric. Size points lwd Numeric. Width error bars pch Numeric. Type point marking best models unit character. type==\"geo\" applied plot. Supported: \"m\" \"km\".","code":""},{"path":"https://hannameyer.github.io/CAST/reference/plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot CAST classes — plot","text":"ggplot ggplot","code":""},{"path":"https://hannameyer.github.io/CAST/reference/plot.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot CAST classes — plot","text":"Marvin Ludwig, Hanna Meyer Carles Milà","code":""},{"path":"https://hannameyer.github.io/CAST/reference/plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot CAST classes — plot","text":"","code":"if (FALSE) { # \\dontrun{ data(splotdata) splotdata <- st_drop_geometry(splotdata) ffsmodel <- ffs(splotdata[,6:16], splotdata$Species_richness, ntree = 10) plot(ffsmodel) #plot performance of selected variables only: plot(ffsmodel,plotType=\"selected\") } # }"},{"path":"https://hannameyer.github.io/CAST/reference/print.html","id":null,"dir":"Reference","previous_headings":"","what":"Print CAST classes — print","title":"Print CAST classes — print","text":"Generic print function trainDI aoa","code":""},{"path":"https://hannameyer.github.io/CAST/reference/print.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print CAST classes — print","text":"","code":"# S3 method for class 'trainDI' print(x, ...)  show.trainDI(x, ...)  # S3 method for class 'aoa' print(x, ...)  show.aoa(x, ...)  # S3 method for class 'nndm' print(x, ...)  show.nndm(x, ...)  # S3 method for class 'knndm' print(x, ...)  show.knndm(x, ...)  # S3 method for class 'ffs' print(x, ...)  show.ffs(x, ...)"},{"path":"https://hannameyer.github.io/CAST/reference/print.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print CAST classes — print","text":"x object type ffs ... arguments.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/splotdata.html","id":null,"dir":"Reference","previous_headings":"","what":"sPlotOpen Data of Species Richness — splotdata","title":"sPlotOpen Data of Species Richness — splotdata","text":"sPlotOpen Species Richness South America associated predictors","code":""},{"path":"https://hannameyer.github.io/CAST/reference/splotdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"sPlotOpen Data of Species Richness — splotdata","text":"","code":"data(splotdata)"},{"path":"https://hannameyer.github.io/CAST/reference/splotdata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"sPlotOpen Data of Species Richness — splotdata","text":"sf points / data.frame 703 rows 17 columns: PlotObeservationID, GIVD_ID, Country, Biome sPlotOpen Metadata Species_richness Response Variable - Plant species richness sPlotOpen bio_x, elev Predictor Variables - Worldclim SRTM elevation geometry Lat/Lon","code":""},{"path":"https://hannameyer.github.io/CAST/reference/splotdata.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"sPlotOpen Data of Species Richness — splotdata","text":"Plot Species_richness sPlotOpen predictors acquired via R package geodata","code":""},{"path":"https://hannameyer.github.io/CAST/reference/splotdata.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"sPlotOpen Data of Species Richness — splotdata","text":"Sabatini, F. M. et al. sPlotOpen – environmentally balanced, open‐access, global dataset vegetation plots. (2021). doi:10.1111/geb.13346 Lopez-Gonzalez, G. et al. ForestPlots.net: web application research tool manage analyse tropical forest plot data: ForestPlots.net.  Journal Vegetation Science (2011). Pauchard, . et al. Alien Plants Homogenise Protected Areas: Evidence Landscape Regional Scales South Central Chile. Plant Invasions Protected Areas (2013). Peyre, G. et al. VegPáramo, flora vegetation database Andean páramo. phytocoenologia (2015). Vibrans, . C. et al. Insights large-scale inventory southern Brazilian Atlantic Forest. Scientia Agricola (2020).","code":""},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Dissimilarity Index of training data — trainDI","title":"Calculate Dissimilarity Index of training data — trainDI","text":"function estimates Dissimilarity Index (DI) within training data set used prediction model. Optionally, local point density can also calculated. Predictors can weighted based internal variable importance machine learning algorithm used model training.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Dissimilarity Index of training data — trainDI","text":"","code":"trainDI(   model = NA,   train = NULL,   variables = \"all\",   weight = NA,   CVtest = NULL,   CVtrain = NULL,   method = \"L2\",   useWeight = TRUE,   useCV = TRUE,   LPD = FALSE,   verbose = TRUE )"},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Dissimilarity Index of training data — trainDI","text":"model train object created caret used extract weights (based variable importance) well cross-validation folds train data.frame containing data used model training. required model given variables character vector predictor variables. \"\" variables model used model given train dataset. weight data.frame containing weights variable. required model given. CVtest list vector. Either list element contains data points used testing cross validation iteration (.e. held back data). vector contains ID fold training point. required model given. CVtrain list. element contains data points used training cross validation iteration (.e. held back data). required model given required CVtrain opposite CVtest (.e. data point used testing, used training). Relevant data points excluded, e.g. using nndm. method Character. Method used distance calculation. Currently euclidean distance (L2) Mahalanobis distance (MD) implemented L2 tested. Note MD takes considerably longer. useWeight Logical. model given. Weight variables according importance model? useCV Logical. model given. Use CV folds calculate DI threshold? LPD Logical. Indicates whether local point density calculated . verbose Logical. Print progress ?","code":""},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Dissimilarity Index of training data — trainDI","text":"list class trainDI containing: train data frame containing training data weight data frame weights based variable importance. variables Names used variables catvars variables categorial scaleparam Scaling parameters. Output scale trainDist_avrg data frame average distance training point every point trainDist_avrgmean mean trainDist_avrg. Used normalizing DI trainDI Dissimilarity Index training data threshold DI threshold used inside/outside AOA trainLPD LPD training data avrgLPD Average LPD training data","code":""},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Calculate Dissimilarity Index of training data — trainDI","text":"function called within aoa estimate DI AOA new data. However, may also used DI training data interest, facilitate parallelization aoa avoiding repeated calculation DI within training data.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate Dissimilarity Index of training data — trainDI","text":"Meyer, H., Pebesma, E. (2021): Predicting unknown space? Estimating area applicability spatial prediction models. doi:10.1111/2041-210X.13650","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate Dissimilarity Index of training data — trainDI","text":"Hanna Meyer, Marvin Ludwig, Fabian Schumacher","code":""},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Dissimilarity Index of training data — trainDI","text":"","code":"if (FALSE) { # \\dontrun{ library(sf) library(terra) library(caret) library(CAST)  # prepare sample data: data(\"splotdata\") splotdata = st_drop_geometry(splotdata)  # train a model: set.seed(100) model <- caret::train(splotdata[,6:16],                       splotdata$Species_richness,                       importance=TRUE, tuneLength=1, ntree = 15, method = \"rf\",                       trControl = trainControl(method=\"cv\", number=5, savePredictions=T)) # variable importance is used for scaling predictors plot(varImp(model,scale=FALSE))  # calculate the DI of the trained model: DI = trainDI(model=model) plot(DI)  #...or calculate the DI and LPD of the trained model: # DI = trainDI(model=model, LPD = TRUE)  # the DI can now be used to compute the AOA (here with LPD): studyArea = rast(system.file(\"extdata/predictors_chile.tif\", package = \"CAST\")) AOA = aoa(studyArea, model = model, trainDI = DI, LPD = TRUE, maxLPD = 1) print(AOA) plot(AOA) plot(AOA$AOA) plot(AOA$LPD) } # }"},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-102","dir":"Changelog","previous_headings":"","what":"CAST 1.0.2","title":"CAST 1.0.2","text":"CRAN release: 2024-06-14 bug fix: tests run conditionally","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-101","dir":"Changelog","previous_headings":"","what":"CAST 1.0.1","title":"CAST 1.0.1","text":"CRAN release: 2024-04-25 bug fix: fix failed tests global_validation","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-100","dir":"Changelog","previous_headings":"","what":"CAST 1.0.0","title":"CAST 1.0.0","text":"CRAN release: 2024-04-08 calculate local point density within AOA option spatial error profiles (errorProfiles variable=“geodist”) normalize_DI intuitive interpretation geodist allows calculating temporal distances ffs now can run parallel (Linux ) vignette “Cross-validation methods CAST” knndm feature space (experimental) nndm feature space (experimental) function DItoErrormetric renamed errorProfiles allows dissimilarity measures Improvement homogenization plotting methods nndm, knndm geodist objects aoa trainDI weight now allows list input vignette Introduction CAST updated deprecated: plot_geodist (replaced plot.geodist) plot_ffs (replaced plot.ffs) *calibrate_aoa (replaced errorProfiles)","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-090","dir":"Changelog","previous_headings":"","what":"CAST 0.9.0","title":"CAST 0.9.0","text":"CRAN release: 2024-01-09 CAST functions now return classes generic plotting printing new dataset examples, tutorials testing: data(splotdata) calibrate_aoa now DItoErrormetric returns model (see function documentation) plot_geodist now geodist. result can visualized plot() plot_ffs now plot(ffs) fix issue #65 (threshold) plot_geodist, plot_ffs, calibrate_aoa","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-081","dir":"Changelog","previous_headings":"","what":"CAST 0.8.1","title":"CAST 0.8.1","text":"CRAN release: 2023-05-30 failed checks Fedora 34 fixed","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-080","dir":"Changelog","previous_headings":"","what":"CAST 0.8.0","title":"CAST 0.8.0","text":"CRAN release: 2023-05-21 knndm alternative nndm large training data transition raster terra","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-071","dir":"Changelog","previous_headings":"","what":"CAST 0.7.1","title":"CAST 0.7.1","text":"CRAN release: 2023-02-04 Mahalanobis distance AOA assessment option faster estimation AOA delineation default threshold fixed suggested github.com/HannaMeyer/CAST/issues/46 fixed issue github.com/ropensci/rnaturalearth/issues/69","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-070","dir":"Changelog","previous_headings":"","what":"CAST 0.7.0","title":"CAST 0.7.0","text":"CRAN release: 2022-08-24 nndm cross-validation suggested Milà et al. (2022) plot_geodist works NNDM trainDI works NNDM rename parameter folds AOA trainDI","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-060","dir":"Changelog","previous_headings":"","what":"CAST 0.6.0","title":"CAST 0.6.0","text":"CRAN release: 2022-03-17 trainDI allows calculate DI training dataset separately aoa function plot print functions AOA function plot nearest neighbor distance distributions geographic feature space function global_validation added extensive restructuring AOA function ffs bss can used global_validation error manual assignment weights fixed","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-051","dir":"Changelog","previous_headings":"","what":"CAST 0.5.1","title":"CAST 0.5.1","text":"CRAN release: 2021-04-07 resolved dependence package “GSIF” removed CRAN repository","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-050","dir":"Changelog","previous_headings":"","what":"CAST 0.5.0","title":"CAST 0.5.0","text":"CRAN release: 2021-02-19 AOA can run parallel calibration DI (calibrate_aoa) aoa work now large training sets default threshold AOA changed","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-042","dir":"Changelog","previous_headings":"","what":"CAST 0.4.2","title":"CAST 0.4.2","text":"CRAN release: 2020-07-17 aoa now working categorical variables fixed error ffs >170 variables used changed order parameters aoa tutorial “Introduction CAST” improved","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-041","dir":"Changelog","previous_headings":"","what":"CAST 0.4.1","title":"CAST 0.4.1","text":"CRAN release: 2020-05-19 vignette: tutorial introducing “area applicability” variable threshold aoa various modifications aoa line submitted paper","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-040","dir":"Changelog","previous_headings":"","what":"CAST 0.4.0","title":"CAST 0.4.0","text":"CRAN release: 2020-04-06 new function “aoa”: quantify visualize area applicability spatial prediction models “minVar” ffs: Instead always starting 2-pair combinations, ffs can now also started combinations variables (e.g starting combinations 3) ffs failed “svmLinear” previous version S4 class issues. Fixed now.","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-031","dir":"Changelog","previous_headings":"","what":"CAST 0.3.1","title":"CAST 0.3.1","text":"CRAN release: 2018-11-19 CreateSpaceTimeFolds accepts tibbles CreateSpaceTimeFolds automatically reduces k necessary ffs accepts arguments taken caret::train new feature: plot_ffs option plot selected variables ","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-030","dir":"Changelog","previous_headings":"","what":"CAST 0.3.0","title":"CAST 0.3.0","text":"CRAN release: 2018-10-11 new feature: Best subset selection (bss) target-oriented validation (slow reliable) alternative ffs minor adaptations: verbose option included, improved examples ffs bugfix: minor adaptations done usage plsr","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-021","dir":"Changelog","previous_headings":"","what":"CAST 0.2.1","title":"CAST 0.2.1","text":"CRAN release: 2018-07-12 new feature: Introduction CAST included vignette. bugfix: minor error fixed using user defined metrics model selection.","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-020","dir":"Changelog","previous_headings":"","what":"CAST 0.2.0","title":"CAST 0.2.0","text":"CRAN release: 2018-05-03 bugfix: ffs option withinSE=TRUE choose model “best model” within SE model trained earlier run number variables. bug fixed withinSE=TRUE ffs now compares performance models use less variables (e.g. model using 5 variables better model using 4 variables still SE 4-variable model, 4-variable model rated better model). new feature: plot_ffs plots results ffs visualize performance changes according model run number variables used.","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-010","dir":"Changelog","previous_headings":"","what":"CAST 0.1.0","title":"CAST 0.1.0","text":"CRAN release: 2018-01-09 Initial public version CRAN","code":""}]
