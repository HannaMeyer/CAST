[{"path":[]},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"background","dir":"Articles","previous_headings":"Introduction","what":"Background","title":"1. Introduction to CAST","text":"One key task environmental science obtaining information environmental variables continuously space space time, usually based remote sensing limited field data. respect, machine learning algorithms proven important tool learn patterns nonlinear complex systems. However, standard machine learning applications suitable spatio-temporal data, usually ignore spatio-temporal dependencies data. becomes problematic (least) two aspects predictive modelling: Overfitted models well overly optimistic error assessment (see Meyer et al 2018 Meyer et al 2019 ). approach problems, CAST supports well-known caret package (Kuhn 2018 provide methods designed spatio-temporal data. tutorial shows set spatio-temporal prediction model includes objective reliable error estimation. shows spatio-temporal overfitting can detected comparison validation strategies. shown certain variables responsible problem overfitting due spatio-temporal autocorrelation patterns. Therefore, tutorial also shows automatically exclude variables lead overfitting aim improve spatio-temporal prediction model. order follow tutorial, assume reader familiar basics predictive modelling nicely explained Kuhn Johnson 2013 well machine learning applications via caret package.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"how-to-start","dir":"Articles","previous_headings":"Introduction","what":"How to start","title":"1. Introduction to CAST","text":"work tutorial, first install CAST package load library: need help, see","code":"#install.packages(\"CAST\") library(CAST) help(CAST)"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"example-of-a-typical-spatio-temporal-prediction-task","dir":"Articles","previous_headings":"","what":"Example of a typical spatio-temporal prediction task","title":"1. Introduction to CAST","text":"example prediction task tutorial following: set data loggers distributed farm, want map soil moisture, based set spatial temporal predictor variables. use Random Forests machine learning algorithm tutorial.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"description-of-the-example-dataset","dir":"Articles","previous_headings":"Example of a typical spatio-temporal prediction task","what":"Description of the example dataset","title":"1. Introduction to CAST","text":", work cookfarm dataset, described e.g. Gasch et al 2015 available via GSIF package (Hengl 2017). dataset included CAST package re-structured dataset used analysis Meyer et al 2018. want point following information dataset: “SOURCEID” represents ID data logger, “VW” soil moisture response variable, “Easting” “Northing” coordinates data loggers, “altitude” indicates depth soil VW measured, remaining columns represent different potential predictor variables terrain related (e.g. “DEM”, “TWI”), vegetation indices (e.g. “NDRE”), soil properties (e.g. “BLD”) climate-related predictors (e.g. “Precip_wrcc”). See Gasch et al 2015 description dataset. get impression spatial properties dataset, let’s look spatial distribution data loggers cookfarm:  see data taken 42 locations (SOURCEID) field. loggers recorded data 2007 2013 (dataset contains data 2010 ). VW data given daily basis.","code":"data <- get(load(system.file(\"extdata\",\"Cookfarm.RData\",package=\"CAST\"))) head(data) ##        SOURCEID    VW  Easting Northing altitude      DEM      TWI     NDRE.M ## 101689   CAF357 0.303 493828.1  5181021     -0.3 792.5756 3.791253 0.08161208 ## 213001   CAF357 0.328 493828.1  5181021     -0.6 792.5756 3.791253 0.08161208 ## 324313   CAF357 0.376 493828.1  5181021     -0.9 792.5756 3.791253 0.08161208 ## 435625   CAF357 0.350 493828.1  5181021     -1.2 792.5756 3.791253 0.08161208 ## 546937   CAF357 0.323 493828.1  5181021     -1.5 792.5756 3.791253 0.08161208 ## 101690   CAF357 0.297 493828.1  5181021     -0.3 792.5756 3.791253 0.08161208 ##          NDRE.Sd     Bt  BLD  PHI Crop       Date Precip_wrcc MaxT_wrcc ## 101689 0.2805182 0.0000 1.22 5.84   SL 2010-01-01         5.8       2.8 ## 213001 0.2805182 0.0000 1.36 6.32   SL 2010-01-01         5.8       2.8 ## 324313 0.2805182 0.0000 1.48 6.52   SL 2010-01-01         5.8       2.8 ## 435625 0.2805182 0.0000 1.56 6.68   SL 2010-01-01         5.8       2.8 ## 546937 0.2805182 0.0106 1.60 6.72   SL 2010-01-01         5.8       2.8 ## 101690 0.2805182 0.0000 1.22 5.84   SL 2010-01-02         6.9       6.1 ##        MinT_wrcc Precip_cum  cday       cdayt ## 101689      -3.3        5.8 14611 -0.05233596 ## 213001      -3.3        5.8 14611 -0.05233596 ## 324313      -3.3        5.8 14611 -0.05233596 ## 435625      -3.3        5.8 14611 -0.05233596 ## 546937      -3.3        5.8 14611 -0.05233596 ## 101690       0.6       12.7 14612 -0.03489950 library(sf) data_sp <- unique(data[,c(\"SOURCEID\",\"Easting\",\"Northing\")]) data_sp <- st_as_sf(data_sp,coords=c(\"Easting\",\"Northing\"),crs=26911) plot(data_sp,axes=T,col=\"black\") #...or plot the data with mapview: library(mapview) mapviewOptions(basemaps = c(\"Esri.WorldImagery\")) mapview(data_sp)"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"data-subsetting","dir":"Articles","previous_headings":"Example of a typical spatio-temporal prediction task","what":"Data subsetting","title":"1. Introduction to CAST","text":"reduce data amount can handled tutorial, let’s restrict data depth -0.3 two weeks year 2012. subsetting let’s overview soil moisture time series measured data loggers.  can see (expected) logger location unique time series soil moisture.","code":"library(lubridate) library(ggplot2) trainDat <- data[data$altitude==-0.3&                    year(data$Date)==2012&                    week(data$Date)%in%c(10:12),] ggplot(data = trainDat, aes(x=Date, y=VW)) +   geom_line(aes(colour=SOURCEID))"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"model-training-and-prediction","dir":"Articles","previous_headings":"","what":"Model training and prediction","title":"1. Introduction to CAST","text":"following use subset cookfarm data example spatially predict soil moisture (.e. map soil moisture) (without) consideration spatio-temporal dependencies. start , lets use dataset create “default” Random Forest model predicts soil moisture based predictor variables. keep computation time minimum, don’t include hyperparameter tuning (hence mtry set 2) reasonable Random Forests comparably insensitive tuning. Based trained model can make spatial predictions soil moisture. load RasterStack contains spatial data predictor variables 25th March 2012 (example). apply trained model data set.  result spatially comprehensive map soil moisture day. see simply creating map using machine learning caret easy task, however accurately measuring performance less simple. Though map looks good first sight now follow question accurate map , hence need ask well model able map soil moisture. visible inspection noticeable model produces strange linear features eastern side farm looks suspicious. let’s come back later first focus statistical validation model.","code":"library(caret) predictors <- c(\"DEM\",\"TWI\",\"Precip_cum\",\"cday\",                 \"MaxT_wrcc\",\"Precip_wrcc\",\"BLD\",                 \"Northing\",\"Easting\",\"NDRE.M\") set.seed(10) model <- train(trainDat[,predictors],trainDat$VW,                method=\"rf\",tuneGrid=data.frame(\"mtry\"=2),                importance=TRUE,ntree=50,                trControl=trainControl(method=\"cv\",number=3)) library(raster) predictors_sp <- stack(system.file(\"extdata\",\"predictors_2012-03-25.grd\",package=\"CAST\")) prediction <- predict(predictors_sp,model) spplot(prediction)"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"cross-validation-strategies-for-spatio-temporal-data","dir":"Articles","previous_headings":"","what":"Cross validation strategies for spatio-temporal data","title":"1. Introduction to CAST","text":"Among validation strategies, k-fold cross validation (CV) popular estimate performance model view data used model training. CV, models repeatedly trained (k models) model run, data one fold put side used model training model validation. way, performance model can estimated using data included model training.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"the-standard-approach-random-k-fold-cv","dir":"Articles","previous_headings":"Cross validation strategies for spatio-temporal data","what":"The Standard approach: Random k-fold CV","title":"1. Introduction to CAST","text":"example used random k-fold CV defined caret’s trainControl argument. specifically, used random 3-fold CV. Hence, data points dataset RANDOMLY split 3 folds. assess performance model let’s look output Random CV: see soil moisture modelled high R² (0.90) indicates nearly perfect fit data. Sounds good, unfortunately, random k fold CV give us good indication map accuracy. Random k-fold CV means three folds (highest certainty) contains data points data logger. Therefore, random CV indicate ability model make predictions beyond location training data (.e. map soil moisture). Since aim map soil moisture, rather need perform target-oriented validation validates model view spatial mapping.","code":"model ## Random Forest  ##  ## 654 samples ##  10 predictor ##  ## No pre-processing ## Resampling: Cross-Validated (3 fold)  ## Summary of sample sizes: 436, 437, 435  ## Resampling results: ##  ##   RMSE        Rsquared   MAE        ##   0.02188303  0.9044144  0.01273172 ##  ## Tuning parameter 'mtry' was held constant at a value of 2"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"target-oriented-validation","dir":"Articles","previous_headings":"Cross validation strategies for spatio-temporal data","what":"Target-oriented validation","title":"1. Introduction to CAST","text":"interested model performance view random subsets data loggers, need know well model able make predictions areas without data loggers. find , need repeatedly leave complete time series one data loggers use test data CV. first need create meaningful folds rather random folds. CAST’s function “CreateSpaceTimeFolds” designed provide index arguments used caret’s trainControl. index defines data points used model training model run reversely defines data points held back. Hence, using index argument can account dependencies data leaving complete data one data loggers (LLO CV), one time steps (LTO CV) data loggers time steps (LLTO CV). example ’re focusing LLO CV, therefore use column “SOURCEID” define location data logger split data folds using information. Analog random CV split data five folds, hence five model runs performed leaving one fifth data loggers validation. Note several suggestions spatial CV exist. call LLO just simple example. See references Meyer Pebesma (2022) examples. inspecting output model, see view new locations, R² 0.16 performance much lower expected random CV (R² = 0.90). Apparently, considerable overfitting model, causing good random performance poor performance view new locations. might partly attributed choice variables must suspect certain variables misinterpreted model (see Meyer et al 2018 [talk OpenGeoHub summer school 2019] (https://www.youtube.com/watch?v=mkHlmYEzsVQ)). Let’s look variable importance ranking Random Forest see find something suspicious:  importance ranking indicates among others, “Easting” important variable. fits observation inappropriate linear features predicted map. Apparently model assigns high importance variable causes high random CV performance. time model fails prediction new locations variable unsuitable predictions beyond locations data loggers used model training. Assuming certain variables misinterpreted algorithm able produce higher LLO performance variables removed. Let’s see true…","code":"set.seed(10) indices <- CreateSpacetimeFolds(trainDat,spacevar = \"SOURCEID\",                                 k=3) set.seed(10) model_LLO <- train(trainDat[,predictors],trainDat$VW,                    method=\"rf\",tuneGrid=data.frame(\"mtry\"=2), importance=TRUE,                    trControl=trainControl(method=\"cv\",                                           index = indices$index)) model_LLO ## Random Forest  ##  ## 654 samples ##  10 predictor ##  ## No pre-processing ## Resampling: Cross-Validated (10 fold)  ## Summary of sample sizes: 433, 430, 445  ## Resampling results: ##  ##   RMSE        Rsquared   MAE        ##   0.07645742  0.1616273  0.05994028 ##  ## Tuning parameter 'mtry' was held constant at a value of 2 plot(varImp(model_LLO))"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"removing-variables-that-cause-overfitting","dir":"Articles","previous_headings":"","what":"Removing variables that cause overfitting","title":"1. Introduction to CAST","text":"CAST’s forward feature selection (ffs) selects variables make sense view selected CV method excludes counterproductive (meaningless) view selected CV method. use LLO CV method, ffs selects variables lead combination highest LLO performance (.e. best spatial model). variables spatial meaning even counterproductive won’t improve even reduce LLO performance therefore excluded model ffs. ffs job first training models using possible pairs two predictor variables. best model initial models kept. basis best model predictor variables iterativly increased remaining variables tested improvement currently best model. process stops none remaining variables increases model performance added current best model. let’s run ffs case study using R² metric select optimal variables. process take 1-2 minutes… Using ffs LLO CV, R² increased 0.16 0.28. variables used model “DEM”,“NDRE.M” “Northing”. others removed (least small example) spatial meaning even counterproductive. Using plot\\(\\_\\)ffs function can visualize performance model changed depending variables used:  See best model using two variables led R² slightly 0.2. Using third variable slightly increase R². variable improve LLO performance. Note R² features high standard deviation regardless variables used. due small dataset used lead robust results. effect new model spatial representation soil moisture?  see variable selection effect statistical performance also predicted spatial patterns change considerably. note linear feature resulting soil moisture map likely “Easting” removed set predictor variables ffs.","code":"set.seed(10) ffsmodel_LLO <- ffs(trainDat[,predictors],trainDat$VW,metric=\"Rsquared\",                     method=\"rf\", tuneGrid=data.frame(\"mtry\"=2),                     verbose=FALSE,ntree=50,                     trControl=trainControl(method=\"cv\",                                            index = indices$index)) ffsmodel_LLO ## Random Forest  ##  ## 654 samples ##   3 predictor ##  ## No pre-processing ## Resampling: Cross-Validated (10 fold)  ## Summary of sample sizes: 433, 430, 445  ## Resampling results: ##  ##   RMSE       Rsquared   MAE       ##   0.1013101  0.2833983  0.0767997 ##  ## Tuning parameter 'mtry' was held constant at a value of 2 ffsmodel_LLO$selectedvars ## [1] \"DEM\"      \"NDRE.M\"   \"Northing\" plot_ffs(ffsmodel_LLO) prediction_ffs <- predict(predictors_sp,ffsmodel_LLO) spplot(prediction_ffs)"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"area-of-applicability","dir":"Articles","previous_headings":"","what":"Area of Applicability","title":"1. Introduction to CAST","text":"Still required analyse model can applied entire study area locations different predictor properties model learned . See details vignette Area applicability Meyer Pebesma 2021.   figure shows grey areas outside area applicability, hence predictions considered locations. See tutorial AOA package information.","code":"library(latticeExtra)  ### AOA for which the spatial CV error applies: AOA <- aoa(predictors_sp,ffsmodel_LLO)  spplot(prediction_ffs,main=\"prediction for the AOA \\n(spatial CV error applied)\")+ spplot(AOA$AOA,col.regions=c(\"grey\",\"transparent\")) ### AOA for which the random CV error applies: AOA_random <- aoa(predictors_sp,model) spplot(prediction,main=\"prediction for the AOA \\n(random CV error applied)\")+ spplot(AOA_random$AOA,col.regions=c(\"grey\",\"transparent\"))"},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"1. Introduction to CAST","text":"conclude, tutorial shown CAST can used facilitate target-oriented (: spatial) CV spatial spatio-temporal data crucial obtain meaningful validation results. Using ffs conjunction target-oriented validation, variables can excluded counterproductive view target-oriented performance due misinterpretations algorithm. ffs therefore helps select ideal set predictor variables spatio-temporal prediction tasks gives objective error estimates.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"final-notes","dir":"Articles","previous_headings":"","what":"Final notes","title":"1. Introduction to CAST","text":"intention tutorial describe motivation led development CAST well functionality. Priority modelling soil moisture cookfarm best possible way provide example motivation functionality CAST can run within minutes. Hence, small subset entire cookfarm dataset used. Keep mind due small subset example robust quite different results might obtained depending small changes settings. intention showing motivation CAST also reason coordinates used predictor variables. Though coordinates used predictors quite scientific studies rather provide extreme example misleading variables can lead overfitting.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast01-CAST-intro.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further reading","title":"1. Introduction to CAST","text":"Meyer, H., & Pebesma, E. (2022): Machine learning-based global maps ecological variables challenge assessing . Nature Communications. Accepted. Meyer, H., & Pebesma, E. (2021). Predicting unknown space? Estimating area applicability spatial prediction models. Methods Ecology Evolution, 12, 1620– 1633. [https://doi.org/10.1111/2041-210X.13650] Meyer H, Reudenbach C, Wöllauer S,Nauss T (2019) Importance spatial predictor variable selection machine learning applications–Moving data reproduction spatial prediction. Ecological Modelling 411: 108815 [https://doi.org/10.1016/j.ecolmodel.2019.108815] Meyer H, Reudenbach C, Hengl T, Katurij M, Nauss T (2018) Improving performance spatio-temporal machine learning models using forward feature selection target-oriented validation. Environmental Modelling & Software 101: 1–9 [https://doi.org/10.1016/j.envsoft.2017.12.001] Talk OpenGeoHub summer school 2019 spatial validation variable selection: https://www.youtube.com/watch?v=mkHlmYEzsVQ. Tutorial (https://youtu./EyP04zLe9qo) Lecture (https://youtu./OoNH6Nl-X2s) recording OpenGeoHub summer school 2020 area applicability. well talk OpenGeoHub summer school 2021: https://av.tib.eu/media/54879","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"2. Area of applicability of spatial prediction models","text":"spatial predictive mapping, models often applied make predictions far beyond sampling locations (.e. field observations used map variable even global scale), new locations might considerably differ environmental properties. However, areas predictor space without support training data problematic. model enabled learn relationships environments predictions areas considered highly uncertain. CAST, implement methodology described Meyer&Pebesma (2021) estimate “area applicability” (AOA) (spatial) prediction models. AOA defined area enabled model learn relationships based training data, estimated cross-validation performance holds. delineate AOA, first dissimilarity index (DI) calculated based distances training data multidimensional predictor variable space. account relevance predictor variables responsible prediction patterns weight variables model-derived importance scores prior distance calculation. AOA derived applying threshold based DI observed training data using cross-validation. tutorial shows example estimate area applicability spatial prediction models. information see: Meyer, H., & Pebesma, E. (2021). Predicting unknown space? Estimating area applicability spatial prediction models. Methods Ecology Evolution, 12, 1620– 1633. [https://doi.org/10.1111/2041-210X.13650]","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"getting-started","dir":"Articles","previous_headings":"Introduction","what":"Getting started","title":"2. Area of applicability of spatial prediction models","text":"","code":"library(CAST) library(virtualspecies) library(caret) library(raster) library(sp) library(sf) library(viridis) library(latticeExtra) library(gridExtra)"},{"path":[]},{"path":[]},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"generate-predictors","dir":"Articles","previous_headings":"Example 1: Using simulated data > Get data","what":"Generate Predictors","title":"2. Area of applicability of spatial prediction models","text":"predictor variables, set bioclimatic variables used (https://www.worldclim.org). tutorial, originally downloaded using getData function raster package cropped area central Europe. cropped data provided CAST package.","code":"predictors <- stack(system.file(\"extdata\",\"bioclim.grd\",package=\"CAST\")) spplot(stretch(predictors,0,1),col.regions=viridis(100))"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"generate-response","dir":"Articles","previous_headings":"Example 1: Using simulated data > Get data","what":"Generate Response","title":"2. Area of applicability of spatial prediction models","text":"able test reliability method, ’re using simulated prediction task virtualspecies package. Therefore, virtual response variable simulated bioclimatic variables. See Leroy et al. 2016 information methodology.","code":"response <- generateSpFromPCA(predictors,                               means = c(3,1),sds = c(2,2), plot=F)$suitab.raster"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"simulate-sampling-locations","dir":"Articles","previous_headings":"Example 1: Using simulated data > Get data","what":"Simulate sampling locations","title":"2. Area of applicability of spatial prediction models","text":"simulate typical prediction task, field sampling locations randomly selected. , randomly select 20 points. Note small data set, used avoid long computation times.","code":"mask <- predictors[[1]] values(mask)[!is.na(values(mask))] <- 1 mask <- rasterToPolygons(mask) set.seed(15) samplepoints <- spsample(mask,20,\"random\") spplot(response,col.regions=viridis(100),             sp.layout=list(\"sp.points\", samplepoints, col = \"red\", first = FALSE, cex=2))"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"model-training","dir":"Articles","previous_headings":"Example 1: Using simulated data","what":"Model training","title":"2. Area of applicability of spatial prediction models","text":"Next, machine learning algorithm applied learn relationships predictors response.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"prepare-data","dir":"Articles","previous_headings":"Example 1: Using simulated data > Model training","what":"Prepare data","title":"2. Area of applicability of spatial prediction models","text":"Therefore, predictors response extracted sampling locations.","code":"trainDat <- extract(predictors,samplepoints,df=TRUE) trainDat$response <- extract (response,samplepoints) trainDat <- trainDat[complete.cases(trainDat),]"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"train-the-model","dir":"Articles","previous_headings":"Example 1: Using simulated data > Model training","what":"Train the model","title":"2. Area of applicability of spatial prediction models","text":"Random Forest applied machine learning algorithm (others can used well, long variable importance returned). model validated default cross-validation estimate prediction error.","code":"set.seed(10) model <- train(trainDat[,names(predictors)],                trainDat$response,                method=\"rf\",                importance=TRUE,                trControl = trainControl(method=\"cv\")) print(model) ## Random Forest  ##  ## 20 samples ##  6 predictor ##  ## No pre-processing ## Resampling: Cross-Validated (10 fold)  ## Summary of sample sizes: 18, 18, 18, 18, 18, 18, ...  ## Resampling results across tuning parameters: ##  ##   mtry  RMSE       Rsquared  MAE       ##   2     0.1103267  1         0.0962007 ##   4     0.1200531  1         0.1025987 ##   6     0.1187460  1         0.1014125 ##  ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 2."},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"variable-importance","dir":"Articles","previous_headings":"Example 1: Using simulated data > Model training","what":"Variable importance","title":"2. Area of applicability of spatial prediction models","text":"estimation AOA require importance individual predictor variables.","code":"plot(varImp(model,scale = F),col=\"black\")"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"predict-and-calculate-error","dir":"Articles","previous_headings":"Example 1: Using simulated data > Model training","what":"Predict and calculate error","title":"2. Area of applicability of spatial prediction models","text":"trained model used make predictions entire area interest. Since simulated area-wide response used, ’s possible tutorial compare predictions true reference.","code":"prediction <- predict(predictors,model) truediff <- abs(prediction-response) spplot(stack(prediction,response),main=c(\"prediction\",\"reference\"))"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"aoa-calculation","dir":"Articles","previous_headings":"Example 1: Using simulated data","what":"AOA Calculation","title":"2. Area of applicability of spatial prediction models","text":"visualization shows predictions made model. next step, DI AOA calculated. AOA calculation takes model input extract importance predictors, used weights multidimensional distance calculation. Note AOA can also calculated without trained model (.e. using training data new data ). case predictor variables treated equally important (unless weights given form table). Plotting aoa object shows distribution DI values within training data DI new data. output aoa function two raster data: first DI normalized weighted minimum distance nearest training data point divided average distance within training data. AOA derived DI using threshold. threshold (outlier-removed) maximum DI observed training data DI training data calculated considering cross-validation folds. used threshold relevant information training data DI returned parameters list entry. can plot DI well predictions onyl AOA:  patterns DI general agreement true prediction error. high values present Alps, covered training data feature distinct environmental conditions. Since DI values areas threshold, regard area outside AOA.","code":"AOA <- aoa(predictors, model) class(AOA) ## [1] \"aoa\" names(AOA) ## [1] \"parameters\" \"DI\"         \"AOA\" print(AOA) ## DI: ## class      : RasterLayer  ## band       : 1  (of  6  bands) ## dimensions : 75, 126, 9450  (nrow, ncol, ncell) ## resolution : 0.1666667, 0.1666667  (x, y) ## extent     : 0, 21, 42.33333, 54.83333  (xmin, xmax, ymin, ymax) ## crs        : +proj=longlat +datum=WGS84 +no_defs  ## source     : memory ## names      : DI  ## values     : 0, 4.35002  (min, max) ##  ## AOA: ## class      : RasterLayer  ## dimensions : 75, 126, 9450  (nrow, ncol, ncell) ## resolution : 0.1666667, 0.1666667  (x, y) ## extent     : 0, 21, 42.33333, 54.83333  (xmin, xmax, ymin, ymax) ## crs        : +proj=longlat +datum=WGS84 +no_defs  ## source     : memory ## names      : AOA  ## values     : 0, 1  (min, max) ##  ##  ##  ## Predictor Weights: ##        bio2     bio5    bio10    bio13    bio14    bio19 ## 1 0.1683883 7.203433 10.84442 5.490218 8.340626 5.558477 ##  ##  ## AOA Threshold: 0.5850463 plot(AOA) grid.arrange(   spplot(truediff,col.regions=viridis(100),main=\"true prediction error\"),   spplot(AOA$DI,col.regions=viridis(100),main=\"DI\"),   spplot(prediction, col.regions=viridis(100),main=\"prediction for AOA\")+ spplot(AOA$AOA,col.regions=c(\"grey\",\"transparent\")), ncol=3)"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"aoa-for-spatially-clustered-data","dir":"Articles","previous_headings":"Example 1: Using simulated data","what":"AOA for spatially clustered data?","title":"2. Area of applicability of spatial prediction models","text":"example randomly distributed training samples. However, sampling locations might also highly clustered space. case, random cross-validation meaningful (see e.g. Meyer et al. 2018, Meyer et al. 2019, Valavi et al. 2019, Roberts et al. 2018, Pohjankukka et al. 2017, Brenning 2012) Also threshold AOA reliable, based distance nearest data point within training data (usually small data clustered). Instead, cross-validation based leave-cluster-approach, AOA estimation based distances nearest data point located spatial cluster. show looks like, use 15 spatial locations simulate 5 data points around location.  first train model (case) inappropriate random cross-validation. …model based leave-cluster-cross-validation. AOA calculated (comparison) using model validated random cross-validation, second taking spatial clusters account calculating threshold based minimum distances nearest training point located cluster. done aoa function, folds used cross-validation automatically extracted model.  Note AOA much larger spatial CV approach. However, spatial cross-validation error considerably larger, hence also area error applies larger. random cross-validation performance high, however, area performance applies small. fact also apparent plot aoa objects display distributions DI training data well DI new data. random CV predictionDI larger AOA threshold determined trainDI. Using spatial CV, predictionDI well within DI training samples.","code":"samplepoints <- csample(mask,75,15,maxdist=0.20,seed=15) spplot(response,col.regions=viridis(100),             sp.layout=list(\"sp.points\", samplepoints, col = \"red\", first = FALSE, cex=2)) trainDat <- extract(predictors,samplepoints,df=TRUE) trainDat$response <- extract (response,samplepoints) trainDat <- merge(trainDat,samplepoints,by.x=\"ID\",by.y=\"ID\") trainDat <- trainDat[complete.cases(trainDat),] set.seed(10) model_random <- train(trainDat[,names(predictors)],                trainDat$response,                method=\"rf\",                importance=TRUE,                trControl = trainControl(method=\"cv\")) prediction_random <- predict(predictors,model_random) print(model_random) ## Random Forest  ##  ## 74 samples ##  6 predictor ##  ## No pre-processing ## Resampling: Cross-Validated (10 fold)  ## Summary of sample sizes: 67, 66, 67, 67, 66, 68, ...  ## Resampling results across tuning parameters: ##  ##   mtry  RMSE        Rsquared   MAE        ##   2     0.04091038  0.9808718  0.02701173 ##   4     0.04229663  0.9754960  0.02747445 ##   6     0.04475604  0.9655655  0.02799310 ##  ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 2. folds <- CreateSpacetimeFolds(trainDat, spacevar=\"clstrID\",k=10) set.seed(15) model <- train(trainDat[,names(predictors)],                  trainDat$response,                      method=\"rf\",                  importance=TRUE,                  tuneGrid = expand.grid(mtry = c(2:length(names(predictors)))),                  trControl = trainControl(method=\"cv\",index=folds$index))   print(model) ## Random Forest  ##  ## 74 samples ##  6 predictor ##  ## No pre-processing ## Resampling: Cross-Validated (10 fold)  ## Summary of sample sizes: 65, 64, 69, 64, 69, 69, ...  ## Resampling results across tuning parameters: ##  ##   mtry  RMSE        Rsquared   MAE        ##   2     0.09699313  0.9400206  0.08501311 ##   3     0.09472860  0.8649564  0.08603081 ##   4     0.08904630  0.8859366  0.08042506 ##   5     0.08716000  0.8586236  0.07832948 ##   6     0.09411451  0.8148868  0.08357705 ##  ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 5. prediction <- predict(predictors,model) AOA_spatial <- aoa(predictors, model)  AOA_random <- aoa(predictors, model_random) grid.arrange(spplot(AOA_spatial$DI,col.regions=viridis(100),main=\"DI\"),   spplot(prediction, col.regions=viridis(100),main=\"prediction for AOA \\n(spatial CV error applies)\")+          spplot(AOA_spatial$AOA,col.regions=c(\"grey\",\"transparent\")),   spplot(prediction_random, col.regions=viridis(100),main=\"prediction for AOA \\n(random CV error applies)\")+          spplot(AOA_random$AOA,col.regions=c(\"grey\",\"transparent\")), ncol=3) grid.arrange(plot(AOA_spatial) + ggplot2::ggtitle(\"Spatial CV\"),              plot(AOA_random) + ggplot2::ggtitle(\"Random CV\"), ncol = 2)"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"comparison-prediction-error-with-model-error","dir":"Articles","previous_headings":"Example 1: Using simulated data","what":"Comparison prediction error with model error","title":"2. Area of applicability of spatial prediction models","text":"Since used simulated response variable, can now compare prediction error within AOA model error, assuming model error applies inside AOA outside. results indicate high agreement model CV error (RMSE) true prediction RMSE. case , random well spatial model.","code":"###for the spatial CV: RMSE(values(prediction)[values(AOA_spatial$AOA)==1],values(response)[values(AOA_spatial$AOA)==1]) ## [1] 0.1213942 RMSE(values(prediction)[values(AOA_spatial$AOA)==0],values(response)[values(AOA_spatial$AOA)==1]) ## [1] 0.5353775 model$results ##   mtry       RMSE  Rsquared        MAE     RMSESD RsquaredSD      MAESD ## 1    2 0.09699313 0.9400206 0.08501311 0.06988480 0.09556929 0.06224228 ## 2    3 0.09472860 0.8649564 0.08603081 0.06900899 0.19276546 0.06419389 ## 3    4 0.08904630 0.8859366 0.08042506 0.06986285 0.11612124 0.06564410 ## 4    5 0.08716000 0.8586236 0.07832948 0.07222770 0.13989462 0.06827500 ## 5    6 0.09411451 0.8148868 0.08357705 0.07655589 0.20333811 0.07406734 ###and for the random CV: RMSE(values(prediction_random)[values(AOA_random$AOA)==1],values(response)[values(AOA_random$AOA)==1]) ## [1] 0.01902935 RMSE(values(prediction_random)[values(AOA_random$AOA)==0],values(response)[values(AOA_random$AOA)==1]) ## [1] 0.3444048 model_random$results ##   mtry       RMSE  Rsquared        MAE     RMSESD RsquaredSD      MAESD ## 1    2 0.04091038 0.9808718 0.02701173 0.02830469 0.02843866 0.01841331 ## 2    4 0.04229663 0.9754960 0.02747445 0.02672590 0.03655193 0.01611715 ## 3    6 0.04475604 0.9655655 0.02799310 0.03175719 0.05163937 0.01799873"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"relationship-between-the-di-and-the-performance-measure","dir":"Articles","previous_headings":"Example 1: Using simulated data","what":"Relationship between the DI and the performance measure","title":"2. Area of applicability of spatial prediction models","text":"relationship error DI can used limit predictions area (within AOA) required performance (e.g. RMSE, R2, Kappa, Accuracy) applies. can done using result calibrate_aoa used relationship analyzed window DI values. corresponding model (: shape constrained additive models default: Monotone increasing P-splines dimension basis used represent smooth term 6 2nd order penalty.) can used estimate performance pixel level, allows limiting predictions using threshold. Note used multi-purpose CV estimate relationship DI RMSE (see details paper).","code":"AOA_calib <- calibrate_aoa(AOA_spatial,model,window.size = 5,length.out = 5, multiCV=TRUE,showPlot=FALSE) AOA_calib$plot spplot(AOA_calib$AOA$expected_RMSE,col.regions=viridis(100),main=\"expected RMSE\")+  spplot(AOA$AOA,col.regions=c(\"grey\",\"transparent\"))"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"example-2-a-real-world-example","dir":"Articles","previous_headings":"","what":"Example 2: A real-world example","title":"2. Area of applicability of spatial prediction models","text":"example used simulated data allows analyze reliability AOA. However, simulated area-wide response available usual prediction tasks. Therefore, second example AOA estimated dataset point observations reference .","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"data-and-preprocessing","dir":"Articles","previous_headings":"Example 2: A real-world example","what":"Data and preprocessing","title":"2. Area of applicability of spatial prediction models","text":", work cookfarm dataset, described e.g. Gasch et al 2015. dataset included CAST re-structured dataset. Find details also vignette “Introduction CAST”. use soil moisture (VW) response variable . Hence, ’re aiming making spatial continuous prediction based limited measurements data loggers.","code":"dat <- get(load(system.file(\"extdata\",\"Cookfarm.RData\",package=\"CAST\"))) # calculate average of VW for each sampling site: dat <- aggregate(dat[,c(\"VW\",\"Easting\",\"Northing\")],by=list(as.character(dat$SOURCEID)),mean) # create sf object from the data: pts <- st_as_sf(dat,coords=c(\"Easting\",\"Northing\"))  ##### Extract Predictors for the locations of the sampling points studyArea <- stack(system.file(\"extdata\",\"predictors_2012-03-25.grd\",package=\"CAST\")) st_crs(pts) <- crs(studyArea) trainDat <- extract(studyArea,pts,df=TRUE) pts$ID <- 1:nrow(pts) trainDat <- merge(trainDat,pts,by.x=\"ID\",by.y=\"ID\") # The final training dataset with potential predictors and VW: head(trainDat) ##   ID      DEM      TWI  BLD       NDRE.M   NDRE.Sd     Bt Easting Northing ## 1  1 788.1906 4.304258 1.42 -0.051189531 0.2506899 0.0000  493384  5180587 ## 2  2 788.3813 3.863605 1.29 -0.046459336 0.1754623 0.0000  493514  5180567 ## 3  3 790.5244 3.947488 1.36 -0.040845532 0.2225785 0.0000  493574  5180577 ## 4  4 775.7229 5.395786 1.55 -0.004329725 0.2099845 0.0501  493244  5180587 ## 5  5 796.7618 3.534822 1.31  0.027252737 0.2002646 0.0000  493624  5180607 ## 6  6 795.8370 3.815516 1.40 -0.123434804 0.2180606 0.0000  493694  5180607 ##   MinT_wrcc MaxT_wrcc Precip_cum  cday Precip_wrcc Group.1        VW ## 1       1.1      36.2       10.6 15425           0  CAF003 0.2938029 ## 2       1.1      36.2       10.6 15425           0  CAF007 0.2737227 ## 3       1.1      36.2       10.6 15425           0  CAF009 0.2723993 ## 4       1.1      36.2       10.6 15425           0  CAF019 0.3134010 ## 5       1.1      36.2       10.6 15425           0  CAF031 0.2751161 ## 6       1.1      36.2       10.6 15425           0  CAF033 0.2602674 ##                   geometry ## 1 POINT (493383.1 5180586) ## 2 POINT (493510.7 5180568) ## 3 POINT (493574.6 5180573) ## 4 POINT (493246.6 5180590) ## 5 POINT (493628.3 5180612) ## 6 POINT (493692.2 5180610)"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"model-training-and-prediction","dir":"Articles","previous_headings":"Example 2: A real-world example","what":"Model training and prediction","title":"2. Area of applicability of spatial prediction models","text":"set variables used predictors VW random Forest model. model validated leave one cross-validation. Note model performance low, due small dataset used (small dataset low ability predictors model VW).","code":"predictors <- c(\"DEM\",\"NDRE.Sd\",\"TWI\",\"Bt\") response <- \"VW\"  model <- train(trainDat[,predictors],trainDat[,response],                method=\"rf\",tuneLength=3,importance=TRUE,                trControl=trainControl(method=\"LOOCV\")) model ## Random Forest  ##  ## 42 samples ##  4 predictor ##  ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation  ## Summary of sample sizes: 41, 41, 41, 41, 41, 41, ...  ## Resampling results across tuning parameters: ##  ##   mtry  RMSE        Rsquared     MAE        ##   2     0.03842827  0.005425658  0.03051082 ##   3     0.03933811  0.001429581  0.03134197 ##   4     0.03938455  0.003577853  0.03185219 ##  ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 2."},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"prediction","dir":"Articles","previous_headings":"Example 2: A real-world example > Model training and prediction","what":"Prediction","title":"2. Area of applicability of spatial prediction models","text":"Next, model used make predictions entire study area.","code":"#Predictors: spplot(stretch(studyArea[[predictors]])) #prediction: prediction <- predict(studyArea,model)"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"aoa-estimation","dir":"Articles","previous_headings":"Example 2: A real-world example","what":"AOA estimation","title":"2. Area of applicability of spatial prediction models","text":"Next ’re limiting predictions AOA. Predictions outside AOA excluded.","code":"AOA <- aoa(studyArea,model)  #### Plot results: grid.arrange(spplot(AOA$DI,col.regions=viridis(100),main=\"DI with sampling locations (red)\")+                spplot(as_Spatial(pts),zcol=\"ID\",col.regions=\"red\"),   spplot(prediction, col.regions=viridis(100),main=\"prediction for AOA \\n(LOOCV error applies)\")+ spplot(AOA$AOA,col.regions=c(\"grey\",\"transparent\")),ncol=2)"},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"final-notes","dir":"Articles","previous_headings":"","what":"Final notes","title":"2. Area of applicability of spatial prediction models","text":"AOA estimated based training data new data (.e. raster stack entire area interest). trained model used getting variable importance needed weight predictor variables. can given table either, approach can used packages caret well. Knowledge AOA important predictions used baseline decision making subsequent environmental modelling. suggest AOA provided alongside prediction map complementary communication validation performances.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast02-AOA-tutorial.html","id":"further-reading","dir":"Articles","previous_headings":"Final notes","what":"Further reading","title":"2. Area of applicability of spatial prediction models","text":"Meyer, H., & Pebesma, E. (2022): Machine learning-based global maps ecological variables challenge assessing . Nature Communications. Accepted. Meyer, H., & Pebesma, E. (2021). Predicting unknown space? Estimating area applicability spatial prediction models. Methods Ecology Evolution, 12, 1620– 1633. [https://doi.org/10.1111/2041-210X.13650] Tutorial (https://youtu./EyP04zLe9qo) Lecture (https://youtu./OoNH6Nl-X2s) recording OpenGeoHub summer school 2020 area applicability. well talk OpenGeoHub summer school 2021: https://av.tib.eu/media/54879","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast03-AOA-parallel.html","id":"generate-example-data","dir":"Articles","previous_headings":"","what":"Generate Example Data","title":"3. AOA in Parallel","text":"","code":"library(CAST) library(virtualspecies) library(caret) library(raster) library(sp) library(sf) library(viridis) library(latticeExtra) library(gridExtra) predictors <- stack(system.file(\"extdata\",\"bioclim.grd\",package=\"CAST\")) response <- generateSpFromPCA(predictors,                               means = c(3,1),sds = c(2,2), plot=F)$suitab.raster   mask <- predictors[[1]] values(mask)[!is.na(values(mask))] <- 1 mask <- rasterToPolygons(mask)  # Generate Clustered Training Samples csample <- function(x,n,nclusters,maxdist,seed){   set.seed(seed)   cpoints <- sp::spsample(x, n = nclusters, type=\"random\")   result <- cpoints   result$clstrID <- 1:length(cpoints)   for (i in 1:length(cpoints)){     ext <- rgeos::gBuffer(cpoints[i,], width = maxdist)     newsamples <- sp::spsample(ext, n = (n-nclusters)/nclusters,                                 type=\"random\")     newsamples$clstrID <- rep(i,length(newsamples))     result <- rbind(result,newsamples)        }   result$ID <- 1:nrow(result)   return(result) }   samplepoints <- csample(mask,75,15,maxdist=0.20,seed=15)   trainDat <- extract(predictors,samplepoints,df=TRUE) trainDat$response <- extract (response,samplepoints) trainDat <- merge(trainDat,samplepoints,by.x=\"ID\",by.y=\"ID\") trainDat <- trainDat[complete.cases(trainDat),] set.seed(10) model_random <- train(trainDat[,names(predictors)],                trainDat$response,                method=\"rf\",                importance=TRUE,                trControl = trainControl(method=\"cv\")) prediction_random <- raster::predict(predictors,model_random)"},{"path":"https://hannameyer.github.io/CAST/articles/cast03-AOA-parallel.html","id":"parallel-aoa-1-providing-a-cluster","dir":"Articles","previous_headings":"","what":"Parallel AOA 1: Providing a Cluster","title":"3. AOA in Parallel","text":"simplest methods compute AOA parallel providing cluster object function call. way distance calculation training data new data run multiple cores (utilizing parallel::parApply). recommended training set relatively small new locations AOA computed abundant.","code":"library(doParallel) library(parallel) cl <- makeCluster(4) registerDoParallel(cl) AOA <- aoa(studyArea,model,cl=cl)"},{"path":"https://hannameyer.github.io/CAST/articles/cast03-AOA-parallel.html","id":"parallel-aoa-2-divide-and-conquer","dir":"Articles","previous_headings":"","what":"Parallel AOA 2: Divide and Conquer","title":"3. AOA in Parallel","text":"even better performances, recommended compute AOA two steps. First, DI training data resulting DI threshold computed model training data function trainDI. result trainDI usually first step aoa function, however can skipped providing trainDI object function call. makes possible compute AOA multiple raster tiles (e.g. different cores). especially useful large prediction areas, e.g. global mapping. large raster, divide multiple smaller tiles apply trainDI object afterwards tile.  Use trainDI argument aoa function specify, want use previously computed trainDI object.  can now run aoa function parallel different tiles! course can use favorite parallel backend task, use mclapply parallel package.  larger tasks might useful save tiles hard-drive load one one avoid filling RAM.","code":"model_random_trainDI = trainDI(model_random) print(model_random_trainDI) ## DI of 74 observation  ## Predictors: bio2 bio5 bio10 bio13 bio14 bio19  ##  ## AOA Threshold: 0.08928021 saveRDS(model_random_trainDI, \"path/to/file\") r1 = crop(predictors, c(0,7,42.33333,54.83333)) r2 = crop(predictors, c(7,14,42.33333,54.83333)) r3 = crop(predictors, c(14,21,42.33333,54.83333))  grid.arrange(spplot(r1[[1]], main = \"Tile 1\"),              spplot(r2[[1]], main = \"Tile 2\"),              spplot(r3[[1]], main = \"Tile 3\"), ncol = 3) aoa_r1 = aoa(newdata = r1, trainDI = model_random_trainDI)  grid.arrange(spplot(r1[[1]], main = \"Tile 1: Predictors\"),              spplot(aoa_r1$DI, main = \"Tile 1: DI\"),              spplot(aoa_r1$AOA, main = \"Tile 1: AOA\"), ncol = 3) library(parallel)  tiles_aoa = mclapply(list(r1, r2, r3), function(tile){   aoa(newdata = tile, trainDI = model_random_trainDI)    }, mc.cores = 3) grid.arrange(spplot(tiles_aoa[[1]]$AOA, main = \"Tile 1\"),              spplot(tiles_aoa[[2]]$AOA, main = \"Tile 2\"),              spplot(tiles_aoa[[3]]$AOA, main = \"Tile 3\"), ncol = 3) # Simple Example Code for raster tiles on the hard drive  tiles = list.files(\"path/to/tiles\", full.names = TRUE)  tiles_aoa = mclapply(tiles, function(tile){   current = raster::stack(tile)   aoa(newdata = current, trainDI = model_random_trainDI)    }, mc.cores = 3)"},{"path":"https://hannameyer.github.io/CAST/articles/cast03-AOA-parallel.html","id":"final-remarks","dir":"Articles","previous_headings":"","what":"Final Remarks","title":"3. AOA in Parallel","text":"possible combine parallelization methods! example High Performance Cluster, multiple raster tiles can handled different nodes , node computes distance prediction training data multiple cores.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast04-plotgeodist.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"4. Visualization of nearest neighbor distance distributions","text":"tutorial shows euclidean nearest neighbor distances geographic space feature space can calculated visualized using CAST. type visualization allows assess whether training data feature representative coverage prediction area cross-validation (CV) folds (independent test data) adequately chosen representative prediction locations. See e.g. Meyer Pebesma (2022) Milà et al. (2022) discussion topic.","code":""},{"path":"https://hannameyer.github.io/CAST/articles/cast04-plotgeodist.html","id":"sample-data","dir":"Articles","previous_headings":"","what":"Sample data","title":"4. Visualization of nearest neighbor distance distributions","text":"example data, use two different sets global virtual reference data: One spatial random sample second example, reference data clustered geographic space (see Meyer Pebesma (2022) discussions ). can define parameters run example different settings","code":"library(CAST) library(caret) library(raster) library(sf) library(rnaturalearth) library(ggplot2) seed <- 10 # random realization samplesize <- 300 # how many samples will be used? nparents <- 20 #For clustered samples: How many clusters?  radius <- 7 # For clustered samples: What is the radius of a cluster?"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-plotgeodist.html","id":"prediction-area","dir":"Articles","previous_headings":"Sample data","what":"Prediction area","title":"4. Visualization of nearest neighbor distance distributions","text":"prediction area entire global land area, .e. imagine prediction task aim making global predictions based set reference data.","code":"ee <- st_crs(\"+proj=eqearth\") co <- ne_countries(returnclass = \"sf\") co.ee <- st_transform(co, ee)"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-plotgeodist.html","id":"spatial-random-sample","dir":"Articles","previous_headings":"Sample data","what":"Spatial random sample","title":"4. Visualization of nearest neighbor distance distributions","text":", simulate random sample visualize data entire global prediction area.","code":"sf_use_s2(FALSE) set.seed(seed) pts_random <- st_sample(co, samplesize) ### See points on the map: ggplot() + geom_sf(data = co.ee, fill=\"#00BFC4\",col=\"#00BFC4\") +   geom_sf(data = pts_random, color = \"#F8766D\",size=0.5, shape=3) +   guides(fill = FALSE, col = FALSE) +   labs(x = NULL, y = NULL)"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-plotgeodist.html","id":"clustered-sample","dir":"Articles","previous_headings":"Sample data","what":"Clustered sample","title":"4. Visualization of nearest neighbor distance distributions","text":"second data set use clustered design size.","code":"set.seed(seed) sf_use_s2(FALSE) pts_clustered <- clustered_sample(co, samplesize, nparents, radius)  ggplot() + geom_sf(data = co.ee, fill=\"#00BFC4\",col=\"#00BFC4\") +   geom_sf(data = pts_clustered, color = \"#F8766D\",size=0.5, shape=3) +   guides(fill = FALSE, col = FALSE) +   labs(x = NULL, y = NULL)"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-plotgeodist.html","id":"distances-in-geographic-space","dir":"Articles","previous_headings":"","what":"Distances in geographic space","title":"4. Visualization of nearest neighbor distance distributions","text":"can plot distributions spatial distances reference data nearest neighbor (“sample--sample”) distribution distances points global land surface nearest reference data point (“sample--prediction”). Note samples prediction locations used calculate sample--prediction nearest neighbor distances. Since ’re using global case study , throughout tutorial use sampling=Fibonacci draw prediction locations constant point density sphere.   Note random data set nearest neighbor distance distribution training data quasi identical nearest neighbor distance distribution prediction area. comparison, second data set number training data heavily clustered geographic space. therefore see nearest neighbor distances within reference data rather small. Prediction locations, however, average much away.","code":"dist_random <- plot_geodist(pts_random,co,                             sampling=\"Fibonacci\",                             unit=\"km\",                             showPlot = FALSE) dist_clstr <- plot_geodist(pts_clustered,co,                            sampling=\"Fibonacci\",                            unit=\"km\",                            showPlot = FALSE)  dist_random$plot+scale_x_log10(labels=round)+ggtitle(\"Randomly distributed reference data\") dist_clstr$plot+scale_x_log10(labels=round)+ggtitle(\"Clustered reference data\")"},{"path":[]},{"path":"https://hannameyer.github.io/CAST/articles/cast04-plotgeodist.html","id":"random-cross-validation","dir":"Articles","previous_headings":"Distances in geographic space > Accounting for cross-validation folds","what":"Random Cross-validation","title":"4. Visualization of nearest neighbor distance distributions","text":"Let’s use clustered data set show distribution spatial nearest neighbor distances cross-validation can visualized well. Therefore, first use “default” way random 10-fold cross validation randomly split reference data training test (see Meyer et al., 2018 2019 see might good idea).   Obviously CV folds representative prediction locations (least terms distance nearest training data point). .e. folds used performance assessment model, can expect overly optimistic estimates validate predictions close proximity reference data.","code":"randomfolds <- caret::createFolds(1:nrow(pts_clustered)) dist_clstr <- plot_geodist(pts_clustered,co,                            sampling=\"Fibonacci\",                             cvfolds= randomfolds,                             unit=\"km\",                            showPlot=FALSE) dist_clstr$plot+scale_x_log10(labels=round)"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-plotgeodist.html","id":"spatial-cross-validation","dir":"Articles","previous_headings":"Distances in geographic space > Accounting for cross-validation folds","what":"Spatial Cross-validation","title":"4. Visualization of nearest neighbor distance distributions","text":", however, case CV performance regarded representative prediction task. Therefore, use spatial CV instead. , use leave-cluster-CV, means iteration, one spatial clusters held back.   See fits nearest neighbor distribution prediction area much better. Note plot_geodist also allows inspecting independent test data instead cross validation folds. See ?plot_geodist.","code":"spatialfolds <- CreateSpacetimeFolds(pts_clustered,spacevar=\"parent\",k=length(unique(pts_clustered$parent))) dist_clstr <- plot_geodist(pts_clustered,co,                            sampling=\"Fibonacci\",                            cvfolds= spatialfolds$indexOut,                             unit=\"km\",                            showPlot=FALSE) dist_clstr$plot+scale_x_log10(labels=round)"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-plotgeodist.html","id":"why-has-spatial-cv-sometimes-blamed-for-being-too-pessimistic","dir":"Articles","previous_headings":"Distances in geographic space > Accounting for cross-validation folds","what":"Why has spatial CV sometimes blamed for being too pessimistic ?","title":"4. Visualization of nearest neighbor distance distributions","text":"Recently, Wadoux et al. (2021) published paper title “Spatial cross-validation right way evaluate map accuracy” state “spatial cross-validation strategies resulted grossly pessimistic map accuracy assessment”. come conclusion? reference data used study either regularly, random comparably mildly clustered geographic space, applied spatial CV strategies held large spatial units back CV. can see happens apply spatial CV randomly distributed reference data.   see nearest neighbor distances cross-validation don’t match nearest neighbor distances prediction. compared section , time cross-validation folds far away reference data. Naturally end overly pessimistic performance estimates make prediction situations cross-validation harder, compared required model application entire area interest (global). spatial CV chosen therefore suitable prediction task, prediction situations created CV resemble encountered prediction.","code":"# create a spatial CV for the randomly distributed data. Here: # \"leave region-out-CV\" sf_use_s2(FALSE) pts_random_co <- st_join(st_as_sf(pts_random),co)   ggplot() + geom_sf(data = co.ee, fill=\"#00BFC4\",col=\"#00BFC4\") +   geom_sf(data = pts_random_co, aes(color=subregion),size=0.5, shape=3) +   scale_color_manual(values=rainbow(length(unique(pts_random_co$subregion))))+   guides(fill = FALSE, col = FALSE) +   labs(x = NULL, y = NULL)+ ggtitle(\"spatial fold membership by color\") spfolds_rand <- CreateSpacetimeFolds(pts_random_co,spacevar = \"subregion\",                                      k=length(unique(pts_random_co$subregion))) dist_rand_sp <- plot_geodist(pts_random_co,co,                              sampling=\"Fibonacci\",                               cvfolds= spfolds_rand$indexOut,                               unit=\"km\",                              showPlot=FALSE) dist_rand_sp$plot+scale_x_log10(labels=round)"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-plotgeodist.html","id":"nearest-neighbour-distance-matching-cv","dir":"Articles","previous_headings":"Distances in geographic space > Accounting for cross-validation folds","what":"Nearest Neighbour Distance Matching CV","title":"4. Visualization of nearest neighbor distance distributions","text":"good way approximate geographical prediction distances CV use Nearest Neighbour Distance Matching (NNDM) CV (see Milà et al., 2022 details). NNDM CV variation LOO CV empirical distribution function nearest neighbour distances found prediction matched CV process.  NNDM CV-distance distribution matches sample--prediction distribution well. happens use NNDM CV randomly-distributed sampling points instead?  NNDM CV-distance still matches sample--prediction distance function.","code":"nndmfolds_clstr <- nndm(pts_clustered, modeldomain=co, sampling = \"Fibonacci\",samplesize = 2000) dist_clstr <- plot_geodist(pts_clustered,co,                            sampling = \"Fibonacci\",                            cvfolds = nndmfolds_clstr$indx_test,                             cvtrain = nndmfolds_clstr$indx_train,                            unit=\"km\",                            showPlot = FALSE) dist_clstr$plot+scale_x_log10(labels=round) nndmfolds_rand <- nndm(pts_random_co,  modeldomain=co, sampling = \"Fibonacci\",samplesize = 2000) dist_rand <- plot_geodist(pts_random_co,co,                           sampling = \"Fibonacci\",                           cvfolds = nndmfolds_rand$indx_test,                            cvtrain = nndmfolds_rand$indx_train,                           unit=\"km\",                           showPlot = FALSE) dist_rand$plot+scale_x_log10(labels=round)"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-plotgeodist.html","id":"distances-in-feature-space","dir":"Articles","previous_headings":"","what":"Distances in feature space","title":"4. Visualization of nearest neighbor distance distributions","text":"far compared nearest neighbor distances geographic space. can also feature space. Therefore, set bioclimatic variables used (https://www.worldclim.org) features (.e. predictors) virtual prediction task.  visualize nearest neighbor feature space distances consideration cross-validation.   regard chosen predictor variables see nearest neighbor distance clustered training data rather small, compared required prediction. random CV representative prediction locations spatial CV better job.","code":"predictors_global <- stack(system.file(\"extdata\",\"bioclim_global.grd\",package=\"CAST\"))  plot(predictors_global) # use random CV: dist_clstr_rCV <- plot_geodist(pts_clustered,predictors_global,                                type = \"feature\",                                 sampling=\"Fibonacci\",                                cvfolds = randomfolds,                                showPlot=FALSE)  # use spatial CV: dist_clstr_sCV <- plot_geodist(pts_clustered,predictors_global,                                type = \"feature\", sampling=\"Fibonacci\",                                cvfolds = spatialfolds$indexOut,                                showPlot=FALSE)   # Plot results: dist_clstr_rCV$plot+scale_x_log10()+ggtitle(\"Clustered reference data and random CV\") dist_clstr_sCV$plot+scale_x_log10()+ggtitle(\"Clustered reference data and spatial CV\")"},{"path":"https://hannameyer.github.io/CAST/articles/cast04-plotgeodist.html","id":"references","dir":"Articles","previous_headings":"Distances in feature space","what":"References","title":"4. Visualization of nearest neighbor distance distributions","text":"Meyer, H., Pebesma, E. (2022): Machine learning-based global maps ecological variables challenge assessing . Nature Communications 13, 2208. https://doi.org/10.1038/s41467-022-29838-9 Milà, C., Mateu, J., Pebesma, E., Meyer, H. (2022): Nearest Neighbour Distance Matching Leave-One-Cross-Validation map validation. Methods Ecology Evolution 00, 1– 13. https://doi.org/10.1111/2041-210X.13851.","code":""},{"path":"https://hannameyer.github.io/CAST/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Hanna Meyer. Maintainer, author. Carles Milà. Author. Marvin Ludwig. Author. Chris Reudenbach. Contributor. Thomas Nauss. Contributor. Edzer Pebesma. Contributor.","code":""},{"path":"https://hannameyer.github.io/CAST/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Meyer H, Milà C, Ludwig M (2022). CAST: 'caret' Applications Spatial-Temporal Models. https://github.com/HannaMeyer/CAST, https://hannameyer.github.io/CAST/.","code":"@Manual{,   title = {CAST: 'caret' Applications for Spatial-Temporal Models},   author = {Hanna Meyer and Carles Milà and Marvin Ludwig},   year = {2022},   note = {https://github.com/HannaMeyer/CAST, https://hannameyer.github.io/CAST/}, }"},{"path":"https://hannameyer.github.io/CAST/index.html","id":"cast-caret-applications-for-spatio-temporal-models","dir":"","previous_headings":"","what":"caret Applications for Spatial-Temporal Models","title":"caret Applications for Spatial-Temporal Models","text":"Supporting functionality run ‘caret’ spatial spatial-temporal data. ‘caret’ frequently used package model training prediction using machine learning. CAST includes functions improve spatial spatial-temporal modelling tasks using ‘caret’. decrease spatial overfitting improve model performances, package implements forward feature selection selects suitable predictor variables view contribution spatial spatio-temporal model performance. CAST includes functionality estimate (spatial) area applicability prediction models. Note: developer version CAST can found https://github.com/HannaMeyer/CAST. CRAN Version can found https://CRAN.R-project.org/package=CAST","code":""},{"path":"https://hannameyer.github.io/CAST/index.html","id":"package-website","dir":"","previous_headings":"","what":"Package Website","title":"caret Applications for Spatial-Temporal Models","text":"https://hannameyer.github.io/CAST/","code":""},{"path":"https://hannameyer.github.io/CAST/index.html","id":"tutorials","dir":"","previous_headings":"","what":"Tutorials","title":"caret Applications for Spatial-Temporal Models","text":"Introduction CAST Area applicability spatial prediction models Area applicability parallel Visualization nearest neighbor distance distributions talk OpenGeoHub summer school 2019 spatial validation variable selection: https://www.youtube.com/watch?v=mkHlmYEzsVQ. Tutorial (https://youtu./EyP04zLe9qo) Lecture (https://youtu./OoNH6Nl-X2s) recording OpenGeoHub summer school 2020 area applicability. well talk OpenGeoHub summer school 2021: https://av.tib.eu/media/54879","code":""},{"path":"https://hannameyer.github.io/CAST/index.html","id":"scientific-documentation-of-the-methods","dir":"","previous_headings":"","what":"Scientific documentation of the methods","title":"caret Applications for Spatial-Temporal Models","text":"Meyer, H., Reudenbach, C., Hengl, T., Katurji, M., Nauss, T. (2018): Improving performance spatio-temporal machine learning models using forward feature selection target-oriented validation. Environmental Modelling & Software, 101, 1-9. https://doi.org/10.1016/j.envsoft.2017.12.001 Meyer, H., Reudenbach, C., Wöllauer, S., Nauss, T. (2019): Importance spatial predictor variable selection machine learning applications - Moving data reproduction spatial prediction. Ecological Modelling. 411. https://doi.org/10.1016/j.ecolmodel.2019.108815 Meyer, H., Pebesma, E. (2021). Predicting unknown space? Estimating area applicability spatial prediction models. Methods Ecology Evolution, 12, 1620– 1633. https://doi.org/10.1111/2041-210X.13650 Meyer, H., Pebesma, E. (2022): Machine learning-based global maps ecological variables challenge assessing . Nature Communications, 13. https://www.nature.com/articles/s41467-022-29838-9 Milà, C., Mateu, J., Pebesma, E., Meyer, H. (2022): Nearest Neighbour Distance Matching Leave-One-Cross-Validation map validation. Methods Ecology Evolution 00, 1– 13. https://doi.org/10.1111/2041-210X.13851","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CAST.html","id":null,"dir":"Reference","previous_headings":"","what":"'caret' Applications for Spatial-Temporal Models — CAST","title":"'caret' Applications for Spatial-Temporal Models — CAST","text":"Supporting functionality run 'caret' spatial spatial-temporal data. 'caret' frequently used package model training prediction using machine learning. CAST includes functions improve spatial-temporal modelling tasks using 'caret'. supports Leave-Location-Leave-Time-cross-validation spatial spatial-temporal models allows spatial variable selection selects suitable predictor variables view contribution spatial model performance. CAST includes functionality estimate (spatial) area applicability prediction models analysing similarity new data training data.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CAST.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"'caret' Applications for Spatial-Temporal Models — CAST","text":"'caret' Applications Spatio-Temporal models","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CAST.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"'caret' Applications for Spatial-Temporal Models — CAST","text":"Milà, C., Mateu, J., Pebesma, E., Meyer, H. (2022): Nearest Neighbour Distance Matching Leave-One-Cross-Validation map validation. Methods Ecology Evolution 00, 1– 13. Meyer, H., Pebesma, E. (2022): Machine learning-based global maps ecological variables challenge assessing . Nature Communications. 13. Meyer, H., Pebesma, E. (2021): Predicting unknown space? Estimating area applicability spatial prediction models. Methods Ecology Evolution. 12, 1620– 1633. Meyer, H., Reudenbach, C., Wöllauer, S., Nauss, T. (2019): Importance spatial predictor variable selection machine learning applications - Moving data reproduction spatial prediction. Ecological Modelling. 411, 108815. Meyer, H., Reudenbach, C., Hengl, T., Katurji, M., Nauß, T. (2018): Improving performance spatio-temporal machine learning models using forward feature selection target-oriented validation. Environmental Modelling & Software 101: 1-9.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CAST.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"'caret' Applications for Spatial-Temporal Models — CAST","text":"Hanna Meyer, Carles Milà, Marvin Ludwig","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Space-time Folds — CreateSpacetimeFolds","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"Create spatial, temporal spatio-temporal Folds cross validation based pre-defined groups","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"","code":"CreateSpacetimeFolds(   x,   spacevar = NA,   timevar = NA,   k = 10,   class = NA,   seed = sample(1:1000, 1) )"},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"x data.frame containing spatio-temporal data spacevar Character indicating column x identifies spatial units (e.g. ID weather stations) timevar Character indicating column x identifies temporal units (e.g. day year) k numeric. Number folds. spacevar timevar NA leave one location leave one time step cv performed, set k number unique spatial temporal units. class Character indicating column x identifies class unit (e.g. land cover) seed numeric. See ?seed","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"list contains list model training list model validation can directly used \"index\" \"indexOut\" caret's trainControl function","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"function creates train test sets taking (spatial /temporal) groups account. contrast nndm, requires groups already defined (e.g. spatial clusters blocks temporal units). Using \"class\" helpful case data clustered space categorical. E.g case land cover classifications training data come training polygons. case data split way entire polygons held back (spacevar=\"polygonID\") time distribution classes similar fold (class=\"LUC\").","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"Standard k-fold cross-validation can lead considerable misinterpretation spatial-temporal modelling tasks. function can used prepare Leave-Location-, Leave-Time-Leave-Location--Time-cross-validation target-oriented validation strategies spatial-temporal prediction tasks. See Meyer et al. (2018) information.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"Meyer, H., Reudenbach, C., Hengl, T., Katurji, M., Nauß, T. (2018): Improving performance spatio-temporal machine learning models using forward feature selection target-oriented validation. Environmental Modelling & Software 101: 1-9.","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"Hanna Meyer","code":""},{"path":"https://hannameyer.github.io/CAST/reference/CreateSpacetimeFolds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Space-time Folds — CreateSpacetimeFolds","text":"","code":"dat <- get(load(system.file(\"extdata\",\"Cookfarm.RData\",package=\"CAST\"))) ### Prepare for 10-fold Leave-Location-and-Time-Out cross validation indices <- CreateSpacetimeFolds(dat,\"SOURCEID\",\"Date\") str(indices) #> List of 2 #>  $ index   :List of 10 #>   ..$ : int [1:140853] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:149038] 1 2 3 4 5 11 12 13 14 15 ... #>   ..$ : int [1:149938] 6 7 8 9 10 11 12 13 14 15 ... #>   ..$ : int [1:144298] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:152700] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:141330] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:148138] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:154682] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:152875] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:149962] 291 294 296 298 300 302 303 305 306 308 ... #>  $ indexOut:List of 10 #>   ..$ : int [1:2638] 1097 1105 1113 1120 1126 1315 1324 1333 1342 1343 ... #>   ..$ : int [1:1796] 1589 1595 1600 1606 1612 1618 1624 1629 1636 1643 ... #>   ..$ : int [1:1637] 314 317 320 323 326 431 434 437 440 443 ... #>   ..$ : int [1:2184] 1066 1074 1081 1088 1095 1231 1241 1251 1260 1269 ... #>   ..$ : int [1:1332] 1138 1147 1156 1164 1170 1713 1719 1724 1731 1738 ... #>   ..$ : int [1:2585] 577 581 585 589 593 717 721 725 729 733 ... #>   ..$ : int [1:1905] 373 376 381 384 487 491 495 499 503 2448 ... #>   ..$ : int [1:1093] 1179 1189 1199 1209 1217 1445 1453 1460 1468 1476 ... #>   ..$ : int [1:1304] 3202 3224 3246 3268 3289 5120 5132 5143 5154 5164 ... #>   ..$ : int [1:1660] 31 32 33 34 35 106 107 108 109 110 ... ### Prepare for 10-fold Leave-Location-Out cross validation indices <- CreateSpacetimeFolds(dat,spacevar=\"SOURCEID\") str(indices) #> List of 2 #>  $ index   :List of 10 #>   ..$ : int [1:157443] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:169381] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:165544] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:170928] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:159718] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:166830] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:165214] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:163613] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:166070] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:164149] 291 294 296 298 300 302 305 308 311 314 ... #>  $ indexOut:List of 10 #>   ..$ : int [1:25767] 1205 1213 1253 1261 1298 1306 1342 1350 1385 1393 ... #>   ..$ : int [1:13829] 526 532 538 544 550 1062 1070 1078 1085 1091 ... #>   ..$ : int [1:17666] 2207 2277 2354 2447 2465 2484 2505 2525 2546 2564 ... #>   ..$ : int [1:12282] 448 452 456 460 464 468 472 476 480 484 ... #>   ..$ : int [1:23492] 291 294 296 298 300 302 305 308 311 314 ... #>   ..$ : int [1:16380] 530 536 542 548 554 558 562 566 570 574 ... #>   ..$ : int [1:17996] 2183 2191 2201 2211 2218 2233 2239 2247 2254 2262 ... #>   ..$ : int [1:19597] 1060 1068 1076 1083 1089 1097 1105 1113 1120 1126 ... #>   ..$ : int [1:17140] 1179 1189 1199 1209 1217 1227 1237 1247 1257 1265 ... #>   ..$ : int [1:19061] 1 2 3 4 5 6 7 8 9 10 ... ### Prepare for leave-One-Location-Out cross validation indices <- CreateSpacetimeFolds(dat,spacevar=\"SOURCEID\", k=length(unique(dat$SOURCEID))) str(indices) #> List of 2 #>  $ index   :List of 42 #>   ..$ : int [1:177689] 291 294 296 298 300 302 303 305 306 308 ... #>   ..$ : int [1:178560] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178144] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178043] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178096] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178715] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178887] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:180331] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178005] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178619] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:179049] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178599] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178969] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178547] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178108] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178489] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178629] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178747] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178004] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:177939] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178483] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178609] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178284] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178620] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:179932] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178231] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:179250] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178643] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178506] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:179638] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178773] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178164] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:179085] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:179431] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:178994] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:179373] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:180191] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:179380] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:179715] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:179701] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:180629] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:179809] 1 2 3 4 5 6 7 8 9 10 ... #>  $ indexOut:List of 42 #>   ..$ : int [1:5521] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ : int [1:4650] 291 294 296 298 300 302 305 308 311 314 ... #>   ..$ : int [1:5066] 303 306 309 312 315 318 321 324 327 330 ... #>   ..$ : int [1:5167] 448 452 456 460 464 468 472 476 480 484 ... #>   ..$ : int [1:5114] 526 532 538 544 550 1062 1070 1078 1085 1091 ... #>   ..$ : int [1:4495] 530 536 542 548 554 558 562 566 570 574 ... #>   ..$ : int [1:4323] 1060 1068 1076 1083 1089 1097 1105 1113 1120 1126 ... #>   ..$ : int [1:2879] 1061 1069 1077 1084 1090 1098 1106 1114 1121 1127 ... #>   ..$ : int [1:5205] 1138 1147 1156 1164 1170 1180 1190 1200 1210 1218 ... #>   ..$ : int [1:4591] 1179 1189 1199 1209 1217 1227 1237 1247 1257 1265 ... #>   ..$ : int [1:4161] 1205 1213 1253 1261 1298 1306 1342 1350 1385 1393 ... #>   ..$ : int [1:4611] 2172 2221 2297 2374 2389 2403 2419 2435 2452 2472 ... #>   ..$ : int [1:4241] 2183 2191 2201 2211 2239 2254 2269 2285 2315 2330 ... #>   ..$ : int [1:4663] 2207 2277 2354 2759 2779 2802 2826 2848 2871 2891 ... #>   ..$ : int [1:5102] 2218 2233 2247 2262 2278 2294 2309 2323 2339 2355 ... #>   ..$ : int [1:4721] 2219 2234 2248 2263 2279 2295 2310 2324 2340 2356 ... #>   ..$ : int [1:4581] 2222 2251 2266 2282 2298 2327 2343 2359 2375 2404 ... #>   ..$ : int [1:4463] 2229 2244 2259 2273 2290 2305 2320 2335 2350 2367 ... #>   ..$ : int [1:5206] 2230 2245 2274 2291 2306 2321 2336 2351 2368 2382 ... #>   ..$ : int [1:5271] 2235 2249 2264 2280 2311 2325 2341 2357 2387 2401 ... #>   ..$ : int [1:4727] 2447 2465 2484 2505 2525 2546 2564 2583 2604 2624 ... #>   ..$ : int [1:4601] 2449 2467 2507 2527 2548 2566 2606 2626 2647 2668 ... #>   ..$ : int [1:4926] 2460 2480 2500 2520 2541 2559 2579 2599 2619 2640 ... #>   ..$ : int [1:4590] 2470 2488 2530 2569 2587 2629 2671 2693 2741 2785 ... #>   ..$ : int [1:3278] 2490 2589 2673 2696 2720 2787 2810 2833 2899 2921 ... #>   ..$ : int [1:4979] 2649 2694 2718 2742 2764 2786 2809 2832 2855 2877 ... #>   ..$ : int [1:3960] 4525 4539 4552 4566 4581 4596 4610 4623 4637 4652 ... #>   ..$ : int [1:4567] 5120 5132 5143 5154 5164 5175 5187 5198 5209 5219 ... #>   ..$ : int [1:4704] 5962 6039 6066 6092 6165 6191 6216 6289 6315 6340 ... #>   ..$ : int [1:3572] 5968 5994 6019 6044 6072 16417 16445 16556 16583 16694 ... #>   ..$ : int [1:4437] 5995 6020 6045 6073 6123 6147 6170 6197 6247 6271 ... #>   ..$ : int [1:5046] 7331 7379 7404 7430 7452 7496 7518 7541 7563 7607 ... #>   ..$ : int [1:4125] 12703 12704 12705 12706 12707 12708 12709 12710 12711 12712 ... #>   ..$ : int [1:3779] 13088 13099 13110 13121 13132 13144 13155 13166 13177 13188 ... #>   ..$ : int [1:4216] 13378 13413 13448 13466 13502 13538 13556 13592 13628 13646 ... #>   ..$ : int [1:3837] 13388 13406 13440 13459 13476 13494 13530 13549 13566 13584 ... #>   ..$ : int [1:3019] 36634 36648 36664 36681 36696 36714 36728 36743 36760 36775 ... #>   ..$ : int [1:3830] 39920 39953 39989 40024 40058 40094 40127 40163 40198 40232 ... #>   ..$ : int [1:3495] 39926 39959 39995 40029 40064 40100 40133 40169 40203 40238 ... #>   ..$ : int [1:3509] 39935 39968 40004 40039 40109 40142 40178 40213 40283 40316 ... #>   ..$ : int [1:2581] 45045 45079 45111 45145 45180 45214 45248 45280 45314 45349 ... #>   ..$ : int [1:3401] 45552 45591 45628 45666 45704 45740 45779 45816 45855 45894 ..."},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":null,"dir":"Reference","previous_headings":"","what":"Area of Applicability — aoa","title":"Area of Applicability — aoa","text":"function estimates Dissimilarity Index (DI) derived Area Applicability (AOA) spatial prediction models considering distance new data (.e. Raster Stack spatial predictors used models) predictor variable space data used model training. Predictors can weighted based internal variable importance machine learning algorithm used model training. AOA derived applying threshold DI (outlier-removed) maximum DI cross-validated training data.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Area of Applicability — aoa","text":"","code":"aoa(   newdata,   model = NA,   trainDI = NA,   cl = NULL,   train = NULL,   weight = NA,   variables = \"all\",   CVtest = NULL,   CVtrain = NULL )"},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Area of Applicability — aoa","text":"newdata RasterStack, RasterBrick, stars object, SpatRaster data.frame containing data model meant make predictions . model train object created caret used extract weights (based variable importance) well cross-validation folds. See examples case model available models trained via e.g. mlr3. trainDI trainDI object. Optional trainDI calculated beforehand. cl cluster object e.g. created doParallel. Optional. used newdata large. train data.frame containing data used model training. Optional. required model given weight data.frame containing weights variable. Optional. required model given. variables character vector predictor variables. \"\" variables model used model given train dataset. CVtest list vector. Either list element contains data points used testing cross validation iteration (.e. held back data). vector contains ID fold training point. required model given. CVtrain list. element contains data points used training cross validation iteration (.e. held back data). required model given required CVtrain opposite CVtest (.e. data point used testing, used training). Relevant data points excluded, e.g. using nndm.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Area of Applicability — aoa","text":"object class aoa containing: parameters object class trainDI. see trainDI DI raster data frame. Dissimilarity index newdata AOA raster data frame. Area Applicability newdata.   AOA values 0 (outside AOA) 1 (inside AOA)","code":""},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Area of Applicability — aoa","text":"Dissimilarity Index (DI) corresponding Area Applicability (AOA) calculated. variables factors, dummy variables created prior weighting distance calculation. Interpretation results: location similar properties training data low distance predictor variable space (DI towards 0) locations different properties high DI. See Meyer Pebesma (2021) full documentation methodology.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Area of Applicability — aoa","text":"classification models used, currently variable importance can automatically retrieved models trained via train(predictors,response) via formula-interface. fixed.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Area of Applicability — aoa","text":"Meyer, H., Pebesma, E. (2021): Predicting unknown space? Estimating area applicability spatial prediction models. Methods Ecology Evolution 12: 1620-1633. doi:10.1111/2041-210X.13650","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Area of Applicability — aoa","text":"Hanna Meyer","code":""},{"path":"https://hannameyer.github.io/CAST/reference/aoa.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Area of Applicability — aoa","text":"","code":"if (FALSE) { library(sf) library(raster) library(caret) library(viridis) library(latticeExtra)  # prepare sample data: dat <- get(load(system.file(\"extdata\",\"Cookfarm.RData\",package=\"CAST\"))) dat <- aggregate(dat[,c(\"VW\",\"Easting\",\"Northing\")],by=list(as.character(dat$SOURCEID)),mean) pts <- st_as_sf(dat,coords=c(\"Easting\",\"Northing\")) pts$ID <- 1:nrow(pts) set.seed(100) pts <- pts[1:30,] studyArea <- stack(system.file(\"extdata\",\"predictors_2012-03-25.grd\",package=\"CAST\"))[[1:8]] trainDat <- extract(studyArea,pts,df=TRUE) trainDat <- merge(trainDat,pts,by.x=\"ID\",by.y=\"ID\")  # visualize data spatially: spplot(scale(studyArea)) plot(studyArea$DEM) plot(pts[,1],add=TRUE,col=\"black\")  # train a model: set.seed(100) variables <- c(\"DEM\",\"NDRE.Sd\",\"TWI\") model <- train(trainDat[,which(names(trainDat)%in%variables)], trainDat$VW, method=\"rf\", importance=TRUE, tuneLength=1, trControl=trainControl(method=\"cv\",number=5,savePredictions=T)) print(model) #note that this is a quite poor prediction model prediction <- predict(studyArea,model) plot(varImp(model,scale=FALSE))  #...then calculate the AOA of the trained model for the study area: AOA <- aoa(studyArea,model) plot(AOA) spplot(AOA$DI, col.regions=viridis(100),main=\"Dissimilarity Index\") #plot predictions for the AOA only: spplot(prediction, col.regions=viridis(100),main=\"prediction for the AOA\")+ spplot(AOA$AOA,col.regions=c(\"grey\",\"transparent\"))  #### # Calculating the AOA might be time consuming. Consider running it in parallel: #### library(doParallel) library(parallel) cl <- makeCluster(4) registerDoParallel(cl) AOA <- aoa(studyArea,model,cl=cl)  #### #The AOA can also be calculated without a trained model. #All variables are weighted equally in this case: #### AOA <- aoa(studyArea,train=trainDat,variables=variables) spplot(AOA$DI, col.regions=viridis(100),main=\"Dissimilarity Index\") spplot(AOA$AOA,main=\"Area of Applicability\")   #### # The AOA can also be used for models trained via mlr3 (parameters have to be assigned manually): ####  library(mlr3) library(mlr3learners) library(mlr3spatial) library(mlr3spatiotempcv) library(mlr3extralearners)  # initiate and train model: train_df <- trainDat[, c(\"DEM\",\"NDRE.Sd\",\"TWI\", \"VW\")] backend <- as_data_backend(train_df) task <- as_task_regr(backend, target = \"VW\") lrn <- lrn(\"regr.randomForest\", importance = \"mse\") lrn$train(task)  # cross-validation folds rsmp_cv <- rsmp(\"cv\", folds = 5L)$instantiate(task)  ## predict: prediction <- predict(studyArea,lrn$model)  ### Estimate AOA AOA <- aoa(studyArea,            train = as.data.frame(task$data()),            variables = task$feature_names,            weight = data.frame(t(lrn$importance())),            CVtest = rsmp_cv$instance[order(row_id)]$fold)  }"},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":null,"dir":"Reference","previous_headings":"","what":"Best subset feature selection — bss","title":"Best subset feature selection — bss","text":"Evaluate combinations predictors model training","code":""},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Best subset feature selection — bss","text":"","code":"bss(   predictors,   response,   method = \"rf\",   metric = ifelse(is.factor(response), \"Accuracy\", \"RMSE\"),   maximize = ifelse(metric == \"RMSE\", FALSE, TRUE),   globalval = FALSE,   trControl = caret::trainControl(),   tuneLength = 3,   tuneGrid = NULL,   seed = 100,   verbose = TRUE,   ... )"},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Best subset feature selection — bss","text":"predictors see train response see train method see train metric see train maximize see train globalval Logical. models evaluated based 'global' performance? See global_validation trControl see train tuneLength see train tuneGrid see train seed random number verbose Logical. information progress printed? ... arguments passed classification regression routine (randomForest).","code":""},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Best subset feature selection — bss","text":"list class train. Beside usual train content object contains vector \"selectedvars\" \"selectedvars_perf\" give best variables selected well corresponding performance. also contains \"perf_all\" gives performance model runs.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Best subset feature selection — bss","text":"bss alternative ffs ideal training set small. Models iteratively fitted using different combinations predictor variables. Hence, 2^X models calculated. try running bss large datasets computation time much higher compared ffs. internal cross validation can run parallel. See information parallel processing carets train functions details.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Best subset feature selection — bss","text":"variable selection particularly suitable spatial cross validations variable selection MUST based performance model predicting new spatial units. Note bss slow since combinations variables tested. time efficient alternative forward feature selection (ffs)  (ffs).","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Best subset feature selection — bss","text":"Hanna Meyer","code":""},{"path":"https://hannameyer.github.io/CAST/reference/bss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Best subset feature selection — bss","text":"","code":"if (FALSE) { data(iris) bssmodel <- bss(iris[,1:4],iris$Species) bssmodel$perf_all }"},{"path":"https://hannameyer.github.io/CAST/reference/calibrate_aoa.html","id":null,"dir":"Reference","previous_headings":"","what":"Calibrate the AOA based on the relationship between the DI and the prediction error — calibrate_aoa","title":"Calibrate the AOA based on the relationship between the DI and the prediction error — calibrate_aoa","text":"Performance metrics calculated moving windows DI values cross-validated training data","code":""},{"path":"https://hannameyer.github.io/CAST/reference/calibrate_aoa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calibrate the AOA based on the relationship between the DI and the prediction error — calibrate_aoa","text":"","code":"calibrate_aoa(   AOA,   model,   window.size = 5,   calib = \"scam\",   multiCV = FALSE,   length.out = 10,   maskAOA = TRUE,   showPlot = TRUE,   k = 6,   m = 2 )"},{"path":"https://hannameyer.github.io/CAST/reference/calibrate_aoa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calibrate the AOA based on the relationship between the DI and the prediction error — calibrate_aoa","text":"AOA result aoa model model used get AOA window.size Numeric. Size moving window. See rollapply. calib Character. Function model DI~performance relationship. Currently lm scam supported multiCV Logical. Re-run model fitting validation different CV strategies. See details. length.Numeric. used multiCV=TRUE. Number cross-validation folds. See details. maskAOA Logical. areas outside AOA set NA? showPlot Logical. k Numeric. See mgcv::s m Numeric. See mgcv::s","code":""},{"path":"https://hannameyer.github.io/CAST/reference/calibrate_aoa.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calibrate the AOA based on the relationship between the DI and the prediction error — calibrate_aoa","text":"list length 2 elements \"AOA\": rasterStack contains original DI AOA (might updated new test data indicate option), well expected performance based relationship. Data used calibration stored attributes. second element plot showing relationship.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/calibrate_aoa.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calibrate the AOA based on the relationship between the DI and the prediction error — calibrate_aoa","text":"multiCV=TRUE model re-fitted validated length.new cross-validations cross-validation folds defined clusters predictor space, ranging three clusters LOOCV. AOA threshold based calibration data multiple CV larger original AOA threshold, AOA updated accordingly. See Meyer Pebesma (2020) full documentation methodology.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/calibrate_aoa.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calibrate the AOA based on the relationship between the DI and the prediction error — calibrate_aoa","text":"Meyer, H., Pebesma, E. (2021): Predicting unknown space? Estimating area applicability spatial prediction models. doi:10.1111/2041-210X.13650","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/calibrate_aoa.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calibrate the AOA based on the relationship between the DI and the prediction error — calibrate_aoa","text":"Hanna Meyer","code":""},{"path":"https://hannameyer.github.io/CAST/reference/calibrate_aoa.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calibrate the AOA based on the relationship between the DI and the prediction error — calibrate_aoa","text":"","code":"if (FALSE) { library(sf) library(raster) library(caret) library(viridis) library(latticeExtra)  # prepare sample data: library(sf) library(raster) library(caret) # prepare sample data: dat <- get(load(system.file(\"extdata\",\"Cookfarm.RData\",package=\"CAST\"))) dat <- aggregate(dat[,c(\"VW\",\"Easting\",\"Northing\")],by=list(as.character(dat$SOURCEID)),mean) pts <- st_as_sf(dat,coords=c(\"Easting\",\"Northing\")) pts$ID <- 1:nrow(pts) studyArea <- stack(system.file(\"extdata\",\"predictors_2012-03-25.grd\",package=\"CAST\"))[[1:8]] dat <- extract(studyArea,pts,df=TRUE) trainDat <- merge(dat,pts,by.x=\"ID\",by.y=\"ID\")  # train a model: variables <- c(\"DEM\",\"NDRE.Sd\",\"TWI\") set.seed(100) model <- train(trainDat[,which(names(trainDat)%in%variables)],   trainDat$VW,method=\"rf\",importance=TRUE,tuneLength=1,   trControl=trainControl(method=\"cv\",number=5,savePredictions=TRUE))  #...then calculate the AOA of the trained model for the study area: AOA <- aoa(studyArea,model)  AOA_new <- calibrate_aoa(AOA,model) plot(AOA_new$AOA[[3]]) }"},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":null,"dir":"Reference","previous_headings":"","what":"Forward feature selection — ffs","title":"Forward feature selection — ffs","text":"simple forward feature selection algorithm","code":""},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forward feature selection — ffs","text":"","code":"ffs(   predictors,   response,   method = \"rf\",   metric = ifelse(is.factor(response), \"Accuracy\", \"RMSE\"),   maximize = ifelse(metric == \"RMSE\", FALSE, TRUE),   globalval = FALSE,   withinSE = FALSE,   minVar = 2,   trControl = caret::trainControl(),   tuneLength = 3,   tuneGrid = NULL,   seed = sample(1:1000, 1),   verbose = TRUE,   ... )"},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forward feature selection — ffs","text":"predictors see train response see train method see train metric see train maximize see train globalval Logical. models evaluated based 'global' performance? See global_validation withinSE Logical Models selected better currently best models Standard error minVar Numeric. Number variables combine first selection. See Details. trControl see train tuneLength see train tuneGrid see train seed random number used model training verbose Logical. information progress printed? ... arguments passed classification regression routine (randomForest).","code":""},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forward feature selection — ffs","text":"list class train. Beside usual train content object contains vector \"selectedvars\" \"selectedvars_perf\" give order best variables selected well corresponding performance (starting first two variables). also contains \"perf_all\" gives performance model runs.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forward feature selection — ffs","text":"Models two predictors first trained using possible pairs predictor variables. best model initial models kept. basis best model predictor variables iteratively increased remaining variables tested improvement currently best model. process stops none remaining variables increases model performance added current best model. internal cross validation can run parallel. See information parallel processing carets train functions details. Using withinSE favour models less variables probably shorten calculation time Per Default, ffs starts possible 2-pair combinations. minVar allows start selection 2 variables, e.g. minVar=3 starts ffs testing combinations 3 (instead 2) variables first increasing number. important e.g. neural networks often make sense two variables. also relevant assumed optimal variables can found 2 considered time.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Forward feature selection — ffs","text":"variable selection particularly suitable spatial cross validations variable selection MUST based performance model predicting new spatial units. See Meyer et al. (2018) Meyer et al. (2019) details.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forward feature selection — ffs","text":"Gasch, C.K., Hengl, T., Gräler, B., Meyer, H., Magney, T., Brown, D.J. (2015): Spatio-temporal interpolation soil water, temperature, electrical conductivity 3D+T: Cook Agronomy Farm data set. Spatial Statistics 14: 70-90. Meyer, H., Reudenbach, C., Hengl, T., Katurji, M., Nauß, T. (2018): Improving performance spatio-temporal machine learning models using forward feature selection target-oriented validation. Environmental Modelling & Software 101: 1-9.  doi:10.1016/j.envsoft.2017.12.001 Meyer, H., Reudenbach, C., Wöllauer, S., Nauss, T. (2019): Importance spatial predictor variable selection machine learning applications - Moving data reproduction spatial prediction. Ecological Modelling. 411, 108815. doi:10.1016/j.ecolmodel.2019.108815","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Forward feature selection — ffs","text":"Hanna Meyer","code":""},{"path":"https://hannameyer.github.io/CAST/reference/ffs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forward feature selection — ffs","text":"","code":"if (FALSE) { data(iris) ffsmodel <- ffs(iris[,1:4],iris$Species) ffsmodel$selectedvars ffsmodel$selectedvars_perf }  # or perform model with target-oriented validation (LLO CV) #the example is described in Gasch et al. (2015). The ffs approach for this dataset is described in #Meyer et al. (2018). Due to high computation time needed, only a small and thus not robust example #is shown here.  if (FALSE) { #run the model on three cores: library(doParallel) cl <- makeCluster(3) registerDoParallel(cl)  #load and prepare dataset: dat <- get(load(system.file(\"extdata\",\"Cookfarm.RData\",package=\"CAST\"))) trainDat <- dat[dat$altitude==-0.3&year(dat$Date)==2012&week(dat$Date)%in%c(13:14),]  #visualize dataset: ggplot(data = trainDat, aes(x=Date, y=VW)) + geom_line(aes(colour=SOURCEID))  #create folds for Leave Location Out Cross Validation: set.seed(10) indices <- CreateSpacetimeFolds(trainDat,spacevar = \"SOURCEID\",k=3) ctrl <- trainControl(method=\"cv\",index = indices$index)  #define potential predictors: predictors <- c(\"DEM\",\"TWI\",\"BLD\",\"Precip_cum\",\"cday\",\"MaxT_wrcc\", \"Precip_wrcc\",\"NDRE.M\",\"Bt\",\"MinT_wrcc\",\"Northing\",\"Easting\")  #run ffs model with Leave Location out CV set.seed(10) ffsmodel <- ffs(trainDat[,predictors],trainDat$VW,method=\"rf\", tuneLength=1,trControl=ctrl) ffsmodel  #compare to model without ffs: model <- train(trainDat[,predictors],trainDat$VW,method=\"rf\", tuneLength=1, trControl=ctrl) model stopCluster(cl) }"},{"path":"https://hannameyer.github.io/CAST/reference/global_validation.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate 'global' cross-validation — global_validation","title":"Evaluate 'global' cross-validation — global_validation","text":"Calculate validation metric using held back predictions ","code":""},{"path":"https://hannameyer.github.io/CAST/reference/global_validation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate 'global' cross-validation — global_validation","text":"","code":"global_validation(model)"},{"path":"https://hannameyer.github.io/CAST/reference/global_validation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate 'global' cross-validation — global_validation","text":"model object class train","code":""},{"path":"https://hannameyer.github.io/CAST/reference/global_validation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate 'global' cross-validation — global_validation","text":"regression (postResample) classification  (confusionMatrix) statistics","code":""},{"path":"https://hannameyer.github.io/CAST/reference/global_validation.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Evaluate 'global' cross-validation — global_validation","text":"Relevant folds representative entire area interest. case, metrics like R2 meaningful since reflect general ability model explain entire gradient response. Comparable LOOCV, predictions held back folds used together calculate validation statistics.","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/global_validation.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Evaluate 'global' cross-validation — global_validation","text":"Hanna Meyer","code":""},{"path":"https://hannameyer.github.io/CAST/reference/global_validation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate 'global' cross-validation — global_validation","text":"","code":"dat <- get(load(system.file(\"extdata\",\"Cookfarm.RData\",package=\"CAST\"))) dat <- dat[sample(1:nrow(dat),500),] indices <- CreateSpacetimeFolds(dat,\"SOURCEID\",\"Date\") ctrl <- caret::trainControl(method=\"cv\",index = indices$index,savePredictions=\"final\") model <- caret::train(dat[,c(\"DEM\",\"TWI\",\"BLD\")],dat$VW, method=\"rf\", trControl=ctrl, ntree=10) #> note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 . #>  #> Loading required package: ggplot2 #> Loading required package: lattice global_validation(model) #>       RMSE   Rsquared        MAE  #> 0.09546556 0.05444412 0.07625065"},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":null,"dir":"Reference","previous_headings":"","what":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"function implements NNDM algorithm returns necessary indices perform NNDM LOO CV map validation.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"","code":"nndm(   tpoints,   modeldomain = NULL,   ppoints = NULL,   samplesize = 1000,   sampling = \"regular\",   phi = \"max\",   min_train = 0 )"},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"tpoints sf sfc point object. Contains training points samples. modeldomain raster sf object defining prediction area (see Details). ppoints sf sfc point object. Contains target prediction points. Optional. Alternative modeldomain (see Details). samplesize numeric. many sampled modeldomain sampled? required modeldomain used instead ppoints sampling character. draw samples modeldomain? See spsample. Use sampling = \"Fibonacci\" global applications.\" required modeldomain used instead ppoints phi Numeric. Estimate landscape autocorrelation range units tpoints ppoints projected CRS, meters geographic CRS. Per default (phi=\"max\"), size prediction area used. See Details min_train Numeric 0 1. Minimum proportion training data must used CV fold. Defaults 0 (.e. restrictions).","code":""},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"object class nndm consisting list six elements: indx_train, indx_test, indx_exclude (indices observations use training/test/excluded data NNDM LOO CV iteration), Gij (distances multitype G function construction prediction target points), Gj (distances G function construction LOO CV), Gjstar (distances modified G function NNDM LOO CV), phi (landscape autocorrelation range). indx_train indx_test can directly used \"index\" \"indexOut\" caret's trainControl function used initiate custom validation strategy mlr3.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"Details method can found Milà et al. (2022). Euclidean distances used projected non-defined CRS, great circle distances used geographic CRS (units meters). Specifying phi allows limiting distance matching area assumed relevant due spatial autocorrelation. Distances matched phi. Beyond range, data points used training, without exclusions. phi set \"max\", nearest neighbor distance matching performed entire prediction area. modeldomain sf polygon raster defines prediction area. function takes regular point sample (amount defined samplesize) spatial extent. alternative use ppoints instead modeldomain, alre<dy representative sample prediction area.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"NNDM variation LOOCV therefore may take long time large training data sets. may need consider alternatives following ideas Milà et al. (2022) large data sets.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"Milà, C., Mateu, J., Pebesma, E., Meyer, H. (2022): Nearest Neighbour Distance Matching Leave-One-Cross-Validation map validation. Methods Ecology Evolution 00, 1– 13. Meyer, H., Pebesma, E. (2022): Machine learning-based global maps ecological variables challenge assessing . Nature Communications. 13.","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"Carles Milà","code":""},{"path":"https://hannameyer.github.io/CAST/reference/nndm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Nearest Neighbour Distance Matching (NNDM) algorithm — nndm","text":"","code":"library(sf) #> Linking to GEOS 3.8.0, GDAL 3.0.4, PROJ 6.3.1; sf_use_s2() is TRUE  # Simulate 100 random training and test points in a 100x100 square set.seed(123) poly <- list(matrix(c(0,0,0,100,100,100,100,0,0,0), ncol=2, byrow=TRUE)) sample_poly <- sf::st_polygon(poly) train_points <- sf::st_sample(sample_poly, 100, type = \"random\") pred_points <- sf::st_sample(sample_poly, 100, type = \"random\")  # Run NNDM. nndm_pred <- nndm(train_points, ppoints=pred_points) nndm_pred #> nndm object #> Total number of points: 100 #> Mean number of training points: 98.63 #> Minimum number of training points: 95 plot(nndm_pred)   # ...or run NNDM with a known autocorrelation range. # Here, the autocorrelation range (phi) is known to be 10. nndm_pred <- nndm(train_points, ppoints=pred_points, phi = 10) nndm_pred #> nndm object #> Total number of points: 100 #> Mean number of training points: 98.69 #> Minimum number of training points: 96 plot(nndm_pred)    ######################################################################## # Example 2: Real- world example; using a modeldomain instead of previously # sampled prediction locations ######################################################################## library(raster) #> Loading required package: sp  ### prepare sample data: dat <- get(load(system.file(\"extdata\",\"Cookfarm.RData\",package=\"CAST\"))) dat <- aggregate(dat[,c(\"DEM\",\"TWI\", \"NDRE.M\", \"Easting\", \"Northing\",\"VW\")],    by=list(as.character(dat$SOURCEID)),mean) pts <- dat[,-1] pts <- st_as_sf(pts,coords=c(\"Easting\",\"Northing\")) st_crs(pts) <- 26911 studyArea <- raster::stack(system.file(\"extdata\",\"predictors_2012-03-25.grd\",package=\"CAST\"))  nndm_folds <- nndm(pts, modeldomain= studyArea) #> 1000 prediction points are sampled from the modeldomain #> Spherical geometry (s2) switched off #> Warning: CRS object has comment, which is lost in output; in tests, see #> https://cran.r-project.org/web/packages/sp/vignettes/CRS_warnings.html #> tpoints and ppoints must have the same CRS. tpoints have been transformed.  #use for cross-validation: library(caret) ctrl <- trainControl(method=\"cv\",    index=nndm_folds$indx_train,    indexOut=nndm_folds$indx_test,    savePredictions='final') model_nndm <- train(dat[,c(\"DEM\",\"TWI\", \"NDRE.M\")],    dat$VW,    method=\"rf\",    trControl = ctrl) #> note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 . #>  #> Warning: There were missing values in resampled performance measures. model_nndm #> Random Forest  #>  #> 42 samples #>  3 predictor #>  #> No pre-processing #> Resampling: Cross-Validated (10 fold)  #> Summary of sample sizes: 36, 41, 41, 38, 41, 41, ...  #> Resampling results across tuning parameters: #>  #>   mtry  RMSE        Rsquared  MAE        #>   2     0.03383840  NaN       0.03383840 #>   3     0.03457086  NaN       0.03457086 #>  #> RMSE was used to select the optimal model using the smallest value. #> The final value used for the model was mtry = 2. global_validation(model_nndm) #>        RMSE    Rsquared         MAE  #> 0.041609098 0.005184905 0.033838402"},{"path":"https://hannameyer.github.io/CAST/reference/plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot CAST classes — plot","title":"Plot CAST classes — plot","text":"Generic plot function trainDI aoa","code":""},{"path":"https://hannameyer.github.io/CAST/reference/plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot CAST classes — plot","text":"","code":"# S3 method for trainDI plot(x, ...)  # S3 method for aoa plot(x, samplesize = 1000, ...)  # S3 method for nndm plot(x, ...)"},{"path":"https://hannameyer.github.io/CAST/reference/plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot CAST classes — plot","text":"x object type nndm. ... arguments. samplesize numeric. many prediction samples plotted?","code":""},{"path":"https://hannameyer.github.io/CAST/reference/plot.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot CAST classes — plot","text":"Marvin Ludwig, Hanna Meyer Carles Milà","code":""},{"path":"https://hannameyer.github.io/CAST/reference/plot_ffs.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot results of a Forward feature selection or best subset selection — plot_ffs","title":"Plot results of a Forward feature selection or best subset selection — plot_ffs","text":"plotting function forward feature selection result. point mean performance model run. Error bars represent standard errors cross validation. Marked points show best model number variables variable improve results. type==\"selected\", contribution selected variables model performance shown.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/plot_ffs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot results of a Forward feature selection or best subset selection — plot_ffs","text":"","code":"plot_ffs(   ffs_model,   plotType = \"all\",   palette = rainbow,   reverse = FALSE,   marker = \"black\",   size = 1.5,   lwd = 0.5,   pch = 21,   ... )"},{"path":"https://hannameyer.github.io/CAST/reference/plot_ffs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot results of a Forward feature selection or best subset selection — plot_ffs","text":"ffs_model Result forward feature selection see ffs plotType character. Either \"\" \"selected\" palette color palette reverse Character. palette reversed? marker Character. Color mark best models size Numeric. Size points lwd Numeric. Width error bars pch Numeric. Type point marking best models ... arguments base plot type=\"selected\"","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/plot_ffs.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot results of a Forward feature selection or best subset selection — plot_ffs","text":"Marvin Ludwig Hanna Meyer","code":""},{"path":"https://hannameyer.github.io/CAST/reference/plot_ffs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot results of a Forward feature selection or best subset selection — plot_ffs","text":"","code":"if (FALSE) { data(iris) ffsmodel <- ffs(iris[,1:4],iris$Species) plot_ffs(ffsmodel) #plot performance of selected variables only: plot_ffs(ffsmodel,plotType=\"selected\") }"},{"path":"https://hannameyer.github.io/CAST/reference/plot_geodist.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot euclidean nearest neighbor distances in geographic space or feature space — plot_geodist","title":"Plot euclidean nearest neighbor distances in geographic space or feature space — plot_geodist","text":"Density plot nearest neighbor distances geographic space feature space training data well training data prediction locations. Optional, nearest neighbor distances training data test data training data CV iterations shown. plot can used check suitability chosen CV method representative estimate map accuracy. Alternatively distances can also calculated multivariate feature space.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/plot_geodist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot euclidean nearest neighbor distances in geographic space or feature space — plot_geodist","text":"","code":"plot_geodist(   x,   modeldomain,   type = \"geo\",   cvfolds = NULL,   cvtrain = NULL,   testdata = NULL,   samplesize = 2000,   sampling = \"regular\",   variables = NULL,   unit = \"m\",   stat = \"density\",   showPlot = TRUE )"},{"path":"https://hannameyer.github.io/CAST/reference/plot_geodist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot euclidean nearest neighbor distances in geographic space or feature space — plot_geodist","text":"x object class sf, training data locations modeldomain raster sf object defining prediction area (see Details) type \"geo\" \"feature\". distance computed geographic space normalized multivariate predictor space (see Details) cvfolds optional. List row indices x held back CV iteration. See e.g. ?createFolds ?createSpaceTimeFolds cvtrain optional. List row indices x fit model CV iteration. cvtrain null cvfolds , samples included cvfolds used training data testdata optional. object class sf: Data used independent validation samplesize numeric. many prediction samples used? sampling character. draw prediction samples? See spsample. Use sampling = \"Fibonacci\" global applications. variables character vector defining predictor variables used type=\"feature. provided variables included modeldomain used. unit character. type==\"geo\" applied plot. Supported: \"m\" \"km\". stat \"density\" density plot \"ecdf\" cumulative plot. showPlot logical","code":""},{"path":"https://hannameyer.github.io/CAST/reference/plot_geodist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot euclidean nearest neighbor distances in geographic space or feature space — plot_geodist","text":"list including plot corresponding data.frame containing distances. Unit returned geographic distances meters.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/plot_geodist.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot euclidean nearest neighbor distances in geographic space or feature space — plot_geodist","text":"modeldomain sf polygon raster defines prediction area. function takes regular point sample (amount defined samplesize) spatial extent.     type = \"feature\", argument modeldomain (provided also testdata) include predictors. Predictor values x optional modeldomain raster.     provided extracted modeldomain rasterStack.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/plot_geodist.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Plot euclidean nearest neighbor distances in geographic space or feature space — plot_geodist","text":"See Meyer Pebesma (2022) application plotting function","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/plot_geodist.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot euclidean nearest neighbor distances in geographic space or feature space — plot_geodist","text":"Hanna Meyer, Edzer Pebesma, Marvin Ludwig","code":""},{"path":"https://hannameyer.github.io/CAST/reference/plot_geodist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot euclidean nearest neighbor distances in geographic space or feature space — plot_geodist","text":"","code":"library(sf) library(raster) library(caret)  ########### prepare sample data: dat <- get(load(system.file(\"extdata\",\"Cookfarm.RData\",package=\"CAST\"))) dat <- aggregate(dat[,c(\"DEM\",\"TWI\", \"NDRE.M\", \"Easting\", \"Northing\")],   by=list(as.character(dat$SOURCEID)),mean) pts <- st_as_sf(dat,coords=c(\"Easting\",\"Northing\")) st_crs(pts) <- 26911 pts_train <- pts[1:29,] pts_test <- pts[30:42,] studyArea <- raster::stack(system.file(\"extdata\",\"predictors_2012-03-25.grd\",package=\"CAST\")) studyArea <- studyArea[[c(\"DEM\",\"TWI\", \"NDRE.M\", \"NDRE.Sd\", \"Bt\")]]  ########### Distance between training data and new data: dist <- plot_geodist(pts_train,studyArea) #> Warning: CRS object has comment, which is lost in output; in tests, see #> https://cran.r-project.org/web/packages/sp/vignettes/CRS_warnings.html #> Spherical geometry (s2) switched on   ########### Distance between training data, new data and test data: #mapview(pts_train,col.regions=\"blue\")+mapview(pts_test,col.regions=\"red\") dist <- plot_geodist(pts_train,studyArea,testdata=pts_test) #> Spherical geometry (s2) switched off #> Warning: CRS object has comment, which is lost in output; in tests, see #> https://cran.r-project.org/web/packages/sp/vignettes/CRS_warnings.html #> Spherical geometry (s2) switched on   ########### Distance between training data, new data and CV folds: folds <- createFolds(1:nrow(pts_train),k=3,returnTrain=FALSE) dist <- plot_geodist(x=pts_train, modeldomain=studyArea, cvfolds=folds) #> Spherical geometry (s2) switched off #> Warning: CRS object has comment, which is lost in output; in tests, see #> https://cran.r-project.org/web/packages/sp/vignettes/CRS_warnings.html #> Spherical geometry (s2) switched on   ## or use nndm to define folds nndm_pred <- nndm(pts_train, studyArea) #> 1000 prediction points are sampled from the modeldomain #> Spherical geometry (s2) switched off #> Warning: CRS object has comment, which is lost in output; in tests, see #> https://cran.r-project.org/web/packages/sp/vignettes/CRS_warnings.html #> tpoints and ppoints must have the same CRS. tpoints have been transformed. dist <- plot_geodist(x=pts_train, modeldomain=studyArea,     cvfolds=nndm_pred$indx_test, cvtrain=nndm_pred$indx_train) #> Warning: CRS object has comment, which is lost in output; in tests, see #> https://cran.r-project.org/web/packages/sp/vignettes/CRS_warnings.html #> Spherical geometry (s2) switched on    ########### Distances in the feature space: plot_geodist(x=pts_train, modeldomain=studyArea,     type = \"feature\",variables=c(\"DEM\",\"TWI\", \"NDRE.M\")) #> Spherical geometry (s2) switched off #> Warning: CRS object has comment, which is lost in output; in tests, see #> https://cran.r-project.org/web/packages/sp/vignettes/CRS_warnings.html #> Warning: Transforming SpatialPoints to the crs of the Raster  #> $plot  #>  #> $distances #>              dist                 what dist_type #> 1    5.756791e-01     sample-to-sample   feature #> 2    2.415539e-01     sample-to-sample   feature #> 3    2.415539e-01     sample-to-sample   feature #> 4    8.221375e-01     sample-to-sample   feature #> 5    5.834980e-01     sample-to-sample   feature #> 6    7.514592e-01     sample-to-sample   feature #> 7    6.095787e-01     sample-to-sample   feature #> 8    8.833736e-01     sample-to-sample   feature #> 9    6.253269e-01     sample-to-sample   feature #> 10   6.025636e-01     sample-to-sample   feature #> 11   7.561676e-01     sample-to-sample   feature #> 12   8.221375e-01     sample-to-sample   feature #> 13   7.166094e-01     sample-to-sample   feature #> 14   2.959509e-01     sample-to-sample   feature #> 15   5.980377e-01     sample-to-sample   feature #> 16   4.984940e-01     sample-to-sample   feature #> 17   5.593994e-01     sample-to-sample   feature #> 18   8.782812e-01     sample-to-sample   feature #> 19   4.465051e-01     sample-to-sample   feature #> 20   5.593994e-01     sample-to-sample   feature #> 21   5.464848e-01     sample-to-sample   feature #> 22   4.465051e-01     sample-to-sample   feature #> 23   1.696328e+00     sample-to-sample   feature #> 24   2.959509e-01     sample-to-sample   feature #> 25   3.906445e-01     sample-to-sample   feature #> 26   7.755953e-01     sample-to-sample   feature #> 27   7.282922e-01     sample-to-sample   feature #> 28   5.834980e-01     sample-to-sample   feature #> 29   6.128470e-01     sample-to-sample   feature #> 30   1.257734e+00 sample-to-prediction   feature #> 31   1.543274e+00 sample-to-prediction   feature #> 32   1.805100e+00 sample-to-prediction   feature #> 33   1.618307e+00 sample-to-prediction   feature #> 34   1.316989e+00 sample-to-prediction   feature #> 35   1.187613e+00 sample-to-prediction   feature #> 36   1.036578e+00 sample-to-prediction   feature #> 37   1.045350e+00 sample-to-prediction   feature #> 38   1.166589e+00 sample-to-prediction   feature #> 39   1.263156e+00 sample-to-prediction   feature #> 40   1.025540e+00 sample-to-prediction   feature #> 41   9.990347e-01 sample-to-prediction   feature #> 42   8.521373e-01 sample-to-prediction   feature #> 43   9.092097e-01 sample-to-prediction   feature #> 44   8.650953e-01 sample-to-prediction   feature #> 45   6.757239e-01 sample-to-prediction   feature #> 46   5.556895e-01 sample-to-prediction   feature #> 47   4.367782e-01 sample-to-prediction   feature #> 48   3.817281e-01 sample-to-prediction   feature #> 49   1.523441e-08 sample-to-prediction   feature #> 50   4.943916e-02 sample-to-prediction   feature #> 51   1.108635e-01 sample-to-prediction   feature #> 52   9.856608e-02 sample-to-prediction   feature #> 53   3.617978e-01 sample-to-prediction   feature #> 54   3.624323e-01 sample-to-prediction   feature #> 55   1.289175e-01 sample-to-prediction   feature #> 56   4.042433e-01 sample-to-prediction   feature #> 57   3.641002e-01 sample-to-prediction   feature #> 58   3.155072e-01 sample-to-prediction   feature #> 59   2.596415e-01 sample-to-prediction   feature #> 60   2.929095e-01 sample-to-prediction   feature #> 61   1.208897e+00 sample-to-prediction   feature #> 62   1.337856e+00 sample-to-prediction   feature #> 63   1.330926e+00 sample-to-prediction   feature #> 64   2.176452e+00 sample-to-prediction   feature #> 65   2.263455e+00 sample-to-prediction   feature #> 66   1.926736e+00 sample-to-prediction   feature #> 67   1.843541e+00 sample-to-prediction   feature #> 68   2.065954e+00 sample-to-prediction   feature #> 69   1.862777e+00 sample-to-prediction   feature #> 70   1.737123e+00 sample-to-prediction   feature #> 71   2.148592e+00 sample-to-prediction   feature #> 72   2.086177e+00 sample-to-prediction   feature #> 73   1.610860e+00 sample-to-prediction   feature #> 74   1.357748e+00 sample-to-prediction   feature #> 75   3.937401e-01 sample-to-prediction   feature #> 76   1.510149e+00 sample-to-prediction   feature #> 77   8.802565e-01 sample-to-prediction   feature #> 78   6.634979e-01 sample-to-prediction   feature #> 79   2.603456e-09 sample-to-prediction   feature #> 80   7.258874e-01 sample-to-prediction   feature #> 81   9.197435e-01 sample-to-prediction   feature #> 82   8.331301e-01 sample-to-prediction   feature #> 83   7.803475e-01 sample-to-prediction   feature #> 84   7.019132e-01 sample-to-prediction   feature #> 85   5.911843e-01 sample-to-prediction   feature #> 86   4.249186e-01 sample-to-prediction   feature #> 87   5.900313e-01 sample-to-prediction   feature #> 88   2.064820e-01 sample-to-prediction   feature #> 89   9.191279e-10 sample-to-prediction   feature #> 90   3.112628e-01 sample-to-prediction   feature #> 91   4.612276e-01 sample-to-prediction   feature #> 92   4.199359e-01 sample-to-prediction   feature #> 93   2.163125e-01 sample-to-prediction   feature #> 94   2.772018e-01 sample-to-prediction   feature #> 95   2.069836e-01 sample-to-prediction   feature #> 96   1.747288e-01 sample-to-prediction   feature #> 97   3.602983e-01 sample-to-prediction   feature #> 98   6.541115e-01 sample-to-prediction   feature #> 99   6.333536e-01 sample-to-prediction   feature #> 100  5.898628e-01 sample-to-prediction   feature #> 101  1.731566e-01 sample-to-prediction   feature #> 102  2.883237e-01 sample-to-prediction   feature #> 103  1.758435e-01 sample-to-prediction   feature #> 104  3.944417e-01 sample-to-prediction   feature #> 105  3.727720e-01 sample-to-prediction   feature #> 106  4.659412e-01 sample-to-prediction   feature #> 107  5.101456e-01 sample-to-prediction   feature #> 108  3.786393e-01 sample-to-prediction   feature #> 109  4.691956e-01 sample-to-prediction   feature #> 110  3.364187e-01 sample-to-prediction   feature #> 111  2.800486e-01 sample-to-prediction   feature #> 112  3.884646e-01 sample-to-prediction   feature #> 113  6.316373e-01 sample-to-prediction   feature #> 114  7.269869e-01 sample-to-prediction   feature #> 115  1.251177e+00 sample-to-prediction   feature #> 116  1.529788e+00 sample-to-prediction   feature #> 117  6.063261e-01 sample-to-prediction   feature #> 118  5.443685e-01 sample-to-prediction   feature #> 119  7.699266e-01 sample-to-prediction   feature #> 120  1.255998e+00 sample-to-prediction   feature #> 121  1.517457e+00 sample-to-prediction   feature #> 122  1.991912e+00 sample-to-prediction   feature #> 123  2.218721e+00 sample-to-prediction   feature #> 124  2.611684e+00 sample-to-prediction   feature #> 125  2.347568e+00 sample-to-prediction   feature #> 126  2.199802e+00 sample-to-prediction   feature #> 127  1.144991e+00 sample-to-prediction   feature #> 128  8.412450e-01 sample-to-prediction   feature #> 129  9.987556e-01 sample-to-prediction   feature #> 130  1.402028e-01 sample-to-prediction   feature #> 131  9.501076e-01 sample-to-prediction   feature #> 132  8.194657e-01 sample-to-prediction   feature #> 133  1.139669e+00 sample-to-prediction   feature #> 134  1.012553e+00 sample-to-prediction   feature #> 135  9.150771e-01 sample-to-prediction   feature #> 136  7.084819e-01 sample-to-prediction   feature #> 137  5.681516e-01 sample-to-prediction   feature #> 138  4.241636e-01 sample-to-prediction   feature #> 139  5.896643e-01 sample-to-prediction   feature #> 140  4.793248e-01 sample-to-prediction   feature #> 141  1.527430e-01 sample-to-prediction   feature #> 142  1.875306e-01 sample-to-prediction   feature #> 143  3.314488e-01 sample-to-prediction   feature #> 144  5.218430e-01 sample-to-prediction   feature #> 145  4.659973e-01 sample-to-prediction   feature #> 146  3.021309e-01 sample-to-prediction   feature #> 147  2.843259e-01 sample-to-prediction   feature #> 148  3.991144e-01 sample-to-prediction   feature #> 149  2.478820e-01 sample-to-prediction   feature #> 150  3.710423e-01 sample-to-prediction   feature #> 151  2.773407e-01 sample-to-prediction   feature #> 152  4.304281e-01 sample-to-prediction   feature #> 153  4.704535e-01 sample-to-prediction   feature #> 154  5.594323e-01 sample-to-prediction   feature #> 155  3.351733e-01 sample-to-prediction   feature #> 156  3.698996e-01 sample-to-prediction   feature #> 157  3.328505e-01 sample-to-prediction   feature #> 158  7.205059e-09 sample-to-prediction   feature #> 159  4.319354e-01 sample-to-prediction   feature #> 160  3.964244e-01 sample-to-prediction   feature #> 161  6.212374e-01 sample-to-prediction   feature #> 162  4.959966e-01 sample-to-prediction   feature #> 163  1.382301e-08 sample-to-prediction   feature #> 164  1.925412e-01 sample-to-prediction   feature #> 165  5.536921e-01 sample-to-prediction   feature #> 166  5.289659e-01 sample-to-prediction   feature #> 167  3.014483e-01 sample-to-prediction   feature #> 168  2.101748e-01 sample-to-prediction   feature #> 169  4.291830e-01 sample-to-prediction   feature #> 170  5.371442e-01 sample-to-prediction   feature #> 171  3.577175e-01 sample-to-prediction   feature #> 172  1.024675e+00 sample-to-prediction   feature #> 173  1.484292e+00 sample-to-prediction   feature #> 174  1.741247e+00 sample-to-prediction   feature #> 175  1.682463e+00 sample-to-prediction   feature #> 176  1.556972e+00 sample-to-prediction   feature #> 177  1.473550e+00 sample-to-prediction   feature #> 178  1.245553e+00 sample-to-prediction   feature #> 179  1.790859e+00 sample-to-prediction   feature #> 180  2.019441e+00 sample-to-prediction   feature #> 181  1.793897e+00 sample-to-prediction   feature #> 182  1.487819e+00 sample-to-prediction   feature #> 183  9.560447e-01 sample-to-prediction   feature #> 184  1.437944e+00 sample-to-prediction   feature #> 185  5.063273e-01 sample-to-prediction   feature #> 186  1.073938e+00 sample-to-prediction   feature #> 187  1.146831e+00 sample-to-prediction   feature #> 188  8.752233e-01 sample-to-prediction   feature #> 189  8.526190e-01 sample-to-prediction   feature #> 190  8.445751e-01 sample-to-prediction   feature #> 191  8.041334e-01 sample-to-prediction   feature #> 192  3.725747e-01 sample-to-prediction   feature #> 193  3.271529e-01 sample-to-prediction   feature #> 194  2.935726e-01 sample-to-prediction   feature #> 195  1.930720e-01 sample-to-prediction   feature #> 196  2.769190e-01 sample-to-prediction   feature #> 197  5.670281e-01 sample-to-prediction   feature #> 198  4.738982e-01 sample-to-prediction   feature #> 199  3.625670e-01 sample-to-prediction   feature #> 200  5.196650e-01 sample-to-prediction   feature #> 201  4.934035e-01 sample-to-prediction   feature #> 202  3.302283e-01 sample-to-prediction   feature #> 203  3.494958e-01 sample-to-prediction   feature #> 204  4.153607e-01 sample-to-prediction   feature #> 205  3.107629e-01 sample-to-prediction   feature #> 206  2.097944e-01 sample-to-prediction   feature #> 207  5.656766e-01 sample-to-prediction   feature #> 208  7.190207e-01 sample-to-prediction   feature #> 209  3.366714e-01 sample-to-prediction   feature #> 210  3.129991e-01 sample-to-prediction   feature #> 211  5.910333e-01 sample-to-prediction   feature #> 212  4.952343e-01 sample-to-prediction   feature #> 213  2.550399e-01 sample-to-prediction   feature #> 214  5.206263e-01 sample-to-prediction   feature #> 215  5.468060e-01 sample-to-prediction   feature #> 216  7.371784e-01 sample-to-prediction   feature #> 217  6.429887e-01 sample-to-prediction   feature #> 218  6.824207e-01 sample-to-prediction   feature #> 219  5.176960e-01 sample-to-prediction   feature #> 220  2.671145e-01 sample-to-prediction   feature #> 221  3.530517e-01 sample-to-prediction   feature #> 222  2.735725e-01 sample-to-prediction   feature #> 223  5.640535e-01 sample-to-prediction   feature #> 224  5.515213e-01 sample-to-prediction   feature #> 225  6.884949e-01 sample-to-prediction   feature #> 226  5.908566e-01 sample-to-prediction   feature #> 227  1.752986e-08 sample-to-prediction   feature #> 228  9.146771e-01 sample-to-prediction   feature #> 229  9.442155e-01 sample-to-prediction   feature #> 230  9.350261e-01 sample-to-prediction   feature #> 231  8.657215e-01 sample-to-prediction   feature #> 232  1.066014e+00 sample-to-prediction   feature #> 233  1.103986e+00 sample-to-prediction   feature #> 234  9.629467e-01 sample-to-prediction   feature #> 235  1.865671e+00 sample-to-prediction   feature #> 236  1.833612e+00 sample-to-prediction   feature #> 237  1.754962e+00 sample-to-prediction   feature #> 238  9.684746e-01 sample-to-prediction   feature #> 239  4.628229e-01 sample-to-prediction   feature #> 240  4.430030e-01 sample-to-prediction   feature #> 241  1.113222e+00 sample-to-prediction   feature #> 242  3.901862e-01 sample-to-prediction   feature #> 243  7.815092e-01 sample-to-prediction   feature #> 244  9.413979e-01 sample-to-prediction   feature #> 245  3.387696e-01 sample-to-prediction   feature #> 246  7.684336e-01 sample-to-prediction   feature #> 247  6.795840e-01 sample-to-prediction   feature #> 248  4.760810e-01 sample-to-prediction   feature #> 249  2.537147e-01 sample-to-prediction   feature #> 250  1.345924e-01 sample-to-prediction   feature #> 251  2.644273e-01 sample-to-prediction   feature #> 252  3.862190e-01 sample-to-prediction   feature #> 253  2.523971e-01 sample-to-prediction   feature #> 254  6.318331e-01 sample-to-prediction   feature #> 255  5.254361e-01 sample-to-prediction   feature #> 256  2.186381e-01 sample-to-prediction   feature #> 257  2.657500e-01 sample-to-prediction   feature #> 258  3.310179e-01 sample-to-prediction   feature #> 259  2.509945e-01 sample-to-prediction   feature #> 260  2.964368e-01 sample-to-prediction   feature #> 261  2.549257e-01 sample-to-prediction   feature #> 262  5.025010e-01 sample-to-prediction   feature #> 263  1.706700e+00 sample-to-prediction   feature #> 264  4.759462e-01 sample-to-prediction   feature #> 265  3.879326e-01 sample-to-prediction   feature #> 266  7.433624e-01 sample-to-prediction   feature #> 267  9.172347e-01 sample-to-prediction   feature #> 268  9.916207e-01 sample-to-prediction   feature #> 269  7.023982e-01 sample-to-prediction   feature #> 270  9.783190e-01 sample-to-prediction   feature #> 271  1.197517e+00 sample-to-prediction   feature #> 272  8.312944e-01 sample-to-prediction   feature #> 273  4.011238e-01 sample-to-prediction   feature #> 274  4.500914e-01 sample-to-prediction   feature #> 275  6.835020e-01 sample-to-prediction   feature #> 276  5.091183e-01 sample-to-prediction   feature #> 277  2.107933e-01 sample-to-prediction   feature #> 278  4.081568e-01 sample-to-prediction   feature #> 279  8.446972e-01 sample-to-prediction   feature #> 280  8.822973e-01 sample-to-prediction   feature #> 281  7.693023e-01 sample-to-prediction   feature #> 282  1.054225e+00 sample-to-prediction   feature #> 283  1.161464e+00 sample-to-prediction   feature #> 284  1.258764e+00 sample-to-prediction   feature #> 285  8.174346e-01 sample-to-prediction   feature #> 286  8.373422e-01 sample-to-prediction   feature #> 287  1.001788e+00 sample-to-prediction   feature #> 288  1.259146e+00 sample-to-prediction   feature #> 289  1.531725e+00 sample-to-prediction   feature #> 290  1.756657e+00 sample-to-prediction   feature #> 291  1.760162e+00 sample-to-prediction   feature #> 292  1.768540e+00 sample-to-prediction   feature #> 293  1.773415e+00 sample-to-prediction   feature #> 294  1.436572e+00 sample-to-prediction   feature #> 295  1.311922e+00 sample-to-prediction   feature #> 296  6.483697e-01 sample-to-prediction   feature #> 297  5.101404e-01 sample-to-prediction   feature #> 298  5.289570e-01 sample-to-prediction   feature #> 299  3.870973e-01 sample-to-prediction   feature #> 300  7.113412e-01 sample-to-prediction   feature #> 301  6.720871e-01 sample-to-prediction   feature #> 302  7.764333e-01 sample-to-prediction   feature #> 303  9.302656e-01 sample-to-prediction   feature #> 304  9.201091e-01 sample-to-prediction   feature #> 305  5.573352e-01 sample-to-prediction   feature #> 306  5.920326e-01 sample-to-prediction   feature #> 307  4.901043e-01 sample-to-prediction   feature #> 308  4.651736e-01 sample-to-prediction   feature #> 309  4.891869e-01 sample-to-prediction   feature #> 310  4.154646e-01 sample-to-prediction   feature #> 311  3.469627e-01 sample-to-prediction   feature #> 312  4.883493e-01 sample-to-prediction   feature #> 313  2.661878e-01 sample-to-prediction   feature #> 314  5.124411e-01 sample-to-prediction   feature #> 315  3.008931e-01 sample-to-prediction   feature #> 316  1.060224e-01 sample-to-prediction   feature #> 317  6.199320e-01 sample-to-prediction   feature #> 318  5.553277e-01 sample-to-prediction   feature #> 319  8.062306e-01 sample-to-prediction   feature #> 320  6.810367e-01 sample-to-prediction   feature #> 321  2.503835e-01 sample-to-prediction   feature #> 322  1.509723e-01 sample-to-prediction   feature #> 323  2.283480e-01 sample-to-prediction   feature #> 324  2.121910e-01 sample-to-prediction   feature #> 325  4.865558e-01 sample-to-prediction   feature #> 326  6.323833e-01 sample-to-prediction   feature #> 327  7.485293e-01 sample-to-prediction   feature #> 328  4.417982e-01 sample-to-prediction   feature #> 329  1.098616e+00 sample-to-prediction   feature #> 330  1.402201e+00 sample-to-prediction   feature #> 331  1.220165e+00 sample-to-prediction   feature #> 332  1.034348e+00 sample-to-prediction   feature #> 333  6.034115e-01 sample-to-prediction   feature #> 334  5.062376e-01 sample-to-prediction   feature #> 335  2.335851e-01 sample-to-prediction   feature #> 336  3.987089e-01 sample-to-prediction   feature #> 337  4.760522e-01 sample-to-prediction   feature #> 338  3.440072e-01 sample-to-prediction   feature #> 339  4.693115e-01 sample-to-prediction   feature #> 340  6.942851e-01 sample-to-prediction   feature #> 341  6.772157e-01 sample-to-prediction   feature #> 342  7.073242e-01 sample-to-prediction   feature #> 343  6.380304e-01 sample-to-prediction   feature #> 344  5.237479e-01 sample-to-prediction   feature #> 345  5.076131e-01 sample-to-prediction   feature #> 346  4.871027e-01 sample-to-prediction   feature #> 347  7.646322e-01 sample-to-prediction   feature #> 348  8.799036e-01 sample-to-prediction   feature #> 349  9.891309e-01 sample-to-prediction   feature #> 350  1.259582e+00 sample-to-prediction   feature #> 351  1.167388e+00 sample-to-prediction   feature #> 352  1.153160e+00 sample-to-prediction   feature #> 353  1.174675e+00 sample-to-prediction   feature #> 354  1.180935e+00 sample-to-prediction   feature #> 355  1.065547e+00 sample-to-prediction   feature #> 356  2.770975e-01 sample-to-prediction   feature #> 357  2.849941e-01 sample-to-prediction   feature #> 358  3.354590e-01 sample-to-prediction   feature #> 359  6.872010e-01 sample-to-prediction   feature #> 360  7.070200e-01 sample-to-prediction   feature #> 361  7.417654e-09 sample-to-prediction   feature #> 362  7.004236e-01 sample-to-prediction   feature #> 363  9.893912e-01 sample-to-prediction   feature #> 364  9.972857e-01 sample-to-prediction   feature #> 365  8.963229e-01 sample-to-prediction   feature #> 366  7.152892e-01 sample-to-prediction   feature #> 367  5.968651e-01 sample-to-prediction   feature #> 368  4.659185e-01 sample-to-prediction   feature #> 369  3.842612e-01 sample-to-prediction   feature #> 370  4.563287e-01 sample-to-prediction   feature #> 371  3.136086e-01 sample-to-prediction   feature #> 372  3.778004e-01 sample-to-prediction   feature #> 373  3.613130e-01 sample-to-prediction   feature #> 374  3.835684e-01 sample-to-prediction   feature #> 375  4.827835e-02 sample-to-prediction   feature #> 376  4.717019e-01 sample-to-prediction   feature #> 377  6.753258e-01 sample-to-prediction   feature #> 378  9.023790e-01 sample-to-prediction   feature #> 379  1.024605e+00 sample-to-prediction   feature #> 380  1.102882e+00 sample-to-prediction   feature #> 381  6.734918e-01 sample-to-prediction   feature #> 382  5.515612e-01 sample-to-prediction   feature #> 383  6.210540e-01 sample-to-prediction   feature #> 384  4.459458e-01 sample-to-prediction   feature #> 385  1.148121e+00 sample-to-prediction   feature #> 386  6.062712e-01 sample-to-prediction   feature #> 387  1.386926e-01 sample-to-prediction   feature #> 388  4.080534e-01 sample-to-prediction   feature #> 389  3.868710e-01 sample-to-prediction   feature #> 390  2.674838e-01 sample-to-prediction   feature #> 391  2.269942e-01 sample-to-prediction   feature #> 392  4.682897e-01 sample-to-prediction   feature #> 393  1.876314e-01 sample-to-prediction   feature #> 394  2.883029e-01 sample-to-prediction   feature #> 395  5.291938e-01 sample-to-prediction   feature #> 396  3.390804e-01 sample-to-prediction   feature #> 397  3.877181e-01 sample-to-prediction   feature #> 398  2.460701e-01 sample-to-prediction   feature #> 399  4.373182e-01 sample-to-prediction   feature #> 400  6.330109e-01 sample-to-prediction   feature #> 401  6.259875e-01 sample-to-prediction   feature #> 402  4.424902e-01 sample-to-prediction   feature #> 403  4.658484e-01 sample-to-prediction   feature #> 404  1.892007e-01 sample-to-prediction   feature #> 405  4.069994e-01 sample-to-prediction   feature #> 406  4.024089e-01 sample-to-prediction   feature #> 407  4.435541e-01 sample-to-prediction   feature #> 408  4.905111e-01 sample-to-prediction   feature #> 409  5.766136e-01 sample-to-prediction   feature #> 410  7.394972e-01 sample-to-prediction   feature #> 411  8.143323e-01 sample-to-prediction   feature #> 412  5.870223e-01 sample-to-prediction   feature #> 413  6.712333e-01 sample-to-prediction   feature #> 414  5.321475e-01 sample-to-prediction   feature #> 415  1.808440e-01 sample-to-prediction   feature #> 416  3.364661e-01 sample-to-prediction   feature #> 417  3.914214e-01 sample-to-prediction   feature #> 418  4.993365e-01 sample-to-prediction   feature #> 419  4.003774e-01 sample-to-prediction   feature #> 420  5.554060e-01 sample-to-prediction   feature #> 421  1.117721e+00 sample-to-prediction   feature #> 422  5.469580e-01 sample-to-prediction   feature #> 423  4.801994e-01 sample-to-prediction   feature #> 424  4.760980e-01 sample-to-prediction   feature #> 425  1.149811e+00 sample-to-prediction   feature #> 426  9.863227e-01 sample-to-prediction   feature #> 427  3.589989e-01 sample-to-prediction   feature #> 428  5.016164e-01 sample-to-prediction   feature #> 429  3.344163e-01 sample-to-prediction   feature #> 430  2.536195e-01 sample-to-prediction   feature #> 431  5.065225e-01 sample-to-prediction   feature #> 432  4.537350e-01 sample-to-prediction   feature #> 433  4.106060e-01 sample-to-prediction   feature #> 434  3.400366e-01 sample-to-prediction   feature #> 435  3.885048e-01 sample-to-prediction   feature #> 436  3.842580e-01 sample-to-prediction   feature #> 437  2.354876e-01 sample-to-prediction   feature #> 438  6.361748e-01 sample-to-prediction   feature #> 439  7.274101e-01 sample-to-prediction   feature #> 440  8.811556e-01 sample-to-prediction   feature #> 441  8.574401e-01 sample-to-prediction   feature #> 442  5.288496e-01 sample-to-prediction   feature #> 443  4.422124e-01 sample-to-prediction   feature #> 444  8.431398e-01 sample-to-prediction   feature #> 445  7.970545e-01 sample-to-prediction   feature #> 446  6.827085e-01 sample-to-prediction   feature #> 447  6.616193e-01 sample-to-prediction   feature #> 448  6.563813e-01 sample-to-prediction   feature #> 449  6.847743e-01 sample-to-prediction   feature #> 450  8.656362e-01 sample-to-prediction   feature #> 451  5.356788e-01 sample-to-prediction   feature #> 452  3.996879e-01 sample-to-prediction   feature #> 453  2.544721e-01 sample-to-prediction   feature #> 454  3.864935e-01 sample-to-prediction   feature #> 455  3.896351e-01 sample-to-prediction   feature #> 456  4.238015e-01 sample-to-prediction   feature #> 457  5.497874e-01 sample-to-prediction   feature #> 458  1.895858e-01 sample-to-prediction   feature #> 459  1.509272e+00 sample-to-prediction   feature #> 460  1.568206e+00 sample-to-prediction   feature #> 461  7.428461e-01 sample-to-prediction   feature #> 462  2.926689e-01 sample-to-prediction   feature #> 463  5.571223e-01 sample-to-prediction   feature #> 464  4.370161e-01 sample-to-prediction   feature #> 465  3.146074e-01 sample-to-prediction   feature #> 466  1.443739e-01 sample-to-prediction   feature #> 467  2.086786e-01 sample-to-prediction   feature #> 468  4.510664e-01 sample-to-prediction   feature #> 469  4.165861e-01 sample-to-prediction   feature #> 470  7.030415e-01 sample-to-prediction   feature #> 471  9.003737e-01 sample-to-prediction   feature #> 472  9.279676e-01 sample-to-prediction   feature #> 473  8.068714e-01 sample-to-prediction   feature #> 474  9.295849e-01 sample-to-prediction   feature #> 475  6.870760e-01 sample-to-prediction   feature #> 476  8.260921e-01 sample-to-prediction   feature #> 477  2.291385e-01 sample-to-prediction   feature #> 478  4.903274e-01 sample-to-prediction   feature #> 479  4.591762e-01 sample-to-prediction   feature #> 480  4.684820e-01 sample-to-prediction   feature #> 481  4.084938e-01 sample-to-prediction   feature #> 482  3.970614e-01 sample-to-prediction   feature #> 483  5.778467e-01 sample-to-prediction   feature #> 484  4.396357e-01 sample-to-prediction   feature #> 485  8.851338e-01 sample-to-prediction   feature #> 486  2.895358e-01 sample-to-prediction   feature #> 487  1.108041e+00 sample-to-prediction   feature #> 488  9.834494e-01 sample-to-prediction   feature #> 489  6.676054e-01 sample-to-prediction   feature #> 490  5.988654e-01 sample-to-prediction   feature #> 491  4.372762e-01 sample-to-prediction   feature #> 492  3.591764e-01 sample-to-prediction   feature #> 493  5.198401e-01 sample-to-prediction   feature #> 494  1.347359e-01 sample-to-prediction   feature #> 495  4.129760e-01 sample-to-prediction   feature #> 496  1.301672e-01 sample-to-prediction   feature #> 497  3.836968e-01 sample-to-prediction   feature #> 498  5.808418e-01 sample-to-prediction   feature #> 499  4.917475e-01 sample-to-prediction   feature #> 500  5.710954e-02 sample-to-prediction   feature #> 501  1.664867e-08 sample-to-prediction   feature #> 502  2.574491e-01 sample-to-prediction   feature #> 503  9.547534e-01 sample-to-prediction   feature #> 504  9.064303e-01 sample-to-prediction   feature #> 505  7.739260e-01 sample-to-prediction   feature #> 506  8.123901e-01 sample-to-prediction   feature #> 507  1.002933e+00 sample-to-prediction   feature #> 508  4.352990e-01 sample-to-prediction   feature #> 509  2.018571e-01 sample-to-prediction   feature #> 510  3.292671e-08 sample-to-prediction   feature #> 511  4.919269e-01 sample-to-prediction   feature #> 512  8.332309e-01 sample-to-prediction   feature #> 513  4.564657e-01 sample-to-prediction   feature #> 514  2.270268e-01 sample-to-prediction   feature #> 515  1.611753e-01 sample-to-prediction   feature #> 516  5.021234e-01 sample-to-prediction   feature #> 517  6.382491e-01 sample-to-prediction   feature #> 518  6.542885e-01 sample-to-prediction   feature #> 519  5.660442e-01 sample-to-prediction   feature #> 520  5.877748e-01 sample-to-prediction   feature #> 521  4.067277e-01 sample-to-prediction   feature #> 522  3.594121e-01 sample-to-prediction   feature #> 523  1.318314e-01 sample-to-prediction   feature #> 524  4.984248e-01 sample-to-prediction   feature #> 525  5.676431e-01 sample-to-prediction   feature #> 526  8.492316e-01 sample-to-prediction   feature #> 527  8.603769e-01 sample-to-prediction   feature #> 528  6.928221e-01 sample-to-prediction   feature #> 529  4.841533e-01 sample-to-prediction   feature #> 530  3.545434e-01 sample-to-prediction   feature #> 531  4.544684e-01 sample-to-prediction   feature #> 532  4.009664e-01 sample-to-prediction   feature #> 533  5.814956e-01 sample-to-prediction   feature #> 534  6.412832e-01 sample-to-prediction   feature #> 535  6.478540e-01 sample-to-prediction   feature #> 536  7.146483e-01 sample-to-prediction   feature #> 537  7.408068e-01 sample-to-prediction   feature #> 538  5.453463e-01 sample-to-prediction   feature #> 539  2.980530e-01 sample-to-prediction   feature #> 540  3.760202e-01 sample-to-prediction   feature #> 541  7.327991e-01 sample-to-prediction   feature #> 542  5.138387e-01 sample-to-prediction   feature #> 543  3.098581e-01 sample-to-prediction   feature #> 544  4.197610e-01 sample-to-prediction   feature #> 545  2.270264e-01 sample-to-prediction   feature #> 546  9.744504e-01 sample-to-prediction   feature #> 547  9.518970e-01 sample-to-prediction   feature #> 548  5.474010e-01 sample-to-prediction   feature #> 549  6.300539e-01 sample-to-prediction   feature #> 550  8.518829e-01 sample-to-prediction   feature #> 551  1.890075e-01 sample-to-prediction   feature #> 552  2.604361e-01 sample-to-prediction   feature #> 553  7.446939e-01 sample-to-prediction   feature #> 554  9.908511e-01 sample-to-prediction   feature #> 555  9.235615e-01 sample-to-prediction   feature #> 556  1.011960e+00 sample-to-prediction   feature #> 557  1.016924e+00 sample-to-prediction   feature #> 558  1.060200e+00 sample-to-prediction   feature #> 559  8.586111e-01 sample-to-prediction   feature #> 560  8.554184e-01 sample-to-prediction   feature #> 561  8.517228e-01 sample-to-prediction   feature #> 562  8.889495e-01 sample-to-prediction   feature #> 563  1.290026e+00 sample-to-prediction   feature #> 564  1.399418e+00 sample-to-prediction   feature #> 565  1.182049e+00 sample-to-prediction   feature #> 566  1.274060e+00 sample-to-prediction   feature #> 567  5.517674e-01 sample-to-prediction   feature #> 568  6.695638e-01 sample-to-prediction   feature #> 569  8.373076e-01 sample-to-prediction   feature #> 570  2.313633e-01 sample-to-prediction   feature #> 571  7.641328e-01 sample-to-prediction   feature #> 572  8.157243e-01 sample-to-prediction   feature #> 573  4.546815e-01 sample-to-prediction   feature #> 574  2.651547e-01 sample-to-prediction   feature #> 575  3.740443e-01 sample-to-prediction   feature #> 576  3.837083e-01 sample-to-prediction   feature #> 577  2.603149e-01 sample-to-prediction   feature #> 578  4.921333e-01 sample-to-prediction   feature #> 579  3.275046e-01 sample-to-prediction   feature #> 580  3.156804e-01 sample-to-prediction   feature #> 581  4.149091e-01 sample-to-prediction   feature #> 582  4.301525e-01 sample-to-prediction   feature #> 583  4.728512e-01 sample-to-prediction   feature #> 584  1.742794e-01 sample-to-prediction   feature #> 585  3.637375e-01 sample-to-prediction   feature #> 586  5.441334e-01 sample-to-prediction   feature #> 587  6.795043e-01 sample-to-prediction   feature #> 588  6.079899e-01 sample-to-prediction   feature #> 589  4.487784e-01 sample-to-prediction   feature #> 590  7.638537e-01 sample-to-prediction   feature #> 591  9.674392e-01 sample-to-prediction   feature #> 592  9.572688e-01 sample-to-prediction   feature #> 593  1.010530e+00 sample-to-prediction   feature #> 594  1.148804e+00 sample-to-prediction   feature #> 595  1.124514e+00 sample-to-prediction   feature #> 596  1.218762e+00 sample-to-prediction   feature #> 597  1.320387e+00 sample-to-prediction   feature #> 598  9.106841e-01 sample-to-prediction   feature #> 599  7.039232e-01 sample-to-prediction   feature #> 600  6.317342e-01 sample-to-prediction   feature #> 601  5.402502e-01 sample-to-prediction   feature #> 602  2.123788e-01 sample-to-prediction   feature #> 603  3.759722e-01 sample-to-prediction   feature #> 604  5.654173e-01 sample-to-prediction   feature #> 605  5.135679e-01 sample-to-prediction   feature #> 606  6.757634e-01 sample-to-prediction   feature #> 607  6.008741e-01 sample-to-prediction   feature #> 608  6.994620e-01 sample-to-prediction   feature #> 609  8.086601e-01 sample-to-prediction   feature #> 610  7.136578e-01 sample-to-prediction   feature #> 611  6.235827e-01 sample-to-prediction   feature #> 612  4.824410e-01 sample-to-prediction   feature #> 613  6.291139e-01 sample-to-prediction   feature #> 614  8.122328e-01 sample-to-prediction   feature #> 615  8.807013e-01 sample-to-prediction   feature #> 616  7.727885e-01 sample-to-prediction   feature #> 617  7.394927e-01 sample-to-prediction   feature #> 618  8.852600e-01 sample-to-prediction   feature #> 619  9.427495e-01 sample-to-prediction   feature #> 620  9.557366e-01 sample-to-prediction   feature #> 621  7.702097e-01 sample-to-prediction   feature #> 622  8.415201e-01 sample-to-prediction   feature #> 623  1.089947e+00 sample-to-prediction   feature #> 624  1.097843e+00 sample-to-prediction   feature #> 625  1.227714e+00 sample-to-prediction   feature #> 626  1.233346e+00 sample-to-prediction   feature #> 627  1.254476e+00 sample-to-prediction   feature #> 628  1.398113e+00 sample-to-prediction   feature #> 629  1.444281e+00 sample-to-prediction   feature #> 630  1.042341e+00 sample-to-prediction   feature #> 631  6.352286e-01 sample-to-prediction   feature #> 632  3.203694e-01 sample-to-prediction   feature #> 633  1.681542e-01 sample-to-prediction   feature #> 634  3.848507e-01 sample-to-prediction   feature #> 635  5.759133e-01 sample-to-prediction   feature #> 636  3.904718e-01 sample-to-prediction   feature #> 637  2.616854e-01 sample-to-prediction   feature #> 638  2.021382e-01 sample-to-prediction   feature #> 639  2.127547e-01 sample-to-prediction   feature #> 640  2.380345e-01 sample-to-prediction   feature #> 641  3.831053e-01 sample-to-prediction   feature #> 642  2.918562e-01 sample-to-prediction   feature #> 643  4.191673e-01 sample-to-prediction   feature #> 644  3.722850e-01 sample-to-prediction   feature #> 645  3.596538e-01 sample-to-prediction   feature #> 646  3.703669e-01 sample-to-prediction   feature #> 647  6.698387e-01 sample-to-prediction   feature #> 648  9.482079e-01 sample-to-prediction   feature #> 649  1.503917e+00 sample-to-prediction   feature #> 650  2.349867e+00 sample-to-prediction   feature #> 651  2.503911e+00 sample-to-prediction   feature #> 652  1.715365e+00 sample-to-prediction   feature #> 653  1.538723e+00 sample-to-prediction   feature #> 654  1.282035e+00 sample-to-prediction   feature #> 655  1.204038e+00 sample-to-prediction   feature #> 656  7.610042e-01 sample-to-prediction   feature #> 657  7.316050e-01 sample-to-prediction   feature #> 658  5.660934e-01 sample-to-prediction   feature #> 659  6.502853e-01 sample-to-prediction   feature #> 660  4.479777e-01 sample-to-prediction   feature #> 661  4.886568e-01 sample-to-prediction   feature #> 662  5.191027e-01 sample-to-prediction   feature #> 663  4.265402e-01 sample-to-prediction   feature #> 664  8.520285e-01 sample-to-prediction   feature #> 665  1.298763e+00 sample-to-prediction   feature #> 666  5.746580e-01 sample-to-prediction   feature #> 667  3.372829e-01 sample-to-prediction   feature #> 668  4.141984e-01 sample-to-prediction   feature #> 669  7.539594e-01 sample-to-prediction   feature #> 670  5.074775e-01 sample-to-prediction   feature #> 671  6.163978e-01 sample-to-prediction   feature #> 672  5.773632e-01 sample-to-prediction   feature #> 673  7.935017e-01 sample-to-prediction   feature #> 674  7.175267e-01 sample-to-prediction   feature #> 675  8.751242e-01 sample-to-prediction   feature #> 676  6.408588e-01 sample-to-prediction   feature #> 677  7.064442e-01 sample-to-prediction   feature #> 678  3.162234e-01 sample-to-prediction   feature #> 679  3.362521e-01 sample-to-prediction   feature #> 680  6.113770e-01 sample-to-prediction   feature #> 681  6.207969e-01 sample-to-prediction   feature #> 682  9.585601e-01 sample-to-prediction   feature #> 683  1.081132e+00 sample-to-prediction   feature #> 684  1.261805e+00 sample-to-prediction   feature #> 685  1.198187e+00 sample-to-prediction   feature #> 686  1.069049e+00 sample-to-prediction   feature #> 687  9.937499e-01 sample-to-prediction   feature #> 688  1.030963e+00 sample-to-prediction   feature #> 689  1.161634e+00 sample-to-prediction   feature #> 690  1.125514e+00 sample-to-prediction   feature #> 691  1.036199e+00 sample-to-prediction   feature #> 692  9.633285e-01 sample-to-prediction   feature #> 693  1.112786e+00 sample-to-prediction   feature #> 694  1.103748e+00 sample-to-prediction   feature #> 695  4.851359e-01 sample-to-prediction   feature #> 696  2.627256e-01 sample-to-prediction   feature #> 697  6.137520e-01 sample-to-prediction   feature #> 698  1.070722e+00 sample-to-prediction   feature #> 699  3.922125e-01 sample-to-prediction   feature #> 700  3.954213e-01 sample-to-prediction   feature #> 701  9.420156e-01 sample-to-prediction   feature #> 702  8.847819e-01 sample-to-prediction   feature #> 703  9.560840e-01 sample-to-prediction   feature #> 704  9.447523e-01 sample-to-prediction   feature #> 705  9.713864e-01 sample-to-prediction   feature #> 706  1.130769e+00 sample-to-prediction   feature #> 707  1.178502e+00 sample-to-prediction   feature #> 708  2.203163e+00 sample-to-prediction   feature #> 709  2.623428e+00 sample-to-prediction   feature #> 710  1.812070e+00 sample-to-prediction   feature #> 711  1.085271e+00 sample-to-prediction   feature #> 712  9.448499e-01 sample-to-prediction   feature #> 713  9.115968e-01 sample-to-prediction   feature #> 714  9.117952e-01 sample-to-prediction   feature #> 715  8.726600e-01 sample-to-prediction   feature #> 716  7.212580e-01 sample-to-prediction   feature #> 717  5.881122e-01 sample-to-prediction   feature #> 718  5.229906e-01 sample-to-prediction   feature #> 719  4.289733e-01 sample-to-prediction   feature #> 720  4.368992e-01 sample-to-prediction   feature #> 721  2.071867e-01 sample-to-prediction   feature #> 722  6.461381e-01 sample-to-prediction   feature #> 723  9.943952e-01 sample-to-prediction   feature #> 724  6.118790e-01 sample-to-prediction   feature #> 725  8.634746e-01 sample-to-prediction   feature #> 726  7.054589e-01 sample-to-prediction   feature #> 727  5.116240e-01 sample-to-prediction   feature #> 728  5.156835e-01 sample-to-prediction   feature #> 729  7.790103e-01 sample-to-prediction   feature #> 730  1.014579e+00 sample-to-prediction   feature #> 731  1.124548e+00 sample-to-prediction   feature #> 732  1.550328e+00 sample-to-prediction   feature #> 733  9.408582e-01 sample-to-prediction   feature #> 734  6.545914e-01 sample-to-prediction   feature #> 735  6.473080e-01 sample-to-prediction   feature #> 736  8.173731e-01 sample-to-prediction   feature #> 737  5.518053e-01 sample-to-prediction   feature #> 738  9.764691e-01 sample-to-prediction   feature #> 739  9.859832e-01 sample-to-prediction   feature #> 740  8.893718e-01 sample-to-prediction   feature #> 741  8.943952e-01 sample-to-prediction   feature #> 742  6.286442e-01 sample-to-prediction   feature #> 743  9.539434e-01 sample-to-prediction   feature #> 744  1.114320e+00 sample-to-prediction   feature #> 745  1.102455e+00 sample-to-prediction   feature #> 746  1.061575e+00 sample-to-prediction   feature #> 747  1.183479e+00 sample-to-prediction   feature #> 748  1.023896e+00 sample-to-prediction   feature #> 749  9.808131e-01 sample-to-prediction   feature #> 750  1.097384e+00 sample-to-prediction   feature #> 751  1.145853e+00 sample-to-prediction   feature #> 752  1.091186e+00 sample-to-prediction   feature #> 753  7.996655e-01 sample-to-prediction   feature #> 754  8.862325e-01 sample-to-prediction   feature #> 755  5.567276e-01 sample-to-prediction   feature #> 756  2.990937e-01 sample-to-prediction   feature #> 757  9.806433e-01 sample-to-prediction   feature #> 758  7.756028e-01 sample-to-prediction   feature #> 759  5.567363e-01 sample-to-prediction   feature #> 760  2.890604e-01 sample-to-prediction   feature #> 761  3.183876e-01 sample-to-prediction   feature #> 762  5.505719e-01 sample-to-prediction   feature #> 763  7.608133e-01 sample-to-prediction   feature #> 764  7.637942e-01 sample-to-prediction   feature #> 765  1.868670e+00 sample-to-prediction   feature #> 766  1.890080e+00 sample-to-prediction   feature #> 767  1.446866e+00 sample-to-prediction   feature #> 768  1.597031e+00 sample-to-prediction   feature #> 769  1.378118e+00 sample-to-prediction   feature #> 770  7.614923e-01 sample-to-prediction   feature #> 771  5.025921e-01 sample-to-prediction   feature #> 772  4.388340e-01 sample-to-prediction   feature #> 773  1.948785e-01 sample-to-prediction   feature #> 774  3.404704e-01 sample-to-prediction   feature #> 775  3.874522e-01 sample-to-prediction   feature #> 776  3.313217e-01 sample-to-prediction   feature #> 777  2.350108e-01 sample-to-prediction   feature #> 778  4.152175e-01 sample-to-prediction   feature #> 779  4.742911e-01 sample-to-prediction   feature #> 780  1.046175e+00 sample-to-prediction   feature #> 781  3.551901e-01 sample-to-prediction   feature #> 782  9.626794e-01 sample-to-prediction   feature #> 783  5.275347e-01 sample-to-prediction   feature #> 784  3.294934e-08 sample-to-prediction   feature #> 785  3.719955e-01 sample-to-prediction   feature #> 786  6.827890e-01 sample-to-prediction   feature #> 787  6.324944e-01 sample-to-prediction   feature #> 788  7.133446e-01 sample-to-prediction   feature #> 789  7.632611e-01 sample-to-prediction   feature #> 790  1.020183e+00 sample-to-prediction   feature #> 791  6.112995e-01 sample-to-prediction   feature #> 792  5.409477e-01 sample-to-prediction   feature #> 793  2.831899e-01 sample-to-prediction   feature #> 794  5.392684e-01 sample-to-prediction   feature #> 795  2.864254e-01 sample-to-prediction   feature #> 796  3.747370e-01 sample-to-prediction   feature #> 797  2.502368e-01 sample-to-prediction   feature #> 798  3.860610e-01 sample-to-prediction   feature #> 799  1.907251e-01 sample-to-prediction   feature #> 800  1.992694e-01 sample-to-prediction   feature #> 801  2.771251e-01 sample-to-prediction   feature #> 802  1.409789e-01 sample-to-prediction   feature #> 803  9.398908e-02 sample-to-prediction   feature #> 804  1.544125e-01 sample-to-prediction   feature #> 805  2.003808e-01 sample-to-prediction   feature #> 806  6.236740e-01 sample-to-prediction   feature #> 807  7.088337e-01 sample-to-prediction   feature #> 808  7.057222e-01 sample-to-prediction   feature #> 809  4.592714e-01 sample-to-prediction   feature #> 810  4.098277e-01 sample-to-prediction   feature #> 811  4.606502e-01 sample-to-prediction   feature #> 812  3.984781e-01 sample-to-prediction   feature #> 813  5.904225e-01 sample-to-prediction   feature #> 814  6.912970e-01 sample-to-prediction   feature #> 815  3.488833e-01 sample-to-prediction   feature #> 816  2.023483e-01 sample-to-prediction   feature #> 817  4.203862e-01 sample-to-prediction   feature #> 818  5.088941e-01 sample-to-prediction   feature #> 819  5.399183e-01 sample-to-prediction   feature #> 820  1.247825e-08 sample-to-prediction   feature #> 821  4.838973e-02 sample-to-prediction   feature #> 822  3.566876e-01 sample-to-prediction   feature #> 823  5.290270e-01 sample-to-prediction   feature #> 824  7.494975e-01 sample-to-prediction   feature #> 825  1.270604e+00 sample-to-prediction   feature #> 826  1.052429e+00 sample-to-prediction   feature #> 827  5.292510e-01 sample-to-prediction   feature #> 828  3.900069e-01 sample-to-prediction   feature #> 829  4.313494e-01 sample-to-prediction   feature #> 830  4.671330e-01 sample-to-prediction   feature #> 831  5.301689e-01 sample-to-prediction   feature #> 832  5.159300e-01 sample-to-prediction   feature #> 833  3.922965e-01 sample-to-prediction   feature #> 834  3.289908e-01 sample-to-prediction   feature #> 835  4.713701e-01 sample-to-prediction   feature #> 836  7.905410e-01 sample-to-prediction   feature #> 837  1.178558e+00 sample-to-prediction   feature #> 838  2.307405e+00 sample-to-prediction   feature #> 839  1.547121e+00 sample-to-prediction   feature #> 840  1.020049e+00 sample-to-prediction   feature #> 841  9.463957e-01 sample-to-prediction   feature #> 842  5.893424e-01 sample-to-prediction   feature #> 843  5.355943e-01 sample-to-prediction   feature #> 844  6.788036e-01 sample-to-prediction   feature #> 845  5.356758e-01 sample-to-prediction   feature #> 846  3.661506e-01 sample-to-prediction   feature #> 847  5.176721e-01 sample-to-prediction   feature #> 848  1.453154e-01 sample-to-prediction   feature #> 849  3.680291e-01 sample-to-prediction   feature #> 850  5.927787e-01 sample-to-prediction   feature #> 851  6.215744e-01 sample-to-prediction   feature #> 852  2.591600e-01 sample-to-prediction   feature #> 853  5.034656e-01 sample-to-prediction   feature #> 854  3.490684e-01 sample-to-prediction   feature #> 855  4.652739e-01 sample-to-prediction   feature #> 856  4.218615e-01 sample-to-prediction   feature #> 857  5.498780e-01 sample-to-prediction   feature #> 858  5.342743e-01 sample-to-prediction   feature #> 859  3.596054e-01 sample-to-prediction   feature #> 860  4.565462e-01 sample-to-prediction   feature #> 861  4.708726e-01 sample-to-prediction   feature #> 862  5.046072e-01 sample-to-prediction   feature #> 863  5.020695e-01 sample-to-prediction   feature #> 864  5.473966e-01 sample-to-prediction   feature #> 865  6.319179e-01 sample-to-prediction   feature #> 866  2.482462e-01 sample-to-prediction   feature #> 867  8.081816e-09 sample-to-prediction   feature #> 868  3.502744e-01 sample-to-prediction   feature #> 869  5.152387e-01 sample-to-prediction   feature #> 870  3.673595e-01 sample-to-prediction   feature #> 871  3.319973e-01 sample-to-prediction   feature #> 872  9.868449e-02 sample-to-prediction   feature #> 873  3.306326e-01 sample-to-prediction   feature #> 874  5.483082e-01 sample-to-prediction   feature #> 875  2.638358e-01 sample-to-prediction   feature #> 876  3.596912e-01 sample-to-prediction   feature #> 877  2.995006e-01 sample-to-prediction   feature #> 878  2.139295e-01 sample-to-prediction   feature #> 879  3.994370e-01 sample-to-prediction   feature #> 880  6.439808e-01 sample-to-prediction   feature #> 881  1.439383e+00 sample-to-prediction   feature #> 882  1.413498e+00 sample-to-prediction   feature #> 883  9.858028e-01 sample-to-prediction   feature #> 884  6.025493e-01 sample-to-prediction   feature #> 885  5.432411e-01 sample-to-prediction   feature #> 886  6.385258e-01 sample-to-prediction   feature #> 887  6.431981e-01 sample-to-prediction   feature #> 888  6.054504e-01 sample-to-prediction   feature #> 889  5.734621e-01 sample-to-prediction   feature #> 890  2.991622e-01 sample-to-prediction   feature #> 891  5.193915e-01 sample-to-prediction   feature #> 892  7.899001e-01 sample-to-prediction   feature #> 893  1.605171e+00 sample-to-prediction   feature #> 894  1.280786e+00 sample-to-prediction   feature #> 895  8.714719e-01 sample-to-prediction   feature #> 896  5.735715e-01 sample-to-prediction   feature #> 897  1.095126e+00 sample-to-prediction   feature #> 898  1.040550e+00 sample-to-prediction   feature #> 899  5.096685e-01 sample-to-prediction   feature #> 900  4.900921e-01 sample-to-prediction   feature #> 901  9.012636e-01 sample-to-prediction   feature #> 902  3.294222e-01 sample-to-prediction   feature #> 903  3.867689e-01 sample-to-prediction   feature #> 904  4.637509e-01 sample-to-prediction   feature #> 905  5.580905e-01 sample-to-prediction   feature #> 906  5.618086e-01 sample-to-prediction   feature #> 907  5.334138e-01 sample-to-prediction   feature #> 908  1.151367e-01 sample-to-prediction   feature #> 909  3.044523e-01 sample-to-prediction   feature #> 910  4.727340e-01 sample-to-prediction   feature #> 911  2.175112e-01 sample-to-prediction   feature #> 912  2.764136e-01 sample-to-prediction   feature #> 913  2.741966e-01 sample-to-prediction   feature #> 914  3.106879e-01 sample-to-prediction   feature #> 915  4.292274e-01 sample-to-prediction   feature #> 916  2.253181e-01 sample-to-prediction   feature #> 917  1.255359e-01 sample-to-prediction   feature #> 918  4.891122e-01 sample-to-prediction   feature #> 919  6.220588e-01 sample-to-prediction   feature #> 920  3.078488e-01 sample-to-prediction   feature #> 921  3.348288e-01 sample-to-prediction   feature #> 922  5.808284e-01 sample-to-prediction   feature #> 923  8.508824e-01 sample-to-prediction   feature #> 924  5.035430e-01 sample-to-prediction   feature #> 925  5.009438e-01 sample-to-prediction   feature #> 926  3.186878e-01 sample-to-prediction   feature #> 927  6.317908e-01 sample-to-prediction   feature #> 928  4.152072e-01 sample-to-prediction   feature #> 929  5.356830e-01 sample-to-prediction   feature #> 930  5.053146e-01 sample-to-prediction   feature #> 931  4.440894e-01 sample-to-prediction   feature #> 932  6.161467e-01 sample-to-prediction   feature #> 933  8.033734e-01 sample-to-prediction   feature #> 934  6.712705e-01 sample-to-prediction   feature #> 935  1.121391e+00 sample-to-prediction   feature #> 936  1.383138e+00 sample-to-prediction   feature #> 937  1.525887e+00 sample-to-prediction   feature #> 938  1.358486e+00 sample-to-prediction   feature #> 939  6.120040e-01 sample-to-prediction   feature #> 940  5.848965e-01 sample-to-prediction   feature #> 941  6.650827e-01 sample-to-prediction   feature #> 942  1.074704e+00 sample-to-prediction   feature #> 943  9.896481e-01 sample-to-prediction   feature #> 944  3.292814e-01 sample-to-prediction   feature #> 945  2.618903e-01 sample-to-prediction   feature #> 946  8.367402e-01 sample-to-prediction   feature #> 947  1.983586e+00 sample-to-prediction   feature #> 948  2.689852e+00 sample-to-prediction   feature #> 949  1.575281e+00 sample-to-prediction   feature #> 950  6.704422e-01 sample-to-prediction   feature #> 951  4.461626e-01 sample-to-prediction   feature #> 952  1.153550e+00 sample-to-prediction   feature #> 953  2.966641e-01 sample-to-prediction   feature #> 954  2.933609e-01 sample-to-prediction   feature #> 955  5.173908e-01 sample-to-prediction   feature #> 956  6.253339e-01 sample-to-prediction   feature #> 957  6.601672e-01 sample-to-prediction   feature #> 958  7.006505e-01 sample-to-prediction   feature #> 959  6.486898e-01 sample-to-prediction   feature #> 960  4.191697e-01 sample-to-prediction   feature #> 961  4.056456e-01 sample-to-prediction   feature #> 962  5.806218e-01 sample-to-prediction   feature #> 963  4.986730e-01 sample-to-prediction   feature #> 964  4.503838e-01 sample-to-prediction   feature #> 965  1.708243e-01 sample-to-prediction   feature #> 966  2.848231e-01 sample-to-prediction   feature #> 967  2.342206e-01 sample-to-prediction   feature #> 968  5.075841e-01 sample-to-prediction   feature #> 969  4.439204e-01 sample-to-prediction   feature #> 970  3.777231e-01 sample-to-prediction   feature #> 971  3.216777e-01 sample-to-prediction   feature #> 972  8.446177e-01 sample-to-prediction   feature #> 973  9.790618e-01 sample-to-prediction   feature #> 974  7.110071e-01 sample-to-prediction   feature #> 975  6.277956e-01 sample-to-prediction   feature #> 976  5.950859e-01 sample-to-prediction   feature #> 977  4.906205e-01 sample-to-prediction   feature #> 978  8.707670e-01 sample-to-prediction   feature #> 979  1.608521e+00 sample-to-prediction   feature #> 980  6.512262e-01 sample-to-prediction   feature #> 981  5.040282e-01 sample-to-prediction   feature #> 982  3.160880e-01 sample-to-prediction   feature #> 983  4.997167e-01 sample-to-prediction   feature #> 984  5.068993e-01 sample-to-prediction   feature #> 985  6.346098e-01 sample-to-prediction   feature #> 986  6.710133e-01 sample-to-prediction   feature #> 987  8.397094e-01 sample-to-prediction   feature #> 988  8.939825e-01 sample-to-prediction   feature #> 989  7.298595e-01 sample-to-prediction   feature #> 990  7.356428e-01 sample-to-prediction   feature #> 991  9.020425e-01 sample-to-prediction   feature #> 992  9.018158e-01 sample-to-prediction   feature #> 993  1.039740e+00 sample-to-prediction   feature #> 994  1.229055e+00 sample-to-prediction   feature #> 995  1.286884e+00 sample-to-prediction   feature #> 996  1.550906e+00 sample-to-prediction   feature #> 997  1.975778e+00 sample-to-prediction   feature #> 998  1.730352e+00 sample-to-prediction   feature #> 999  1.444171e+00 sample-to-prediction   feature #> 1000 1.539068e+00 sample-to-prediction   feature #> 1001 2.674851e+00 sample-to-prediction   feature #> 1002 1.120994e+00 sample-to-prediction   feature #> 1003 6.312441e-01 sample-to-prediction   feature #> 1004 4.260024e-01 sample-to-prediction   feature #> 1005 9.554373e-01 sample-to-prediction   feature #> 1006 2.444084e-01 sample-to-prediction   feature #> 1007 6.478371e-01 sample-to-prediction   feature #> 1008 7.162643e-01 sample-to-prediction   feature #> 1009 7.397248e-01 sample-to-prediction   feature #> 1010 6.567160e-01 sample-to-prediction   feature #> 1011 6.129818e-01 sample-to-prediction   feature #> 1012 4.245748e-01 sample-to-prediction   feature #> 1013 3.269698e-01 sample-to-prediction   feature #> 1014 2.675151e-01 sample-to-prediction   feature #> 1015 3.461975e-01 sample-to-prediction   feature #> 1016 2.474309e-01 sample-to-prediction   feature #> 1017 3.462518e-01 sample-to-prediction   feature #> 1018 2.933199e-01 sample-to-prediction   feature #> 1019 4.264855e-01 sample-to-prediction   feature #> 1020 4.861177e-01 sample-to-prediction   feature #> 1021 4.946573e-01 sample-to-prediction   feature #> 1022 5.867695e-01 sample-to-prediction   feature #> 1023 5.789306e-01 sample-to-prediction   feature #> 1024 8.118037e-01 sample-to-prediction   feature #> 1025 8.358516e-01 sample-to-prediction   feature #> 1026 8.440221e-01 sample-to-prediction   feature #> 1027 8.793267e-01 sample-to-prediction   feature #> 1028 7.768003e-01 sample-to-prediction   feature #> 1029 7.940564e-01 sample-to-prediction   feature #> 1030 8.621128e-01 sample-to-prediction   feature #> 1031 5.124250e-01 sample-to-prediction   feature #> 1032 4.074184e-01 sample-to-prediction   feature #> 1033 4.711435e-01 sample-to-prediction   feature #> 1034 3.461993e-01 sample-to-prediction   feature #> 1035 3.483455e-01 sample-to-prediction   feature #> 1036 5.190943e-01 sample-to-prediction   feature #> 1037 9.159553e-01 sample-to-prediction   feature #> 1038 9.983330e-01 sample-to-prediction   feature #> 1039 8.986529e-01 sample-to-prediction   feature #> 1040 9.171344e-01 sample-to-prediction   feature #> 1041 1.144615e+00 sample-to-prediction   feature #> 1042 1.243178e+00 sample-to-prediction   feature #> 1043 7.927821e-01 sample-to-prediction   feature #> 1044 1.152825e+00 sample-to-prediction   feature #> 1045 1.126733e+00 sample-to-prediction   feature #> 1046 1.153019e+00 sample-to-prediction   feature #> 1047 1.298684e+00 sample-to-prediction   feature #> 1048 1.705334e+00 sample-to-prediction   feature #> 1049 2.308659e+00 sample-to-prediction   feature #> 1050 5.281959e+00 sample-to-prediction   feature #> 1051 2.221820e+00 sample-to-prediction   feature #> 1052 2.018859e+00 sample-to-prediction   feature #> 1053 1.764802e+00 sample-to-prediction   feature #> 1054 1.170462e+00 sample-to-prediction   feature #> 1055 6.180297e-01 sample-to-prediction   feature #> 1056 8.726330e-01 sample-to-prediction   feature #> 1057 7.213151e-01 sample-to-prediction   feature #> 1058 3.786862e-01 sample-to-prediction   feature #> 1059 5.188851e-01 sample-to-prediction   feature #> 1060 5.871140e-01 sample-to-prediction   feature #> 1061 4.069219e-01 sample-to-prediction   feature #> 1062 3.675127e-01 sample-to-prediction   feature #> 1063 4.359454e-01 sample-to-prediction   feature #> 1064 5.654988e-01 sample-to-prediction   feature #> 1065 9.296376e-01 sample-to-prediction   feature #> 1066 8.138797e-01 sample-to-prediction   feature #> 1067 4.427908e-01 sample-to-prediction   feature #> 1068 4.254910e-01 sample-to-prediction   feature #> 1069 5.497072e-01 sample-to-prediction   feature #> 1070 9.060334e-01 sample-to-prediction   feature #> 1071 1.053500e+00 sample-to-prediction   feature #> 1072 6.295217e-01 sample-to-prediction   feature #> 1073 1.218373e+00 sample-to-prediction   feature #> 1074 1.248822e+00 sample-to-prediction   feature #> 1075 1.035483e+00 sample-to-prediction   feature #> 1076 8.012301e-01 sample-to-prediction   feature #> 1077 7.465956e-01 sample-to-prediction   feature #> 1078 6.693918e-01 sample-to-prediction   feature #> 1079 6.321969e-01 sample-to-prediction   feature #> 1080 6.517487e-01 sample-to-prediction   feature #> 1081 6.359306e-01 sample-to-prediction   feature #> 1082 4.610545e-01 sample-to-prediction   feature #> 1083 5.078293e-01 sample-to-prediction   feature #> 1084 2.916036e-01 sample-to-prediction   feature #> 1085 2.673493e-01 sample-to-prediction   feature #> 1086 5.684056e-01 sample-to-prediction   feature #> 1087 4.651723e-01 sample-to-prediction   feature #> 1088 5.154370e-01 sample-to-prediction   feature #> 1089 1.228647e+00 sample-to-prediction   feature #> 1090 1.241595e+00 sample-to-prediction   feature #> 1091 1.106815e+00 sample-to-prediction   feature #> 1092 1.260303e+00 sample-to-prediction   feature #> 1093 1.358851e+00 sample-to-prediction   feature #> 1094 1.888105e+00 sample-to-prediction   feature #> 1095 2.399553e+00 sample-to-prediction   feature #> 1096 2.994036e+00 sample-to-prediction   feature #> 1097 2.643822e+00 sample-to-prediction   feature #> 1098 2.002812e+00 sample-to-prediction   feature #> 1099 1.274043e+00 sample-to-prediction   feature #> 1100 1.056259e+00 sample-to-prediction   feature #> 1101 1.503599e+00 sample-to-prediction   feature #> 1102 1.324281e+00 sample-to-prediction   feature #> 1103 1.054427e+00 sample-to-prediction   feature #> 1104 7.174400e-01 sample-to-prediction   feature #> 1105 1.180460e+00 sample-to-prediction   feature #> 1106 1.227367e+00 sample-to-prediction   feature #> 1107 3.883708e-01 sample-to-prediction   feature #> 1108 4.227609e-01 sample-to-prediction   feature #> 1109 3.048618e-01 sample-to-prediction   feature #> 1110 6.204587e-01 sample-to-prediction   feature #> 1111 5.925555e-01 sample-to-prediction   feature #> 1112 5.873700e-01 sample-to-prediction   feature #> 1113 4.985886e-01 sample-to-prediction   feature #> 1114 4.444686e-01 sample-to-prediction   feature #> 1115 4.794456e-01 sample-to-prediction   feature #> 1116 3.925180e-01 sample-to-prediction   feature #> 1117 2.729047e-01 sample-to-prediction   feature #> 1118 3.170981e-01 sample-to-prediction   feature #> 1119 4.590724e-01 sample-to-prediction   feature #> 1120 4.583902e-01 sample-to-prediction   feature #> 1121 4.963110e-01 sample-to-prediction   feature #> 1122 4.464715e-01 sample-to-prediction   feature #> 1123 4.697173e-01 sample-to-prediction   feature #> 1124 4.754842e-01 sample-to-prediction   feature #> 1125 5.320238e-01 sample-to-prediction   feature #> 1126 6.038321e-01 sample-to-prediction   feature #> 1127 6.655536e-01 sample-to-prediction   feature #> 1128 6.067191e-01 sample-to-prediction   feature #> 1129 5.595668e-01 sample-to-prediction   feature #> 1130 4.201011e-01 sample-to-prediction   feature #> 1131 3.237395e-01 sample-to-prediction   feature #> 1132 3.576979e-01 sample-to-prediction   feature #> 1133 4.365260e-01 sample-to-prediction   feature #> 1134 6.319334e-01 sample-to-prediction   feature #> 1135 8.076778e-01 sample-to-prediction   feature #> 1136 1.032872e+00 sample-to-prediction   feature #> 1137 9.973832e-01 sample-to-prediction   feature #> 1138 1.321989e+00 sample-to-prediction   feature #> 1139 1.709873e+00 sample-to-prediction   feature #> 1140 1.970192e+00 sample-to-prediction   feature #> 1141 1.918412e+00 sample-to-prediction   feature #> 1142 1.812770e+00 sample-to-prediction   feature #> 1143 2.736995e+00 sample-to-prediction   feature #> 1144 3.066505e+00 sample-to-prediction   feature #> 1145 2.787585e+00 sample-to-prediction   feature #> 1146 2.020954e+00 sample-to-prediction   feature #> 1147 1.164047e+00 sample-to-prediction   feature #> 1148 4.390871e-01 sample-to-prediction   feature #> 1149 5.790573e-01 sample-to-prediction   feature #> 1150 8.191043e-01 sample-to-prediction   feature #> 1151 3.382656e-01 sample-to-prediction   feature #> 1152 1.540043e-01 sample-to-prediction   feature #> 1153 3.290228e-01 sample-to-prediction   feature #> 1154 7.367694e-01 sample-to-prediction   feature #> 1155 6.957712e-01 sample-to-prediction   feature #> 1156 6.551726e-01 sample-to-prediction   feature #> 1157 5.645949e-01 sample-to-prediction   feature #> 1158 4.633708e-01 sample-to-prediction   feature #> 1159 6.008659e-01 sample-to-prediction   feature #> 1160 4.904719e-01 sample-to-prediction   feature #> 1161 5.824585e-01 sample-to-prediction   feature #> 1162 7.110306e-01 sample-to-prediction   feature #> 1163 1.050458e+00 sample-to-prediction   feature #> 1164 6.120324e-01 sample-to-prediction   feature #> 1165 5.120807e-01 sample-to-prediction   feature #> 1166 5.905636e-01 sample-to-prediction   feature #> 1167 5.487696e-01 sample-to-prediction   feature #> 1168 4.959093e-01 sample-to-prediction   feature #> 1169 3.478469e-01 sample-to-prediction   feature #> 1170 5.017808e-01 sample-to-prediction   feature #> 1171 4.524744e-01 sample-to-prediction   feature #> 1172 2.933346e-01 sample-to-prediction   feature #> 1173 1.698113e-01 sample-to-prediction   feature #> 1174 3.549484e-01 sample-to-prediction   feature #> 1175 1.921077e-01 sample-to-prediction   feature #> 1176 3.585188e-01 sample-to-prediction   feature #> 1177 7.097523e-01 sample-to-prediction   feature #> 1178 9.308932e-01 sample-to-prediction   feature #> 1179 1.460526e+00 sample-to-prediction   feature #> 1180 1.576845e+00 sample-to-prediction   feature #> 1181 1.447484e+00 sample-to-prediction   feature #> 1182 1.332684e+00 sample-to-prediction   feature #> 1183 2.189425e+00 sample-to-prediction   feature #> 1184 2.616824e+00 sample-to-prediction   feature #> 1185 2.439169e+00 sample-to-prediction   feature #> 1186 1.152104e+00 sample-to-prediction   feature #> 1187 7.228752e-01 sample-to-prediction   feature #> 1188 7.182954e-01 sample-to-prediction   feature #> 1189 8.756340e-01 sample-to-prediction   feature #> 1190 4.639104e-01 sample-to-prediction   feature #> 1191 1.062645e-01 sample-to-prediction   feature #> 1192 3.898540e-01 sample-to-prediction   feature #> 1193 7.611087e-01 sample-to-prediction   feature #> 1194 6.312863e-01 sample-to-prediction   feature #> 1195 6.335136e-01 sample-to-prediction   feature #> 1196 6.415338e-01 sample-to-prediction   feature #> 1197 6.600175e-01 sample-to-prediction   feature #> 1198 6.364895e-01 sample-to-prediction   feature #> 1199 7.142120e-01 sample-to-prediction   feature #> 1200 7.263668e-01 sample-to-prediction   feature #> 1201 9.263737e-01 sample-to-prediction   feature #> 1202 6.222444e-01 sample-to-prediction   feature #> 1203 7.037113e-01 sample-to-prediction   feature #> 1204 6.575015e-01 sample-to-prediction   feature #> 1205 7.385281e-01 sample-to-prediction   feature #> 1206 7.256484e-01 sample-to-prediction   feature #> 1207 1.913486e-01 sample-to-prediction   feature #> 1208 1.394484e-01 sample-to-prediction   feature #> 1209 1.581821e-01 sample-to-prediction   feature #> 1210 5.158571e-01 sample-to-prediction   feature #> 1211 4.043418e-01 sample-to-prediction   feature #> 1212 2.590140e-01 sample-to-prediction   feature #> 1213 3.587848e-01 sample-to-prediction   feature #> 1214 4.366654e-01 sample-to-prediction   feature #> 1215 9.680270e-01 sample-to-prediction   feature #> 1216 1.592700e+00 sample-to-prediction   feature #> 1217 1.660335e+00 sample-to-prediction   feature #> 1218 1.652745e+00 sample-to-prediction   feature #> 1219 1.779558e+00 sample-to-prediction   feature #> 1220 1.515949e+00 sample-to-prediction   feature #> 1221 1.231748e+00 sample-to-prediction   feature #> 1222 1.009058e+00 sample-to-prediction   feature #> 1223 1.099925e+00 sample-to-prediction   feature #> 1224 9.555607e-01 sample-to-prediction   feature #> 1225 7.251738e-01 sample-to-prediction   feature #> 1226 7.228145e-01 sample-to-prediction   feature #> 1227 4.841997e-01 sample-to-prediction   feature #> 1228 2.967998e-01 sample-to-prediction   feature #> 1229 3.522858e-01 sample-to-prediction   feature #> 1230 4.952199e-01 sample-to-prediction   feature #> 1231 3.455177e-01 sample-to-prediction   feature #> 1232 4.646998e-01 sample-to-prediction   feature #> 1233 5.710708e-01 sample-to-prediction   feature #> 1234 5.785868e-01 sample-to-prediction   feature #> 1235 7.002595e-01 sample-to-prediction   feature #> 1236 3.434561e-01 sample-to-prediction   feature #> 1237 1.676411e-01 sample-to-prediction   feature #> 1238 2.086889e-01 sample-to-prediction   feature #> 1239 6.619249e-01 sample-to-prediction   feature #> 1240 5.954110e-01 sample-to-prediction   feature #> 1241 4.441137e-01 sample-to-prediction   feature #> 1242 5.221687e-01 sample-to-prediction   feature #> 1243 4.941878e-01 sample-to-prediction   feature #> 1244 4.862685e-01 sample-to-prediction   feature #> 1245 2.120895e-01 sample-to-prediction   feature #> 1246 3.000811e-01 sample-to-prediction   feature #> 1247 4.848308e-01 sample-to-prediction   feature #> 1248 5.536895e-01 sample-to-prediction   feature #> 1249 9.864849e-01 sample-to-prediction   feature #> 1250 1.114921e+00 sample-to-prediction   feature #> 1251 1.506710e+00 sample-to-prediction   feature #> 1252 1.619073e+00 sample-to-prediction   feature #> 1253 1.155733e+00 sample-to-prediction   feature #> 1254 8.954656e-01 sample-to-prediction   feature #> 1255 1.744157e+00 sample-to-prediction   feature #> 1256 1.132588e+00 sample-to-prediction   feature #> 1257 1.021930e+00 sample-to-prediction   feature #> 1258 6.298984e-01 sample-to-prediction   feature #> 1259 5.071687e-01 sample-to-prediction   feature #> 1260 3.842197e-01 sample-to-prediction   feature #> 1261 6.383784e-01 sample-to-prediction   feature #> 1262 5.747265e-01 sample-to-prediction   feature #> 1263 6.086040e-01 sample-to-prediction   feature #> 1264 7.816861e-01 sample-to-prediction   feature #> 1265 4.580824e-01 sample-to-prediction   feature #> 1266 4.602333e-01 sample-to-prediction   feature #> 1267 4.220045e-01 sample-to-prediction   feature #> 1268 4.903216e-01 sample-to-prediction   feature #> 1269 5.348479e-01 sample-to-prediction   feature #> 1270 2.938100e-01 sample-to-prediction   feature #> 1271 5.098456e-01 sample-to-prediction   feature #> 1272 5.906910e-01 sample-to-prediction   feature #> 1273 6.671399e-01 sample-to-prediction   feature #> 1274 5.926227e-01 sample-to-prediction   feature #> 1275 5.945658e-01 sample-to-prediction   feature #> 1276 6.561431e-01 sample-to-prediction   feature #> 1277 1.012279e+00 sample-to-prediction   feature #> 1278 1.301242e+00 sample-to-prediction   feature #> 1279 9.559789e-01 sample-to-prediction   feature #> 1280 1.098163e+00 sample-to-prediction   feature #> 1281 1.335514e+00 sample-to-prediction   feature #> 1282 1.347295e+00 sample-to-prediction   feature #> 1283 1.152880e+00 sample-to-prediction   feature #> 1284 1.260921e+00 sample-to-prediction   feature #> 1285 1.435185e+00 sample-to-prediction   feature #> 1286 1.069330e+00 sample-to-prediction   feature #> 1287 5.969912e-01 sample-to-prediction   feature #> 1288 4.033292e-01 sample-to-prediction   feature #> 1289 1.705027e-01 sample-to-prediction   feature #> 1290 3.625915e-01 sample-to-prediction   feature #> 1291 1.072003e-01 sample-to-prediction   feature #> 1292 4.501577e-01 sample-to-prediction   feature #> 1293 3.495393e-01 sample-to-prediction   feature #> 1294 5.014103e-01 sample-to-prediction   feature #> 1295 5.667290e-01 sample-to-prediction   feature #> 1296 4.350178e-01 sample-to-prediction   feature #> 1297 1.362582e+00 sample-to-prediction   feature #> 1298 1.290233e+00 sample-to-prediction   feature #> 1299 1.046328e+00 sample-to-prediction   feature #> 1300 1.156141e+00 sample-to-prediction   feature #> 1301 1.262656e+00 sample-to-prediction   feature #> 1302 1.177162e+00 sample-to-prediction   feature #> 1303 1.208887e+00 sample-to-prediction   feature #> 1304 1.200430e+00 sample-to-prediction   feature #> 1305 1.121180e+00 sample-to-prediction   feature #> 1306 1.080579e+00 sample-to-prediction   feature #> 1307 1.146057e+00 sample-to-prediction   feature #> 1308 1.346440e+00 sample-to-prediction   feature #> 1309 1.276925e+00 sample-to-prediction   feature #> 1310 6.147240e-01 sample-to-prediction   feature #> 1311 9.486156e-01 sample-to-prediction   feature #> 1312 9.459172e-01 sample-to-prediction   feature #> 1313 8.635178e-01 sample-to-prediction   feature #> 1314 6.094400e-01 sample-to-prediction   feature #> 1315 2.378251e-01 sample-to-prediction   feature #> 1316 3.001364e-01 sample-to-prediction   feature #> 1317 9.536622e-01 sample-to-prediction   feature #> 1318 1.067189e+00 sample-to-prediction   feature #> 1319 9.949187e-01 sample-to-prediction   feature #> 1320 1.060721e+00 sample-to-prediction   feature #> 1321 1.434102e+00 sample-to-prediction   feature #> 1322 1.259160e+00 sample-to-prediction   feature #> 1323 1.122139e+00 sample-to-prediction   feature #> 1324 1.127722e+00 sample-to-prediction   feature #> 1325 1.120304e+00 sample-to-prediction   feature #> 1326 1.150818e+00 sample-to-prediction   feature #> 1327 8.035441e-01 sample-to-prediction   feature #> 1328 1.073553e+00 sample-to-prediction   feature #> 1329 1.206245e+00 sample-to-prediction   feature #> 1330 1.168821e+00 sample-to-prediction   feature #> 1331 8.520415e-01 sample-to-prediction   feature #> 1332 1.029875e+00 sample-to-prediction   feature #> 1333 1.075128e+00 sample-to-prediction   feature #> 1334 1.132793e+00 sample-to-prediction   feature #> 1335 1.013444e+00 sample-to-prediction   feature #> 1336 1.454687e+00 sample-to-prediction   feature #> 1337 1.351248e+00 sample-to-prediction   feature #> 1338 1.066587e+00 sample-to-prediction   feature #> 1339 8.553463e-01 sample-to-prediction   feature #> 1340 8.293985e-01 sample-to-prediction   feature #> 1341 1.211008e+00 sample-to-prediction   feature #> 1342 2.537961e+00 sample-to-prediction   feature #> 1343 7.043987e-01 sample-to-prediction   feature #> 1344 9.278215e-01 sample-to-prediction   feature #> 1345 1.074723e+00 sample-to-prediction   feature #> 1346 8.890897e-01 sample-to-prediction   feature #> 1347 9.072576e-01 sample-to-prediction   feature #> 1348 2.169641e+00 sample-to-prediction   feature #> 1349 2.506399e+00 sample-to-prediction   feature #> 1350 1.936894e+00 sample-to-prediction   feature #> 1351 1.756169e+00 sample-to-prediction   feature #> 1352 1.615316e+00 sample-to-prediction   feature #> 1353 1.164701e+00 sample-to-prediction   feature #> 1354 2.126575e+00 sample-to-prediction   feature #> 1355 4.819292e+00 sample-to-prediction   feature #>   dist <- plot_geodist(x=pts_train, modeldomain=studyArea, cvfolds = folds, testdata = pts_test,     type = \"feature\",variables=c(\"DEM\",\"TWI\", \"NDRE.M\")) #> Warning: CRS object has comment, which is lost in output; in tests, see #> https://cran.r-project.org/web/packages/sp/vignettes/CRS_warnings.html #> Warning: Transforming SpatialPoints to the crs of the Raster   ############ Example for a random global dataset ############ (refer to figure in Meyer and Pebesma 2022) library(sf) library(rnaturalearth) library(ggplot2)  ### Define prediction area (here: global): ee <- st_crs(\"+proj=eqearth\") co <- ne_countries(returnclass = \"sf\") co.ee <- st_transform(co, ee) #> Warning: GDAL Error 1: PROJ: proj_as_wkt: Unsupported conversion method: Equal Earth  ### Simulate a spatial random sample ### (alternatively replace pts_random by a real sampling dataset (see Meyer and Pebesma 2022): sf_use_s2(FALSE) pts_random <- st_sample(co, 2000) #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar #> although coordinates are longitude/latitude, st_intersects assumes that they are planar  ### See points on the map: ggplot() + geom_sf(data = co.ee, fill=\"#00BFC4\",col=\"#00BFC4\") +      geom_sf(data = pts_random, color = \"#F8766D\",size=0.5, shape=3) +      guides(fill = FALSE, col = FALSE) +      labs(x = NULL, y = NULL) #> Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> = \"none\")` instead. #> Warning: GDAL Error 1: PROJ: proj_as_wkt: Unsupported conversion method: Equal Earth #> Warning: GDAL Error 1: PROJ: proj_as_wkt: Unsupported conversion method: Equal Earth #> Warning: GDAL Error 1: PROJ: proj_as_wkt: Unsupported conversion method: Equal Earth #> Warning: GDAL Error 1: PROJ: proj_as_wkt: Unsupported conversion method: Equal Earth #> Warning: GDAL Error 1: PROJ: proj_as_wkt: Unsupported conversion method: Equal Earth   ### plot distances: dist <- plot_geodist(pts_random,co,showPlot=FALSE) #> Warning: CRS object has comment, which is lost in output; in tests, see #> https://cran.r-project.org/web/packages/sp/vignettes/CRS_warnings.html #> Spherical geometry (s2) switched on dist$plot+scale_x_log10(labels=round)"},{"path":"https://hannameyer.github.io/CAST/reference/print.html","id":null,"dir":"Reference","previous_headings":"","what":"Print CAST classes — print","title":"Print CAST classes — print","text":"Generic print function trainDI aoa","code":""},{"path":"https://hannameyer.github.io/CAST/reference/print.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print CAST classes — print","text":"","code":"# S3 method for trainDI print(x, ...)  show.trainDI(x, ...)  # S3 method for aoa print(x, ...)  show.aoa(x, ...)  # S3 method for nndm print(x, ...)  show.nndm(x, ...)"},{"path":"https://hannameyer.github.io/CAST/reference/print.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print CAST classes — print","text":"x object type nndm. ... arguments.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Dissimilarity Index of training data — trainDI","title":"Calculate Dissimilarity Index of training data — trainDI","text":"function estimates Dissimilarity Index (DI) within training data set used prediction model. Predictors can weighted based internal variable importance machine learning algorithm used model training.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Dissimilarity Index of training data — trainDI","text":"","code":"trainDI(   model = NA,   train = NULL,   variables = \"all\",   weight = NA,   CVtest = NULL,   CVtrain = NULL )"},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Dissimilarity Index of training data — trainDI","text":"model train object created caret used extract weights (based variable importance) well cross-validation folds train data.frame containing data used model training. required model given variables character vector predictor variables. \"\" variables model used model given train dataset. weight data.frame containing weights variable. required model given. CVtest list vector. Either list element contains data points used testing cross validation iteration (.e. held back data). vector contains ID fold training point. required model given. CVtrain list. element contains data points used training cross validation iteration (.e. held back data). required model given required CVtrain opposite CVtest (.e. data point used testing, used training). Relevant data points excluded, e.g. using nndm.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Dissimilarity Index of training data — trainDI","text":"list class trainDI containing: train data frame containing training data weight data frame weights based variable importance. variables Names used variables catvars variables categorial scaleparam Scaling parameters. Output scale trainDist_avrg data frame average eucildean distance training point every point trainDist_avrgmean mean trainDist_avrg. Used normalizing DI trainDI Dissimilarity Index training data threshold DI threshold used inside/outside AOA lower_threshold lower DI threshold. Currently unused.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Calculate Dissimilarity Index of training data — trainDI","text":"function called within aoa estimate DI AOA new data. However, may also used DI training data interest, facilitate parallelization aoa avoiding repeated calculation DI within training data.","code":""},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate Dissimilarity Index of training data — trainDI","text":"Meyer, H., Pebesma, E. (2021): Predicting unknown space? Estimating area applicability spatial prediction models. doi:10.1111/2041-210X.13650","code":""},{"path":[]},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate Dissimilarity Index of training data — trainDI","text":"Hanna Meyer, Marvin Ludwig","code":""},{"path":"https://hannameyer.github.io/CAST/reference/trainDI.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Dissimilarity Index of training data — trainDI","text":"","code":"if (FALSE) { library(sf) library(raster) library(caret) library(viridis) library(latticeExtra) library(ggplot2)  # prepare sample data: dat <- get(load(system.file(\"extdata\",\"Cookfarm.RData\",package=\"CAST\"))) dat <- aggregate(dat[,c(\"VW\",\"Easting\",\"Northing\")],by=list(as.character(dat$SOURCEID)),mean) pts <- st_as_sf(dat,coords=c(\"Easting\",\"Northing\")) pts$ID <- 1:nrow(pts) set.seed(100) pts <- pts[1:30,] studyArea <- stack(system.file(\"extdata\",\"predictors_2012-03-25.grd\",package=\"CAST\"))[[1:8]] trainDat <- extract(studyArea,pts,df=TRUE) trainDat <- merge(trainDat,pts,by.x=\"ID\",by.y=\"ID\")  # visualize data spatially: spplot(scale(studyArea)) plot(studyArea$DEM) plot(pts[,1],add=TRUE,col=\"black\")  # train a model: set.seed(100) variables <- c(\"DEM\",\"NDRE.Sd\",\"TWI\") model <- train(trainDat[,which(names(trainDat)%in%variables)], trainDat$VW, method=\"rf\", importance=TRUE, tuneLength=1, trControl=trainControl(method=\"cv\",number=5,savePredictions=T)) print(model) #note that this is a quite poor prediction model prediction <- predict(studyArea,model) plot(varImp(model,scale=FALSE))  #...then calculate the DI of the trained model: DI = trainDI(model=model) plot(DI)  # the DI can now be used to compute the AOA: AOA = aoa(studyArea, model = model, trainDI = DI) print(AOA) plot(AOA) }"},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-061","dir":"Changelog","previous_headings":"","what":"CAST 0.6.1","title":"CAST 0.6.1","text":"nndm cross-validation suggested Milà et al. (2022) plot_geodist works NNDM trainDI works NNDM rename parameter folds AOA trainDI","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-060","dir":"Changelog","previous_headings":"","what":"CAST 0.6.0","title":"CAST 0.6.0","text":"CRAN release: 2022-03-17 trainDI allows calculate DI training dataset separately aoa function plot print functions AOA function plot nearest neighbor distance distributions geographic feature space function global_validation added extensive restructuring AOA function ffs bss can used global_validation error manual assignment weights fixed","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-051","dir":"Changelog","previous_headings":"","what":"CAST 0.5.1","title":"CAST 0.5.1","text":"CRAN release: 2021-04-07 resolved dependence package “GSIF” removed CRAN repository","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-050","dir":"Changelog","previous_headings":"","what":"CAST 0.5.0","title":"CAST 0.5.0","text":"CRAN release: 2021-02-19 AOA can run parallel calibration DI (calibrate_aoa) aoa work now large training sets default threshold AOA changed","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-042","dir":"Changelog","previous_headings":"","what":"CAST 0.4.2","title":"CAST 0.4.2","text":"CRAN release: 2020-07-17 aoa now working categorical variables fixed error ffs >170 variables used changed order parameters aoa tutorial “Introduction CAST” improved","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-041","dir":"Changelog","previous_headings":"","what":"CAST 0.4.1","title":"CAST 0.4.1","text":"CRAN release: 2020-05-19 vignette: tutorial introducing “area applicability” variable threshold aoa various modifications aoa line submitted paper","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-040","dir":"Changelog","previous_headings":"","what":"CAST 0.4.0","title":"CAST 0.4.0","text":"CRAN release: 2020-04-06 new function “aoa”: quantify visualize area applicability spatial prediction models “minVar” ffs: Instead always starting 2-pair combinations, ffs can now also started combinations variables (e.g starting combinations 3) ffs failed “svmLinear” previous version S4 class issues. Fixed now.","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-031","dir":"Changelog","previous_headings":"","what":"CAST 0.3.1","title":"CAST 0.3.1","text":"CRAN release: 2018-11-19 CreateSpaceTimeFolds accepts tibbles CreateSpaceTimeFolds automatically reduces k necessary ffs accepts arguments taken caret::train new feature: plot_ffs option plot selected variables ","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-030","dir":"Changelog","previous_headings":"","what":"CAST 0.3.0","title":"CAST 0.3.0","text":"CRAN release: 2018-10-11 new feature: Best subset selection (bss) target-oriented validation (slow reliable) alternative ffs minor adaptations: verbose option included, improved examples ffs bugfix: minor adaptations done usage plsr","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-021","dir":"Changelog","previous_headings":"","what":"CAST 0.2.1","title":"CAST 0.2.1","text":"CRAN release: 2018-07-12 new feature: Introduction CAST included vignette. bugfix: minor error fixed using user defined metrics model selection.","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-020","dir":"Changelog","previous_headings":"","what":"CAST 0.2.0","title":"CAST 0.2.0","text":"CRAN release: 2018-05-03 bugfix: ffs option withinSE=TRUE choose model “best model” within SE model trained earlier run number variables. bug fixed withinSE=TRUE ffs now compares performance models use less variables (e.g. model using 5 variables better model using 4 variables still SE 4-variable model, 4-variable model rated better model). new feature: plot_ffs plots results ffs visualize performance changes according model run number variables used.","code":""},{"path":"https://hannameyer.github.io/CAST/news/index.html","id":"cast-010","dir":"Changelog","previous_headings":"","what":"CAST 0.1.0","title":"CAST 0.1.0","text":"CRAN release: 2018-01-09 Initial public version CRAN","code":""}]
