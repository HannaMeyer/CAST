---
title: '5. Cross-validation methods in CAST'
author: "Carles Milà and Jan Linnenbrink"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{5. Cross-validation methods in CAST}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      fig.align = 'center')
```

```{r load packages}
library("sf")
library("CAST")
library("caret")
library("gridExtra")
library("ggplot2")
```

## Introduction

Cross-Validation (CV) is important for many tasks in a predictive mapping workflow, including feature selection (check `CAST::ffs` and `CAST::bss`), hyperparameter tuning, area of applicability estimation (check `CAST::aoa`). Moreover, in the unfortunate case where no independent samples are available to estimate the performance of the final products, it can be used as a last resort to obtain an estimate of the error.

The objective of this vignette is to show the CV options that CAST offers. To do so, we will work with two datasets of annual average air temperature and air pollution (PM$_{2.5}$) in Spain, for which several predictors including a elevation  land cover, impervious surfaces, and population and road density, remote sensing measurements of NDVI, nighttime lights, and land surface temperature among others. For more details, check our [preprint](https://egusphere.copernicus.org/preprints/2024/egusphere-2024-138/) where the dataset is described in more detail. 

```{r read data, fig.width=7}
# Read data
temperature <- read_sf("https://github.com/carlesmila/RF-spatial-proxies/raw/main/data/temp/temp_train.gpkg")
pm25 <- read_sf("https://github.com/carlesmila/RF-spatial-proxies/raw/main/data/AP/PM25_train.gpkg")
spain <- read_sf("https://github.com/carlesmila/RF-spatial-proxies/raw/main/data/boundaries/spain.gpkg")

# Plot them
p1 <- ggplot() +
  geom_sf(data = spain, fill = "grey", alpha = 0.1) +
  geom_sf(data = temperature, aes(col = temp)) +
  scale_colour_distiller(palette = "RdYlBu") +
  theme_bw() +
  labs(col = "") +
  ggtitle("Average 2019 temperature (ºC)")
p2 <- ggplot() +
  geom_sf(data = spain, fill = "grey", alpha = 0.1) +
  geom_sf(data = pm25, aes(col = PM25)) +
  scale_colour_viridis_c(option = "cividis") +
  theme_bw() +
  labs(col = "") +
  ggtitle("Average 2019 PM2.5 (ug/m3)")
grid.arrange(p1, p2, nrow=1)
```

Looking at the two maps we can see that while air temperature stations are nicely distributed around the area of interest; however, air pollution stations are concentrated in specific regions.

## Cross-validation in geographical space

### Evaluation of spatial predictive conditions

In CAST, we deal with data that are indexed in space, i.e. data that present dependencies that do not comply with the assumptions of independence of standard CV methods such as Leave-One-Out (LOO) CV or k-fold CV. Several CV approaches have been proposed to deal with these including blocking and buffering strategies that try to deal with the dependency between the train and hold out data. However, in CAST, we propose strategies that are prediction-oriented, i.e. CV methods that aim to approximate the predictive conditions found when using a model for a specific prediction task. And how we define these conditions? There are several approaches: we could consider the geographical space by focusing on the spatial locations of the samples and prediction points. Another possibility is to consider the feature space, i.e. the covariate values both in the train and prediction set. And there are yet other possibilities, such as considering space-time.

For this section, let's consider geographical space and show what we mean by "predictive conditions" and also see what would happen if we simply used 1) a LOO CV and 2) a random 5-fold CV in each of the two cases. In CAST, this can be easily done by the `CAST::geodist` function, which computes the distribution of nearest neighbour distances between 1) prediction and training points (prediction-to-sample), 2) holdout points and training points during a LOO CV (sample-to-sample), 3) hold out points and training points during a given CV, in our case the 5-fold random CV (CV-distances).

```{r geodist density, fig.width=6, fig.height=7}
set.seed(1234)

# Random 5-fold CV
fold5_temp <- createFolds(1:nrow(temperature), k=5, returnTrain=FALSE)
fold5_pm25 <- createFolds(1:nrow(pm25), k=5, returnTrain=FALSE)

# Explore geographic predictive conditions
predcond_temp <- geodist(temperature, modeldomain = spain, cvfolds = fold5_temp)
predcond_pm25 <- geodist(pm25, modeldomain = spain, cvfolds = fold5_pm25)

# Plot density functions
p1 <- plot(predcond_temp) + ggtitle("Temperature")
p2 <- plot(predcond_pm25) + ggtitle(expression(PM[2.5]))
grid.arrange(p1, p2, nrow=2)
```

We see that, for temperature, although the distribution of geographical distances during CV vs. during prediction do not exactly match, the overlap between the two is substantial. In the PM$_{2.5}$ case, however, the clustering of the stations make prediction-to-sample distances to be much longer that those found during CV.

Before we jump into the CV methods included in `CAST`, it is worth to consider an alternative visualization of the distance distribution using their Empirical Cumulative Density Functions (ECDF) rather than their density functions, which express, for a given distance, the proportion of observations that have a value equal or lower to than distance. Working with ECDFs has the advantage of not having to choose any additional parameter to estimate the density function, and they are one of the building blocks of our proposed methods.

```{r geodist ecdf, fig.width=6, fig.height=7}
# Plot ECDF functions
p1 <- plot(predcond_temp, stat = "ecdf") + ggtitle("Temperature")
p2 <- plot(predcond_pm25, stat = "ecdf") + ggtitle(expression(PM[2.5]))
grid.arrange(p1, p2, nrow=2)
```

### Spatial NNDM LOO CV for small datasets

The cross-validation methods included in `CAST` are named Nearest Neighbour Distance Matching (NNDM) and their goal is precisely to match the distance distributions during CV and prediction we just saw in the last plots. To do so, these methods propose a CV configuration such that the nearest neighbour distance distribution between hold out and training points found during the CV matches as close as possible the distribution of nearest neighbour distances during the prediction.

The first of the two NNDM methods is LOO NNDM. NNDM is a greedy, iterative algorithm that starts with a standard LOO CV and compares the ECDF during CV to that found during prediction. Briefly, when it finds that the ECDF during the CV is greater than the ECDF during the prediction, it removes points in the neighbourhood of the points being validated until a match is achieved. In practice, NNDM is effective for matching the ECDF when training data are clustered, while it generalizes to a standard LOO CV when data are random or regular. All details on the algorithm and a simulation study evaluating its performance can be found in the article [here](https://doi.org/10.5194/egusphere-2023-1308) and listed at the end of the vignette.

Now, let's run the NNDM LOO CV algorithm for both datasets and check their output, first for temperature. Here, we use the polygon of Spain as `modeldomain` from which 1,000 are regularly sampled as prediction points. However, it would also, but it would also be possible to pass to the function a set of prediction points previously defined (check argument `predpoints`).

```{r NNDM LOO temp}
temp_nndm <- nndm(temperature, modeldomain = spain, samplesize = 1000, phi = "max", min_train = 0.5)
print(temp_nndm)
plot(temp_nndm, type = "simple")
```

Now, we do the same for air pollution

```{r NNDM LOO pm25}
pm25_nndm <- nndm(pm25, modeldomain = spain, phi = "max", min_train = 0.5)
print(pm25_nndm)
plot(pm25_nndm, type = "simple")
```

```{r NNDM LOO viz}

```

Now it's time to fit the models and estimate their performance using 1) a standard LOO CV, and 2) NNDM LOO CV.

```{r model fitting LOO}

```

### Spatial kNNDM CV for medium and large datasets

```{r kNNDM}

```

```{r kNNDM viz}

```

```{r model fitting kfold}

```

## Cross-validation in feature space

Coming soon!

## Further reading

* Linnenbrink, J., Milà, C., Ludwig, M., and Meyer, H.: kNNDM: k-fold Nearest Neighbour Distance Matching Cross-Validation for map accuracy estimation, EGUsphere [preprint], https://doi.org/10.5194/egusphere-2023-1308
* Milà, C., Mateu, J., Pebesma, E., Meyer, H. (2022): Nearest Neighbour Distance Matching Leave-One-Out Cross-Validation for map validation. Methods in Ecology and Evolution 00, 1– 13.  https://doi.org/10.1111/2041-210X.13851
